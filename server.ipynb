{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bef79032-6226-423c-97c9-359a80c4c2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: failed to import scopus_app_core.py. Endpoints using it will error.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\yusef.atteyih\\AppData\\Local\\Temp\\ipykernel_25692\\3878367549.py\", line 48, in <module>\n",
      "    import scopus_app_core as core  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'scopus_app_core'\n",
      "\n",
      "Local: http://127.0.0.1:5000\n",
      "LAN:   http://10.0.66.249:5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Sep/2025 17:22:16] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Sep/2025 17:22:16] \"GET /api/authors HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [02/Sep/2025 17:22:27] \"POST /api/run_author HTTP/1.1\" 500 -\n"
     ]
    }
   ],
   "source": [
    "# server_flask_threaded.py\n",
    "\"\"\"\n",
    "Notebook-friendly Flask server that wraps scopus_app_core.process_author and\n",
    "adds three Excel sheets per author file: Articles (all), BAU Articles (affiliated),\n",
    "and APP (Participation Score) for years 2022,2023,2024.\n",
    "\n",
    "Usage (Jupyter):\n",
    "    import server_flask_threaded as server  # module will start the server thread\n",
    "    # Server object is 'server' at module level; to stop:\n",
    "    server.server.shutdown()\n",
    "\n",
    "Or run as script:\n",
    "    python server_flask_threaded.py\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import socket\n",
    "import traceback\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from flask import Flask, request, jsonify, send_file, send_from_directory, abort\n",
    "from werkzeug.serving import make_server\n",
    "from werkzeug.utils import secure_filename\n",
    "\n",
    "# -------------------- Config / paths --------------------\n",
    "ROOT = Path.cwd()\n",
    "STATIC_DIR = ROOT / \"static\"\n",
    "AUTHORS_DIR = ROOT / \"authors\"\n",
    "UPLOADS = ROOT / \"uploads\"\n",
    "\n",
    "for d in (STATIC_DIR, AUTHORS_DIR, UPLOADS):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# default years for APP calculation\n",
    "APP_YEARS = [2022, 2023, 2024]\n",
    "\n",
    "ALLOWED_EXTS = {\".csv\", \".xlsx\", \".xls\"}\n",
    "PORT = int(os.environ.get(\"PORT\", 5000))\n",
    "\n",
    "# -------------------- Try to import user's core --------------------\n",
    "try:\n",
    "    import scopus_app_core as core  # type: ignore\n",
    "except Exception:\n",
    "    core = None\n",
    "    _import_err = traceback.format_exc()\n",
    "    print(\"Warning: failed to import scopus_app_core.py. Endpoints using it will error.\")\n",
    "    print(_import_err)\n",
    "\n",
    "# -------------------- App & util funcs --------------------\n",
    "app = Flask(__name__, static_folder=str(STATIC_DIR), template_folder=str(STATIC_DIR))\n",
    "STATE: Dict[str, Optional[object]] = {\"citescore_path\": None, \"cs_table\": None, \"cs_by_source\": None}\n",
    "\n",
    "\n",
    "def get_lan_ip() -> str:\n",
    "    try:\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        s.connect((\"8.8.8.8\", 80))\n",
    "        ip = s.getsockname()[0]\n",
    "        s.close()\n",
    "        return ip\n",
    "    except Exception:\n",
    "        return \"127.0.0.1\"\n",
    "\n",
    "\n",
    "def allowed_file(filename: str) -> bool:\n",
    "    return Path(filename).suffix.lower() in ALLOWED_EXTS\n",
    "\n",
    "\n",
    "# -------------------- Small helpers for BAU detection & APP calc --------------------\n",
    "def _candidate_bau_filters() -> List[str]:\n",
    "    \"\"\"Possible column names that might carry BAU info in author Excels.\"\"\"\n",
    "    return [\n",
    "        \"BAU Author IDs\", \"BAU_Author_IDs\", \"bau_author_ids\", \"bau_auids\",\n",
    "        \"BAU Author Names\", \"bau_author_names\", \"BAU Author Organizations\",\n",
    "        \"BAU_Author_Organizations\", \"BAU Author Organizations\",\n",
    "        \"affiliation_ids\", \"affiliation id\", \"affiliations\", \"institutions\",\n",
    "        \"Author Org Map JSON\", \"AuthorOrgMapJSON\", \"author_org_map_json\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def _detect_bau_rows(df: pd.DataFrame, aff_id: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attempt to detect BAU-affiliated rows. Looks for many possible columns and\n",
    "    tests whether the aff_id appears.\n",
    "    \"\"\"\n",
    "    aff_id = str(aff_id).strip()\n",
    "    if not aff_id:\n",
    "        return df.iloc[0:0].copy()\n",
    "\n",
    "    cols = [c for c in df.columns]\n",
    "    # first, exact BAU Author IDs style\n",
    "    candidates = [c for c in cols if c.lower().replace(\"_\", \" \") in (x.lower() for x in _candidate_bau_filters())]\n",
    "    # fallback: any column whose values contain the aff_id substring\n",
    "    bau_mask = pd.Series(False, index=df.index)\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            # stringify column\n",
    "            s = df[c].astype(str).fillna(\"\")\n",
    "            bau_mask = bau_mask | s.str.contains(repr(aff_id).strip(\"'\\\"\"), na=False)\n",
    "            # also test raw numeric match\n",
    "            bau_mask = bau_mask | s.str.contains(aff_id, na=False)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # If none of the candidate columns exist, try a heuristic on 'Author Org Map JSON' or 'Institutions'\n",
    "    if not bau_mask.any():\n",
    "        for c in (\"Author Org Map JSON\", \"author_org_map_json\", \"Institutions\", \"institutions\"):\n",
    "            if c in df.columns:\n",
    "                try:\n",
    "                    s = df[c].astype(str).fillna(\"\")\n",
    "                    # look for aff id inside json or semicolon-separated lists\n",
    "                    bau_mask = bau_mask | s.str.contains(aff_id, na=False) | s.str.contains(rf\"\\b{aff_id}\\b\", na=False)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    # As a last resort, test whether aff_id appears inside any column's stringified content\n",
    "    if not bau_mask.any():\n",
    "        for c in cols:\n",
    "            try:\n",
    "                s = df[c].astype(str).fillna(\"\")\n",
    "                if s.str.contains(aff_id, na=False).any():\n",
    "                    bau_mask = bau_mask | s.str.contains(aff_id, na=False)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    return df[bau_mask].copy()\n",
    "\n",
    "\n",
    "def _qc_from_percentile(pct: Optional[float]) -> float:\n",
    "    \"\"\"\n",
    "    Map CiteScore Percentile to QC as per your table:\n",
    "      0–10%  -> 1.4\n",
    "      11–25% -> 1.0\n",
    "      26–50% -> 0.8\n",
    "      51–75% -> 0.6\n",
    "      76–100%-> 0.4\n",
    "    (If missing or out-of-range, default to 0.0)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if pct is None:\n",
    "            return 0.0\n",
    "        p = float(pct)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    if 0 <= p <= 10:\n",
    "        return 1.4\n",
    "    if 11 <= p <= 25:\n",
    "        return 1.0\n",
    "    if 26 <= p <= 50:\n",
    "        return 0.8\n",
    "    if 51 <= p <= 75:\n",
    "        return 0.6\n",
    "    if 76 <= p <= 100:\n",
    "        return 0.4\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def _author_coefficient(num_authors: Optional[int]) -> float:\n",
    "    try:\n",
    "        n = int(num_authors)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    if n <= 0:\n",
    "        return 0.0\n",
    "    if n == 1:\n",
    "        return 1.2\n",
    "    return round(1.2 / n, 4)\n",
    "\n",
    "\n",
    "def compute_app_sheet(df_articles: pd.DataFrame, aff_id: Optional[str], years: List[int] = APP_YEARS) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns (app_details_df, app_summary_df).\n",
    "    - app_details_df: one row per eligible article (subtype 'ar', year in years)\n",
    "        columns: eid, title, year, authors_count, cs_percentile, QC, AC, Contribution\n",
    "        and columns copied from original as helpful.\n",
    "    - app_summary_df: per-year sums and thresholds + total.\n",
    "    \"\"\"\n",
    "    if df_articles is None or df_articles.empty:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    df = df_articles.copy()\n",
    "\n",
    "    # Normalize year column if present\n",
    "    year_col = None\n",
    "    for c in (\"year\", \"Year\", \"publication_year\", \"coverDate\", \"Full Date\"):\n",
    "        if c in df.columns:\n",
    "            year_col = c\n",
    "            break\n",
    "\n",
    "    def _to_year(v):\n",
    "        try:\n",
    "            if pd.isna(v):\n",
    "                return None\n",
    "            s = str(v).strip()\n",
    "            # first try integer\n",
    "            if s.isdigit() and len(s) == 4:\n",
    "                return int(s)\n",
    "            # common format YYYY-MM-DD\n",
    "            if len(s) >= 4 and s[:4].isdigit():\n",
    "                return int(s[:4])\n",
    "            return None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    df[\"_pub_year\"] = df[year_col].apply(_to_year) if year_col else None\n",
    "    if df[\"_pub_year\"] is None:\n",
    "        df[\"_pub_year\"] = pd.Series([None] * len(df))\n",
    "\n",
    "    # Filter by subtype 'ar' (article) — exact match or lowercase contains 'ar' at start.\n",
    "    subtype_col = None\n",
    "    for c in (\"subtype\", \"Publication type\", \"PublicationType\", \"subtypedescription\"):\n",
    "        if c in df.columns:\n",
    "            subtype_col = c\n",
    "            break\n",
    "    if subtype_col:\n",
    "        # accept exact 'ar' or 'article' case-insensitive\n",
    "        df[\"_is_article\"] = df[subtype_col].astype(str).str.lower().isin({\"ar\", \"article\", \"research article\", \"research-article\", \"research\"})\n",
    "    else:\n",
    "        # If no subtype available, conservatively assume all are articles (user asked to include only ar; but we can't detect)\n",
    "        df[\"_is_article\"] = True\n",
    "\n",
    "    # Filter eligible rows\n",
    "    df_elig = df[df[\"_is_article\"] & df[\"_pub_year\"].isin(years)].copy()\n",
    "    if df_elig.empty:\n",
    "        # empty details and a zero summary\n",
    "        summary = pd.DataFrame([{\"Year\": y, \"APP\": 0.0} for y in years] + [{\"Year\": \"Total\", \"APP\": 0.0}])\n",
    "        return pd.DataFrame(), summary\n",
    "\n",
    "    # Authors count column detection\n",
    "    auth_col = None\n",
    "    for c in (\"authors_count\", \"Number of Authors\", \"authors_count\"):\n",
    "        if c in df_elig.columns:\n",
    "            auth_col = c\n",
    "            break\n",
    "    if not auth_col:\n",
    "        # try 'Authors' number from semicolon-separated list\n",
    "        if \"Authors\" in df_elig.columns:\n",
    "            df_elig[\"authors_count_infer\"] = df_elig[\"Authors\"].astype(str).apply(lambda s: 0 if not s.strip() else len([x for x in s.split(\";\") if x.strip()]))\n",
    "            auth_col = \"authors_count_infer\"\n",
    "        else:\n",
    "            df_elig[\"authors_count_infer\"] = 1\n",
    "            auth_col = \"authors_count_infer\"\n",
    "\n",
    "    # CiteScore percentile column detection\n",
    "    pct_col = None\n",
    "    for c in (\"cs_percentile\", \"CiteScore Percentile\", \"cs_percentile\"):\n",
    "        if c in df_elig.columns:\n",
    "            pct_col = c\n",
    "            break\n",
    "\n",
    "    # Compute coefficients\n",
    "    def _compute_row(row):\n",
    "        n_auth = row.get(auth_col)\n",
    "        ac = _author_coefficient(n_auth)\n",
    "        pct = None\n",
    "        if pct_col:\n",
    "            try:\n",
    "                pct = float(row.get(pct_col)) if row.get(pct_col) not in (None, \"\", \"nan\") else None\n",
    "            except Exception:\n",
    "                pct = None\n",
    "        qc = _qc_from_percentile(pct)\n",
    "        contrib = round(ac * qc, 2)\n",
    "        return pd.Series({\"AC\": ac, \"QC\": qc, \"Contribution\": contrib, \"CiteScore Percentile\": pct})\n",
    "\n",
    "    comps = df_elig.apply(_compute_row, axis=1)\n",
    "    df_out = pd.concat([df_elig.reset_index(drop=True), comps.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Create summary per year\n",
    "    sums = df_out.groupby(\"_pub_year\")[\"Contribution\"].sum().reindex(years, fill_value=0.0)\n",
    "    summary_rows = [{\"Year\": int(y), \"APP\": round(float(sums.loc[y]), 2)} for y in years]\n",
    "    total = round(sum(r[\"APP\"] for r in summary_rows), 2)\n",
    "    summary_rows.append({\"Year\": \"Total\", \"APP\": total})\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "    # attach any BAU filtering notes: (we don't need it here; BAU articles will be separate)\n",
    "    return df_out, summary_df\n",
    "\n",
    "\n",
    "# -------------------- Endpoints --------------------\n",
    "@app.get(\"/\")\n",
    "def index():\n",
    "    idx = STATIC_DIR / \"index.html\"\n",
    "    if idx.exists():\n",
    "        return send_from_directory(str(STATIC_DIR), \"index.html\")\n",
    "    return jsonify({\"ok\": True, \"message\": \"Place static/index.html in a 'static' folder.\"})\n",
    "\n",
    "\n",
    "@app.post(\"/api/upload_citescore\")\n",
    "def api_upload_citescore():\n",
    "    \"\"\"\n",
    "    Accept multipart file upload (field 'file'). Loads CiteScore via core.load_citescore_table\n",
    "    and builds cs_by_source.\n",
    "    \"\"\"\n",
    "    if core is None:\n",
    "        return jsonify({\"ok\": False, \"error\": \"scopus_app_core.py not importable.\"}), 500\n",
    "    if \"file\" not in request.files:\n",
    "        return jsonify({\"ok\": False, \"error\": \"No file uploaded\"}), 400\n",
    "    f = request.files[\"file\"]\n",
    "    if f.filename == \"\":\n",
    "        return jsonify({\"ok\": False, \"error\": \"Empty filename\"}), 400\n",
    "    if not allowed_file(f.filename):\n",
    "        return jsonify({\"ok\": False, \"error\": f\"Unsupported extension {f.filename}\"}), 400\n",
    "\n",
    "    dest = UPLOADS / secure_filename(f.filename)\n",
    "    f.save(dest)\n",
    "    try:\n",
    "        cs_table = core.load_citescore_table(dest)\n",
    "        cs_by_source = core.build_cs_by_source(cs_table, serial_sleep=0.0)\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return jsonify({\"ok\": False, \"error\": f\"Failed to load CiteScore: {e}\"}), 500\n",
    "\n",
    "    STATE[\"citescore_path\"] = str(dest)\n",
    "    STATE[\"cs_table\"] = cs_table\n",
    "    STATE[\"cs_by_source\"] = cs_by_source\n",
    "    return jsonify({\"ok\": True, \"path\": str(dest)})\n",
    "\n",
    "\n",
    "@app.post(\"/api/run_author\")\n",
    "def api_run_author():\n",
    "    \"\"\"\n",
    "    JSON body:\n",
    "      { \"auid\": \"55537877400\", \"aff_id\": \"60021379\" }\n",
    "    Runs core.process_author, then post-processes the returned Excel to add:\n",
    "      - Articles (all rows)\n",
    "      - BAU Articles (rows matching aff_id heuristics)\n",
    "      - APP (Participation Score) for years 2022-2024, subtype 'ar' only\n",
    "    \"\"\"\n",
    "    if core is None:\n",
    "        return jsonify({\"ok\": False, \"error\": \"scopus_app_core.py not importable.\"}), 500\n",
    "    data = request.get_json(force=True, silent=True) or {}\n",
    "    auid = str(data.get(\"auid\", \"\")).strip()\n",
    "    aff_id = str(data.get(\"aff_id\", \"\")).strip()\n",
    "    years = data.get(\"years\", APP_YEARS)\n",
    "    try:\n",
    "        years = [int(y) for y in years] if isinstance(years, (list, tuple)) else APP_YEARS\n",
    "    except Exception:\n",
    "        years = APP_YEARS\n",
    "\n",
    "    if not auid or not auid.isdigit():\n",
    "        return jsonify({\"ok\": False, \"error\": \"Provide numeric auid\"}), 400\n",
    "    if STATE.get(\"cs_table\") is None or STATE.get(\"cs_by_source\") is None:\n",
    "        return jsonify({\"ok\": False, \"error\": \"Please upload CiteScore first (POST /api/upload_citescore)\"}), 400\n",
    "\n",
    "    try:\n",
    "        # call your core processing (this should write the author file to authors/)\n",
    "        out_path = core.process_author(\n",
    "            author_id=auid,\n",
    "            cs_table=STATE[\"cs_table\"],\n",
    "            out_dir=AUTHORS_DIR,\n",
    "            sleep=0.05,\n",
    "            serial_sleep=0.0,\n",
    "        )\n",
    "    except TypeError:\n",
    "        # fallback if core.process_author has different signature\n",
    "        try:\n",
    "            out_path = core.process_author(auid, STATE[\"cs_table\"], AUTHORS_DIR, 0.05, 0.0)  # older signature\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            return jsonify({\"ok\": False, \"error\": f\"process_author failed: {e}\"}), 500\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return jsonify({\"ok\": False, \"error\": f\"process_author failed: {e}\"}), 500\n",
    "\n",
    "    if not out_path:\n",
    "        return jsonify({\"ok\": False, \"error\": \"process_author returned no file path\"}), 500\n",
    "\n",
    "    out_fp = Path(out_path)\n",
    "    if not out_fp.exists():\n",
    "        return jsonify({\"ok\": False, \"error\": f\"Author file not found after processing: {out_path}\"}), 500\n",
    "\n",
    "    # Read all existing sheets, then write new file with added sheets\n",
    "    try:\n",
    "        # read all sheets\n",
    "        xl = pd.read_excel(out_fp, sheet_name=None)\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return jsonify({\"ok\": False, \"error\": f\"Failed to read generated excel: {e}\"}), 500\n",
    "\n",
    "    # Ensure we have an 'articles' sheet or detect close variants\n",
    "    articles_df = None\n",
    "    for sname in xl:\n",
    "        if sname.lower() == \"articles\" or \"article\" in sname.lower():\n",
    "            articles_df = xl[sname].copy()\n",
    "            break\n",
    "    if articles_df is None:\n",
    "        # fallback: try first sheet\n",
    "        first = next(iter(xl.keys()))\n",
    "        articles_df = xl[first].copy()\n",
    "\n",
    "    # Build Articles sheet (all rows unfiltered)\n",
    "    articles_sheet = articles_df.copy()\n",
    "\n",
    "    # Build BAU Articles sheet (attempt detection)\n",
    "    bau_sheet = _detect_bau_rows(articles_df, aff_id)\n",
    "\n",
    "    # Build APP sheet(s)\n",
    "    app_details_df, app_summary_df = compute_app_sheet(articles_df, aff_id, years=years)\n",
    "\n",
    "    # Recompose workbook: keep other original sheets too, and add new ones\n",
    "    try:\n",
    "        new_sheets: Dict[str, pd.DataFrame] = {}\n",
    "        # keep original sheets (but do not duplicate 'articles' if we will add 'Articles' separately)\n",
    "        for sname, df in xl.items():\n",
    "            new_sheets[sname] = df\n",
    "\n",
    "        # Add the \"Articles\" sheet (all rows) and \"BAU Articles\", \"APP Details\", \"APP Summary\"\n",
    "        new_sheets[\"Articles\"] = articles_sheet\n",
    "        new_sheets[\"BAU Articles\"] = bau_sheet\n",
    "        new_sheets[\"APP Details\"] = app_details_df\n",
    "        new_sheets[\"APP Summary\"] = app_summary_df\n",
    "\n",
    "        # Write back (overwrite original file)\n",
    "        with pd.ExcelWriter(out_fp, engine=\"openpyxl\") as writer:\n",
    "            for sname, df in new_sheets.items():\n",
    "                try:\n",
    "                    # Some dataframes might be empty — write an informative placeholder\n",
    "                    if df is None or (isinstance(df, pd.DataFrame) and df.empty):\n",
    "                        pd.DataFrame({\"Info\": [f\"Sheet {sname} is empty.\"]}).to_excel(writer, sheet_name=sname, index=False)\n",
    "                    else:\n",
    "                        df.to_excel(writer, sheet_name=sname, index=False)\n",
    "                except Exception:\n",
    "                    # best-effort: write minimal info if sheet write fails\n",
    "                    pd.DataFrame({\"Info\": [f\"Failed to write sheet {sname}\"]}).to_excel(writer, sheet_name=sname, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return jsonify({\"ok\": False, \"error\": f\"Failed to append new sheets: {e}\"}), 500\n",
    "\n",
    "    return jsonify({\"ok\": True, \"path\": str(out_fp), \"filename\": out_fp.name})\n",
    "\n",
    "\n",
    "@app.post(\"/api/run_authors_batch\")\n",
    "def api_run_authors_batch():\n",
    "    \"\"\"\n",
    "    JSON: { \"auids\": \"123,456,789\", \"aff_id\": \"60021379\" }\n",
    "    \"\"\"\n",
    "    if core is None:\n",
    "        return jsonify({\"ok\": False, \"error\": \"scopus_app_core.py not importable.\"}), 500\n",
    "    data = request.get_json(force=True, silent=True) or {}\n",
    "    auids_raw = data.get(\"auids\", \"\")\n",
    "    aff_id = str(data.get(\"aff_id\", \"\") or \"\").strip()\n",
    "    toks = [t.strip() for t in str(auids_raw).replace(\";\", \",\").split(\",\") if t.strip()]\n",
    "    if not toks:\n",
    "        return jsonify({\"ok\": False, \"error\": \"No AU-IDs provided\"}), 400\n",
    "\n",
    "    results = []\n",
    "    for auid in toks:\n",
    "        try:\n",
    "            resp = api_run_author_inner(auid, aff_id)\n",
    "            results.append(resp)\n",
    "        except Exception as e:\n",
    "            results.append({\"auid\": auid, \"ok\": False, \"error\": str(e)})\n",
    "    return jsonify({\"ok\": True, \"results\": results})\n",
    "\n",
    "\n",
    "def api_run_author_inner(auid: str, aff_id: str):\n",
    "    \"\"\"Helper used by batch runner — mirrors /api/run_author logic but returns dict.\"\"\"\n",
    "    # call process_author\n",
    "    try:\n",
    "        out_path = core.process_author(author_id=auid, cs_table=STATE[\"cs_table\"], out_dir=AUTHORS_DIR, sleep=0.05, serial_sleep=0.0)\n",
    "    except Exception as e:\n",
    "        return {\"auid\": auid, \"ok\": False, \"error\": f\"process_author failed: {e}\"}\n",
    "    if not out_path:\n",
    "        return {\"auid\": auid, \"ok\": False, \"error\": \"No output produced\"}\n",
    "\n",
    "    out_fp = Path(out_path)\n",
    "    if not out_fp.exists():\n",
    "        return {\"auid\": auid, \"ok\": False, \"error\": \"Output file missing after processing\"}\n",
    "\n",
    "    # same post-processing as /api/run_author\n",
    "    try:\n",
    "        xl = pd.read_excel(out_fp, sheet_name=None)\n",
    "        articles_df = None\n",
    "        for sname in xl:\n",
    "            if sname.lower() == \"articles\" or \"article\" in sname.lower():\n",
    "                articles_df = xl[sname].copy()\n",
    "                break\n",
    "        if articles_df is None:\n",
    "            first = next(iter(xl.keys()))\n",
    "            articles_df = xl[first].copy()\n",
    "        bau_sheet = _detect_bau_rows(articles_df, aff_id)\n",
    "        app_details_df, app_summary_df = compute_app_sheet(articles_df, aff_id, years=APP_YEARS)\n",
    "\n",
    "        new_sheets = {}\n",
    "        for sname, df in xl.items():\n",
    "            new_sheets[sname] = df\n",
    "        new_sheets[\"Articles\"] = articles_df\n",
    "        new_sheets[\"BAU Articles\"] = bau_sheet\n",
    "        new_sheets[\"APP Details\"] = app_details_df\n",
    "        new_sheets[\"APP Summary\"] = app_summary_df\n",
    "\n",
    "        with pd.ExcelWriter(out_fp, engine=\"openpyxl\") as writer:\n",
    "            for sname, df in new_sheets.items():\n",
    "                if df is None or (isinstance(df, pd.DataFrame) and df.empty):\n",
    "                    pd.DataFrame({\"Info\": [f\"Sheet {sname} is empty.\"]}).to_excel(writer, sheet_name=sname, index=False)\n",
    "                else:\n",
    "                    df.to_excel(writer, sheet_name=sname, index=False)\n",
    "    except Exception as e:\n",
    "        return {\"auid\": auid, \"ok\": False, \"error\": f\"Post-processing failed: {e}\"}\n",
    "\n",
    "    return {\"auid\": auid, \"ok\": True, \"path\": str(out_fp), \"filename\": out_fp.name}\n",
    "\n",
    "\n",
    "@app.get(\"/api/authors\")\n",
    "def api_list_authors():\n",
    "    items = []\n",
    "    for p in sorted(AUTHORS_DIR.glob(\"*.xlsx\")):\n",
    "        try:\n",
    "            x = pd.ExcelFile(p)\n",
    "            n = 0\n",
    "            if \"articles\" in (s.lower() for s in x.sheet_names):\n",
    "                n = len(pd.read_excel(x, sheet_name=\"articles\"))\n",
    "            items.append({\"filename\": p.name, \"path\": str(p), \"size\": p.stat().st_size, \"articles\": n})\n",
    "        except Exception:\n",
    "            items.append({\"filename\": p.name, \"path\": str(p), \"size\": p.stat().st_size, \"articles\": None})\n",
    "    return jsonify({\"ok\": True, \"items\": items, \"count\": len(items)})\n",
    "\n",
    "\n",
    "@app.get(\"/api/download\")\n",
    "def api_download():\n",
    "    p = request.args.get(\"path\") or \"\"\n",
    "    if not p:\n",
    "        return jsonify({\"error\": \"Provide path query param\"}), 400\n",
    "    fp = Path(p).resolve()\n",
    "    if not fp.exists():\n",
    "        return jsonify({\"error\": \"File not found\"}), 404\n",
    "    # ensure file is under AUTHORS_DIR for safety\n",
    "    try:\n",
    "        if AUTHORS_DIR not in fp.parents and fp.parent != AUTHORS_DIR:\n",
    "            return jsonify({\"error\": \"Invalid path (must be in authors/)\"}), 400\n",
    "    except Exception:\n",
    "        pass\n",
    "    return send_file(str(fp), as_attachment=True)\n",
    "\n",
    "\n",
    "@app.get(\"/api/health\")\n",
    "def api_health():\n",
    "    return jsonify({\"ok\": True, \"status\": \"up\"})\n",
    "\n",
    "\n",
    "# -------------------- Notebook-friendly server class --------------------\n",
    "class ServerThread(threading.Thread):\n",
    "    def __init__(self, app, host: str = \"127.0.0.1\", port: int = PORT):\n",
    "        super().__init__(daemon=True)\n",
    "        self.srv = make_server(host, port, app)\n",
    "        self.ctx = app.app_context()\n",
    "        self.ctx.push()\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "\n",
    "    def run(self):\n",
    "        self.srv.serve_forever()\n",
    "\n",
    "    def shutdown(self):\n",
    "        try:\n",
    "            self.srv.shutdown()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "# Stop previous server object if reloading in notebook\n",
    "try:\n",
    "    server  # type: ignore\n",
    "    try:\n",
    "        server.shutdown()\n",
    "    except Exception:\n",
    "        pass\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "server = ServerThread(app, host=\"0.0.0.0\", port=PORT)\n",
    "server.start()\n",
    "\n",
    "LAN_IP = get_lan_ip()\n",
    "print(f\"Local: http://127.0.0.1:{PORT}\")\n",
    "print(f\"LAN:   http://{LAN_IP}:{PORT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf37d1b-33bc-44e8-a284-83eb6041c8da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
