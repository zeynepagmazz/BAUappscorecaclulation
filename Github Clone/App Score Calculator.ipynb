{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5ea0d8-b2ac-4b8a-abee-e1464ac189e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found: C:\\Users\\yusef.atteyih\\.pybliometrics\\cache\n",
      "Not found: C:\\Users\\yusef.atteyih\\.pybliometrics\\Scopus\n",
      "Removing cache directory: C:\\Users\\yusef.atteyih\\.cache\\pybliometrics\n",
      "Not found: C:\\Users\\yusef.atteyih\\.cache\\pybliometrics\\Scopus\n",
      "✅ Pybliometrics cache cleared (wherever it was found).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/yusef.atteyih/.config/pybliometrics.cfg')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "home = Path.home()\n",
    "\n",
    "# Common cache locations for pybliometrics\n",
    "cache_dirs = [\n",
    "    home / \".pybliometrics\" / \"cache\",\n",
    "    home / \".pybliometrics\" / \"Scopus\",\n",
    "    home / \".cache\" / \"pybliometrics\",\n",
    "    home / \".cache\" / \"pybliometrics\" / \"Scopus\",\n",
    "]\n",
    "\n",
    "for d in cache_dirs:\n",
    "    if d.exists():\n",
    "        print(f\"Removing cache directory: {d}\")\n",
    "        shutil.rmtree(d)\n",
    "    else:\n",
    "        print(f\"Not found: {d}\")\n",
    "\n",
    "print(\"✅ Pybliometrics cache cleared (wherever it was found).\")\n",
    "\n",
    "\n",
    "\n",
    "import pybliometrics\n",
    "pybliometrics.scopus.init()\n",
    "\n",
    "\n",
    "import pybliometrics\n",
    "\n",
    "pybliometrics.scopus.utils.constants.CONFIG_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bd77f3-058c-41eb-817a-443a5f926649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c01f51f-3083-4ca9-887e-d9695ab85bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CiteScore file: C:\\Users\\yusef.atteyih\\Desktop\\Academic Research Unit\\Yusef ATTEYIH\\Data Solutions\\Data Solutions 2.0\\APP Calculation\\CiteScore 2024\\CiteScore 2024 annual values.csv\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Scopus Author IDs (comma or space separated), or leave blank to cancel:  57193254610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CiteScore rows: 75679 | Authors: 1 | Out: C:\\Users\\yusef.atteyih\\Desktop\\Academic Research Unit\\Yusef ATTEYIH\\Data Solutions\\Data Solutions 2.0\\APP Calculation\\authors | Affiliation filter: 60021379\n",
      "[1/1] Processing AU-ID 57193254610 … (AFF_ID=60021379)\n",
      "→ AU-ID 57193254610 — Fadime İrem Doğan (filtering by AFF_ID=60021379)\n",
      "   ✓ Wrote _fadime_irem_dogan___57193254610.xlsx  (3 rows)\n",
      "\n",
      "Done. Wrote 1 file(s) to C:\\Users\\yusef.atteyih\\Desktop\\Academic Research Unit\\Yusef ATTEYIH\\Data Solutions\\Data Solutions 2.0\\APP Calculation\\authors.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "r\"\"\"\n",
    "Scopus → CiteScore with disambiguation by **Source ID then ASJC (sub-subject area)**\n",
    "\n",
    "Fix in this version\n",
    "• Resolved \"unhashable type: 'set'\" by storing ASJC sets as **frozenset** (hashable)\n",
    "  and updating overlap logic to handle frozenset as well.\n",
    "• Added an affiliation filter so only publications affiliated with a specific\n",
    "  organization (affiliation ID) are collected.\n",
    "• APP sheet now calculates Participation Score only for journal **articles**\n",
    "  with Scopus subtype == \"ar\" (reviews 're' are excluded from APP).\n",
    "\n",
    "What this script does\n",
    "• Pulls publications for given Scopus Author IDs (AU-IDs) via pybliometrics\n",
    "  but only those publications that list the configured `aff_id` as an affiliation.\n",
    "• For each article/review, extracts: source_id, ISSN/e-ISSN, ASJC codes, etc.\n",
    "• Loads your CiteScore 2024 table (supports CSV/XLSX; looks for columns:\n",
    "  Print ISSN, E-ISSN, Percentile, CiteScore, and optionally Source ID + ASJC Code[s])\n",
    "• Matches publications to CiteScore rows with the following priority:\n",
    "    1) Same **Scopus source_id**; if multiple rows (different ASJC), pick the row\n",
    "       whose **ASJC** overlaps the article’s ASJC codes; if still a tie, take the\n",
    "       highest Percentile; if still tied, first occurrence.\n",
    "    2) If no source_id match, try **ISSN / e-ISSN** with the same ASJC logic.\n",
    "• Writes one Excel per author under ./authors, named like `_brahim_nalm___55537877400.xlsx`\n",
    "   Each workbook contains:\n",
    "     - \"articles\" sheet (all collected items that passed initial subtype filter)\n",
    "     - \"APP\" sheet (Participation Score breakdown for last 3 calendar years, **articles only** subtype == \"ar\")\n",
    "\n",
    "Usage notes\n",
    "• By default this script filters publications by affiliation id `60021379`.\n",
    "  You can override this with the CLI flag `--aff-id` or by passing `aff_id`\n",
    "  into the `run(..., aff_id=\"...\")` function.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence, Set, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Scopus (pybliometrics) ---\n",
    "try:\n",
    "    from pybliometrics.scopus import AbstractRetrieval, AuthorRetrieval, ScopusSearch, SerialTitle\n",
    "    from pybliometrics.scopus.exception import ScopusException\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"pybliometrics is required. Install with `pip install pybliometrics` \"\n",
    "        \"and ensure your Scopus API credentials are configured.\"\n",
    "    ) from e\n",
    "\n",
    "# ----------------------- IPython detection ----------------------------\n",
    "\n",
    "def _is_ipython() -> bool:\n",
    "    return (\"ipykernel\" in sys.modules) or (\"IPython\" in sys.modules)\n",
    "\n",
    "# ----------------------- Defaults & Affiliation filter ----------------\n",
    "\n",
    "# Default affiliation id to filter publications by (Bahçeşehir University Scopus Affil ID)\n",
    "AFF_ID_DEFAULT = \"60021379\"\n",
    "# Active AFF_ID used by the script (can be overridden via CLI or run())\n",
    "AFF_ID: Optional[str] = AFF_ID_DEFAULT\n",
    "\n",
    "DEFAULT_USER_CITESCORE_DIRS = [\n",
    "    Path(r\"C:\\\\Users\\\\yusef.atteyih\\\\Desktop\\\\Academic Research Unit\\\\Yusef ATTEYIH\\\\Data Solutions\\\\Data Solutions 2.0\\\\APP Calculation\\\\CiteScore 2024\"),\n",
    "]\n",
    "DEFAULT_CITESCORE_DIRS = [Path(\"CiteScore 2024\"), Path(\".\")]\n",
    "POSSIBLE_CS_FILENAMES = [\n",
    "    \"CiteScore 2024 annual values.csv\",\n",
    "    \"CiteScore 2024 annual values.xlsx\",\n",
    "    \"CiteScore 2024.csv\",\n",
    "    \"citescore.csv\",\n",
    "    \"citescore.xlsx\",\n",
    "]\n",
    "\n",
    "# ----------------------- Small helpers --------------------------------\n",
    "\n",
    "def _s(x) -> str:\n",
    "    try:\n",
    "        if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return x if isinstance(x, str) else str(x)\n",
    "\n",
    "def _norm_issn(s: str) -> str:\n",
    "    \"\"\"Uppercase; keep 0-9 and 'X'; strip others.\"\"\"\n",
    "    return re.sub(r\"[^0-9X]\", \"\", _s(s).upper())\n",
    "\n",
    "def _norm_asjc_codes(raw: Any) -> Set[str]:\n",
    "    \"\"\"Return a set of 4-digit ASJC codes as strings.\"\"\"\n",
    "    out: Set[str] = set()\n",
    "    if raw is None:\n",
    "        return out\n",
    "    if isinstance(raw, (list, tuple, set)):\n",
    "        it = raw\n",
    "    else:\n",
    "        # split on non-digits; keep 4-digit tokens\n",
    "        it = re.split(r\"[^0-9]\", _s(raw))\n",
    "    for tok in it:\n",
    "        if tok and tok.isdigit():\n",
    "            if len(tok) == 4:\n",
    "                out.add(tok)\n",
    "            elif len(tok) > 4:\n",
    "                # sometimes concatenated; take last 4 digits\n",
    "                out.add(tok[-4:])\n",
    "    return out\n",
    "\n",
    "def _coerce_percentile(val) -> Optional[float]:\n",
    "    txt = _s(val).strip()\n",
    "    if not txt:\n",
    "        return None\n",
    "    txt = txt.replace(\"%\", \"\").replace(\" \", \"\").replace(\",\", \".\")\n",
    "    m = re.search(r\"[-+]?\\d*\\.?\\d+\", txt)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _coerce_float(val) -> Optional[float]:\n",
    "    txt = _s(val).strip().replace(\",\", \".\")\n",
    "    if not txt:\n",
    "        return None\n",
    "    m = re.search(r\"[-+]?\\d*\\.?\\d+\", txt)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def quartile_from_percentile(p: Optional[float]) -> str:\n",
    "    if p is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        p = float(p)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    if p >= 90: return \"QT\"\n",
    "    if p >= 75: return \"Q1\"\n",
    "    if p >= 50: return \"Q2\"\n",
    "    if p >= 25: return \"Q3\"\n",
    "    return \"Q4\"\n",
    "\n",
    "# ----------------------- Filenames ------------------------------------\n",
    "\n",
    "def _ascii_slug(s: str) -> str:\n",
    "    import unicodedata\n",
    "    s = unicodedata.normalize(\"NFKD\", _s(s)).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^0-9a-z]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def make_author_filename(author_name: str, author_id: str) -> str:\n",
    "    name_slug = _ascii_slug(author_name)\n",
    "    leading = \"_\" + name_slug if not name_slug.startswith(\"_\") else name_slug\n",
    "    return f\"{leading}___{author_id}.xlsx\"\n",
    "\n",
    "# ----------------------- Read AU-IDs ----------------------------------\n",
    "\n",
    "author_id_file_candidates = (\"authors.txt\", \"auids.txt\", \"ScopusAuthorIDs.txt\")\n",
    "\n",
    "def read_auids_from_cli_or_file(cli_auids: Optional[str]) -> List[str]:\n",
    "    ids: List[str] = []\n",
    "    if cli_auids:\n",
    "        for tok in re.split(r\"[,\\s]+\", cli_auids.strip()):\n",
    "            if tok and tok.isdigit():\n",
    "                ids.append(tok)\n",
    "    if ids:\n",
    "        return sorted(set(ids))\n",
    "    for fname in author_id_file_candidates:\n",
    "        p = Path(fname)\n",
    "        if p.exists():\n",
    "            try:\n",
    "                raw = p.read_text(encoding=\"utf-8\")\n",
    "            except Exception:\n",
    "                raw = p.read_text(errors=\"ignore\")\n",
    "            for line in raw.splitlines():\n",
    "                t = line.split(\"#\", 1)[0].strip()\n",
    "                if t.isdigit():\n",
    "                    ids.append(t)\n",
    "            if ids:\n",
    "                return sorted(set(ids))\n",
    "    return []\n",
    "\n",
    "# ----------------------- CiteScore path --------------------------------\n",
    "\n",
    "def _candidate_files_in_dir(d: Path) -> List[Path]:\n",
    "    return [p for name in POSSIBLE_CS_FILENAMES if (p := d / name).exists() and p.is_file()]\n",
    "\n",
    "def resolve_citescore_path(arg: Optional[str], no_prompt: bool = False) -> Optional[Path]:\n",
    "    if arg:\n",
    "        p = Path(arg)\n",
    "        if p.exists():\n",
    "            if p.is_file(): return p\n",
    "            if p.is_dir():\n",
    "                cand = _candidate_files_in_dir(p)\n",
    "                if cand: return cand[0]\n",
    "    envp = os.environ.get(\"CITESCORE_CSV\")\n",
    "    if envp:\n",
    "        p = Path(envp)\n",
    "        if p.exists() and p.is_file(): return p\n",
    "    for d in DEFAULT_USER_CITESCORE_DIRS + DEFAULT_CITESCORE_DIRS:\n",
    "        if d.exists() and d.is_dir():\n",
    "            cand = _candidate_files_in_dir(d)\n",
    "            if cand: return cand[0]\n",
    "    if not no_prompt:\n",
    "        try:\n",
    "            path_in = input(\"Path to CiteScore CSV/XLSX (or folder containing it): \").strip('\"').strip()\n",
    "            if path_in:\n",
    "                p = Path(path_in)\n",
    "                if p.exists():\n",
    "                    if p.is_file(): return p\n",
    "                    if p.is_dir():\n",
    "                        cand = _candidate_files_in_dir(p)\n",
    "                        if cand: return cand[0]\n",
    "        except EOFError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# ----------------------- Read CiteScore table --------------------------\n",
    "\n",
    "def robust_read_table(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"CiteScore file not found: {path}\")\n",
    "    suffix = path.suffix.lower()\n",
    "    if suffix in (\".xlsx\", \".xls\"):\n",
    "        return pd.read_excel(path)\n",
    "    encodings = [\"utf-8-sig\", \"utf-16\", \"utf-16le\", \"utf-16be\", \"cp1254\", \"iso-8859-9\", \"cp1252\", \"latin1\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, engine=\"python\", sep=None)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding=\"latin1\", engine=\"python\", sep=None, on_bad_lines=\"skip\")\n",
    "    except TypeError:\n",
    "        return pd.read_csv(path, encoding=\"latin1\", engine=\"python\", sep=None, error_bad_lines=False)  # type: ignore\n",
    "\n",
    "\n",
    "def load_citescore_table(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return columns:\n",
    "      source_id (if present), issn_key, eissn_key, asjc_set (frozenset), cs_percentile, citescore\n",
    "    Accepts flexible column names (case-insensitive)\n",
    "    \"\"\"\n",
    "    cs = robust_read_table(path)\n",
    "    norm = {c.strip().lower(): c for c in cs.columns}\n",
    "\n",
    "    p = norm.get(\"print issn\") or norm.get(\"p-issn\") or norm.get(\"issn\")\n",
    "    e = norm.get(\"e-issn\") or norm.get(\"eissn\")\n",
    "    pct = norm.get(\"percentile\") or norm.get(\"citescore percentile\")\n",
    "    val = norm.get(\"citescore\") or norm.get(\"citescore 2024\")\n",
    "    src = norm.get(\"source id\") or norm.get(\"scopus source id\") or norm.get(\"scopus sourceid\")\n",
    "    asjc_col = norm.get(\"asjc\") or norm.get(\"asjc code\") or norm.get(\"asjc codes\") or norm.get(\"subject area asjc\")\n",
    "\n",
    "    if not all([p, e, pct, val]):\n",
    "        raise KeyError(\n",
    "            f\"CiteScore table must include 'Print ISSN', 'E-ISSN', 'Percentile', 'CiteScore'. Found: {list(cs.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Standardize names\n",
    "    cs = cs.rename(columns={p: \"print_issn\", e: \"e_issn\", pct: \"cs_percentile\", val: \"citescore\"})\n",
    "    if src:\n",
    "        cs = cs.rename(columns={src: \"source_id\"})\n",
    "    if asjc_col:\n",
    "        cs = cs.rename(columns={asjc_col: \"asjc_raw\"})\n",
    "    else:\n",
    "        cs[\"asjc_raw\"] = \"\"\n",
    "\n",
    "    # Coerce types\n",
    "    cs[\"print_issn\"] = cs[\"print_issn\"].astype(str)\n",
    "    cs[\"e_issn\"] = cs[\"e_issn\"].astype(str)\n",
    "    cs[\"cs_percentile\"] = cs[\"cs_percentile\"].apply(_coerce_percentile)\n",
    "    cs[\"citescore\"] = cs[\"citescore\"].apply(_coerce_float)\n",
    "    if \"source_id\" in cs.columns:\n",
    "        cs[\"source_id\"] = cs[\"source_id\"].astype(str).str.extract(r\"(\\d+)\", expand=False).fillna(\"\")\n",
    "\n",
    "    # Normalized keys\n",
    "    cs[\"issn_key\"] = cs[\"print_issn\"].map(_norm_issn)\n",
    "    cs[\"eissn_key\"] = cs[\"e_issn\"].map(_norm_issn)\n",
    "\n",
    "    # ASJC as frozenset (hashable)\n",
    "    cs[\"asjc_set\"] = cs[\"asjc_raw\"].apply(lambda v: frozenset(_norm_asjc_codes(v)))\n",
    "\n",
    "    # Keep minimal columns\n",
    "    keep = [\"issn_key\", \"eissn_key\", \"asjc_set\", \"cs_percentile\", \"citescore\"]\n",
    "    if \"source_id\" in cs.columns:\n",
    "        keep.insert(0, \"source_id\")\n",
    "    cs = cs[keep]\n",
    "    return cs\n",
    "\n",
    "# ----------------------- Scopus helpers --------------------------------\n",
    "\n",
    "def _extract_issns(ar: Any) -> Tuple[str, str]:\n",
    "    p = \"\"; e = \"\"\n",
    "    if hasattr(ar, \"eIssn\"):\n",
    "        e = _s(getattr(ar, \"eIssn\"))\n",
    "    if not e and hasattr(ar, \"e_issn\"):\n",
    "        e = _s(getattr(ar, \"e_issn\"))\n",
    "    if hasattr(ar, \"issn\"):\n",
    "        obj = getattr(ar, \"issn\")\n",
    "        if isinstance(obj, str):\n",
    "            if \"ISSN(\" in obj:\n",
    "                mp = re.search(r\"print\\s*=\\s*'([^']+)'\", obj)\n",
    "                me = re.search(r\"electronic\\s*=\\s*'([^']+)'\", obj)\n",
    "                if mp: p = mp.group(1)\n",
    "                if me and not e: e = me.group(1)\n",
    "            else:\n",
    "                p = _s(obj)\n",
    "        else:\n",
    "            try:\n",
    "                p_obj = getattr(obj, \"print\", \"\"); e_obj = getattr(obj, \"electronic\", \"\")\n",
    "                if p_obj: p = _s(p_obj)\n",
    "                if not e and e_obj: e = _s(e_obj)\n",
    "            except Exception:\n",
    "                txt = _s(obj)\n",
    "                mp = re.search(r\"print\\s*=\\s*'([^']+)'\", txt)\n",
    "                me = re.search(r\"electronic\\s*=\\s*'([^']+)'\", txt)\n",
    "                if mp: p = mp.group(1)\n",
    "                if me and not e: e = me.group(1)\n",
    "    return p, e\n",
    "\n",
    "\n",
    "def _extract_asjc(ar: Any) -> Tuple[str, str, str, Set[str]]:\n",
    "    codes: Set[str] = set(); areas: Set[str] = set(); abbrevs: Set[str] = set()\n",
    "    sa = getattr(ar, \"subject_areas\", None)\n",
    "    if sa:\n",
    "        try:\n",
    "            for it in sa:\n",
    "                c = getattr(it, \"code\", None)\n",
    "                a = getattr(it, \"area\", None)\n",
    "                ab = getattr(it, \"abbrev\", None)\n",
    "                if c is not None:\n",
    "                    codes.add(f\"{int(c):04d}\" if str(c).isdigit() else str(c))\n",
    "                if a: areas.add(_s(a))\n",
    "                if ab: abbrevs.add(_s(ab))\n",
    "        except Exception:\n",
    "            try:  # dict-like\n",
    "                for it in sa:\n",
    "                    c = it.get(\"code\"); a = it.get(\"area\"); ab = it.get(\"abbrev\")\n",
    "                    if c is not None:\n",
    "                        codes.add(f\"{int(c):04d}\" if str(c).isdigit() else str(c))\n",
    "                    if a: areas.add(_s(a))\n",
    "                    if ab: abbrevs.add(_s(ab))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return \", \".join(sorted(codes)), \", \".join(sorted(areas)), \", \".join(sorted(abbrevs)), codes\n",
    "\n",
    "# ----------------------- ISSN → Source ID (optional) -------------------\n",
    "\n",
    "def fetch_source_id_for_issn(issn: str) -> Optional[str]:\n",
    "    issn = _norm_issn(issn)\n",
    "    if not issn:\n",
    "        return None\n",
    "    try:\n",
    "        res = SerialTitle(issn)\n",
    "        items: Iterable[Any]\n",
    "        try:\n",
    "            items = list(res) if isinstance(res, (list, tuple)) else [res]\n",
    "        except Exception:\n",
    "            items = [res]\n",
    "        for it in items:\n",
    "            for attr in (\"source_id\", \"sourcerecord_id\", \"sourceid\"):\n",
    "                if hasattr(it, attr):\n",
    "                    sid = _s(getattr(it, attr))\n",
    "                    if sid.isdigit():\n",
    "                        return sid\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"SerialTitle lookup failed for ISSN {issn}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_cs_by_source(cs_table: pd.DataFrame, serial_sleep: float = 0.1) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a cs_by_source table with 'source_id' present, mapping from cs_table.\n",
    "    If cs_table already has source_id, just dedupe and return.\n",
    "    Otherwise, look up source_id from ISSN/e-ISSN via SerialTitle.\n",
    "    \"\"\"\n",
    "    if \"source_id\" in cs_table.columns:\n",
    "        df = cs_table.copy()\n",
    "    else:\n",
    "        df = cs_table.copy()\n",
    "        # NOTE: These are vectorized calls; add throttling inside fetch if needed.\n",
    "        df[\"source_id_from_print\"] = df[\"issn_key\"].apply(fetch_source_id_for_issn)\n",
    "        if serial_sleep:\n",
    "            time.sleep(serial_sleep)\n",
    "        df[\"source_id_from_e\"] = df[\"eissn_key\"].apply(fetch_source_id_for_issn)\n",
    "        df[\"source_id\"] = df[\"source_id_from_print\"].where(df[\"source_id_from_print\"].notna(), df[\"source_id_from_e\"])\n",
    "        df = df.drop(columns=[\"source_id_from_print\", \"source_id_from_e\"], errors=\"ignore\")\n",
    "    # Keep only rows with a source_id\n",
    "    df = df[df[\"source_id\"].astype(str).str.strip().ne(\"\")].copy()\n",
    "    # Deduplicate by (source_id, asjc_set); asjc_set is frozenset → hashable\n",
    "    df = df.drop_duplicates(subset=[\"source_id\", \"asjc_set\"], keep=\"first\")\n",
    "    return df[[\"source_id\", \"asjc_set\", \"cs_percentile\", \"citescore\"]]\n",
    "\n",
    "# ----------------------- Matching logic --------------------------------\n",
    "\n",
    "def _pick_best_candidate(cands: pd.DataFrame, article_asjc: Set[str]) -> Tuple[Optional[float], Optional[float]]:\n",
    "    \"\"\"Choose best row: prefer ASJC overlap, then highest percentile, else first.\n",
    "    Returns (cs_percentile, citescore).\n",
    "    \"\"\"\n",
    "    if cands is None or cands.empty:\n",
    "        return None, None\n",
    "    # compute overlap count\n",
    "    over = []\n",
    "    for _i, row in cands.iterrows():\n",
    "        cs_set = row.get(\"asjc_set\") or set()\n",
    "        # Accept frozenset/list/tuple/set/string\n",
    "        if isinstance(cs_set, (set, frozenset)):\n",
    "            cs_set2 = set(cs_set)\n",
    "        elif isinstance(cs_set, (list, tuple)):\n",
    "            cs_set2 = {str(x) for x in cs_set}\n",
    "        else:\n",
    "            cs_set2 = _norm_asjc_codes(cs_set)\n",
    "        over.append(len(article_asjc & cs_set2))\n",
    "    cands = cands.copy()\n",
    "    cands[\"_overlap\"] = over\n",
    "    # sort by: overlap desc, percentile desc (None last)\n",
    "    cands[\"_pct\"] = cands[\"cs_percentile\"].fillna(-1e9)\n",
    "    cands = cands.sort_values([\"_overlap\", \"_pct\"], ascending=[False, False])\n",
    "    top = cands.iloc[0]\n",
    "    return top.get(\"cs_percentile\"), top.get(\"citescore\")\n",
    "\n",
    "\n",
    "def enrich_with_citescore_sourceid_asjc(\n",
    "    df_articles: pd.DataFrame,\n",
    "    cs_table: pd.DataFrame,\n",
    "    cs_by_source: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    if df_articles is None or df_articles.empty:\n",
    "        return df_articles.copy()\n",
    "\n",
    "    df = df_articles.copy()\n",
    "    for col in (\"issn_print\", \"issn_electronic\", \"source_id\"):\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "\n",
    "    # Prepare article keys\n",
    "    df[\"issn_key\"] = df[\"issn_print\"].astype(str).map(_norm_issn)\n",
    "    df[\"eissn_key\"] = df[\"issn_electronic\"].astype(str).map(_norm_issn)\n",
    "\n",
    "    # Pre-split article ASJC sets\n",
    "    a_asjc_sets: List[Set[str]] = []\n",
    "    for v in df.get(\"asjc_codes\", pd.Series([\"\"] * len(df))):\n",
    "        a_asjc_sets.append(_norm_asjc_codes(v))\n",
    "\n",
    "    cs_p = cs_table[[\"issn_key\", \"asjc_set\", \"cs_percentile\", \"citescore\"]].drop_duplicates()\n",
    "    cs_e = cs_table[[\"eissn_key\", \"asjc_set\", \"cs_percentile\", \"citescore\"]].drop_duplicates()\n",
    "\n",
    "    out_pct: List[Optional[float]] = []\n",
    "    out_val: List[Optional[float]] = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        article_asjc = a_asjc_sets[idx] if idx < len(a_asjc_sets) else set()\n",
    "        sid = _s(row.get(\"source_id\"))\n",
    "        pct = None; val = None\n",
    "        # 1) try source_id\n",
    "        if sid:\n",
    "            cands = cs_by_source[cs_by_source[\"source_id\"].astype(str) == sid]\n",
    "            pct, val = _pick_best_candidate(cands, article_asjc)\n",
    "        # 2) fallback by ISSN\n",
    "        if pct is None and val is None:\n",
    "            issn = _s(row.get(\"issn_key\"))\n",
    "            eissn = _s(row.get(\"eissn_key\"))\n",
    "            cands = pd.concat([\n",
    "                cs_p[cs_p[\"issn_key\"] == issn],\n",
    "                cs_e[cs_e[\"eissn_key\"] == eissn],\n",
    "            ], ignore_index=True)\n",
    "            pct, val = _pick_best_candidate(cands, article_asjc)\n",
    "        out_pct.append(pct)\n",
    "        out_val.append(val)\n",
    "\n",
    "    df[\"cs_percentile\"] = out_pct\n",
    "    df[\"citescore\"] = out_val\n",
    "    df[\"quartile\"] = df[\"cs_percentile\"].apply(quartile_from_percentile)\n",
    "    return df\n",
    "\n",
    "# ----------------------- Scopus collectors (AFF filter applied) -------\n",
    "\n",
    "def get_author_name(author_id: str) -> str:\n",
    "    try:\n",
    "        ar = AuthorRetrieval(author_id)\n",
    "        name = f\"{_s(ar.given_name)} {_s(ar.surname)}\".strip()\n",
    "        return name or author_id\n",
    "    except Exception:\n",
    "        return author_id\n",
    "\n",
    "\n",
    "def get_author_eids(author_id: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Return EIDs for the given author that are affiliated with AFF_ID (if set).\n",
    "    Query uses AU-ID(...) AND AF-ID(<aff_id>) when AFF_ID is provided.\n",
    "    \"\"\"\n",
    "    query = f\"AU-ID({author_id})\"\n",
    "    if AFF_ID:\n",
    "        # AF-ID filters by organization affiliation id in Scopus\n",
    "        query = f\"{query} AND AF-ID({AFF_ID})\"\n",
    "    s = ScopusSearch(query, subscriber=True)\n",
    "    return s.get_eids() or []\n",
    "\n",
    "\n",
    "def get_article_metadata(eid: str):\n",
    "    try:\n",
    "        ar = AbstractRetrieval(eid, view=\"FULL\")\n",
    "    except ScopusException as e:\n",
    "        warnings.warn(f\"AbstractRetrieval failed for {eid}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Keep both articles and reviews in the collected list, but APP will count only subtype \"ar\"\n",
    "    if ar.subtype not in (\"ar\", \"re\"):\n",
    "        return None\n",
    "\n",
    "    year = \"\"\n",
    "    if getattr(ar, \"coverDate\", None):\n",
    "        year = _s(ar.coverDate)[:4]\n",
    "\n",
    "    issn_print, issn_elec = _extract_issns(ar)\n",
    "    asjc_codes_csv, asjc_areas_csv, asjc_abbrevs_csv, _codes_set = _extract_asjc(ar)\n",
    "\n",
    "    return {\n",
    "        \"eid\": _s(eid),\n",
    "        \"title\": _s(ar.title),\n",
    "        \"year\": _s(year),\n",
    "        \"publication_name\": _s(getattr(ar, \"publicationName\", \"\")),\n",
    "        \"subtype\": _s(ar.subtype),\n",
    "        \"doi\": _s(getattr(ar, \"doi\", \"\")),\n",
    "        \"source_id\": _s(getattr(ar, \"source_id\", \"\")),\n",
    "        \"issn_print\": _s(issn_print),\n",
    "        \"issn_electronic\": _s(issn_elec),\n",
    "        \"asjc_codes\": asjc_codes_csv,\n",
    "        \"asjc_areas\": asjc_areas_csv,\n",
    "        \"asjc_abbrevs\": asjc_abbrevs_csv,\n",
    "        \"authors_count\": len(ar.authors) if getattr(ar, \"authors\", None) else 1,\n",
    "        \"combined\": \"; \".join([t for t in (getattr(ar, \"authkeywords\", []) or []) if _s(t)]),\n",
    "        \"abstract\": _s(getattr(ar, \"description\", \"\")),\n",
    "    }\n",
    "\n",
    "# ----------------------- APP calculation helpers -----------------------\n",
    "\n",
    "def _qc_from_percentile(p: Optional[float]) -> Optional[float]:\n",
    "    \"\"\"QC mapping assuming higher percentile is better (>=90 is Top 10%).\"\"\"\n",
    "    if p is None:\n",
    "        return None\n",
    "    try:\n",
    "        p = float(p)\n",
    "    except Exception:\n",
    "        return None\n",
    "    if p >= 90:   # Top 10%\n",
    "        return 1.4\n",
    "    if p >= 75:   # Q1\n",
    "        return 1.0\n",
    "    if p >= 50:   # Q2\n",
    "        return 0.8\n",
    "    if p >= 25:   # Q3\n",
    "        return 0.6\n",
    "    if p >= 0:    # Q4\n",
    "        return 0.4\n",
    "    return None\n",
    "\n",
    "def _ac_from_authors(n: Any) -> float:\n",
    "    \"\"\"Author Coefficient: 1.2 if single author else 1.2 / n_authors.\"\"\"\n",
    "    try:\n",
    "        n = int(n)\n",
    "    except Exception:\n",
    "        n = 1\n",
    "    return 1.2 if n <= 1 else 1.2 / max(n, 1)\n",
    "\n",
    "def _to_int_year(y: Any) -> Optional[int]:\n",
    "    try:\n",
    "        s = str(y)\n",
    "        m = re.search(r\"\\d{4}\", s)\n",
    "        return int(m.group(0)) if m else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def build_app_sheet(df_articles: pd.DataFrame, now_year: Optional[int] = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Returns (df_app, summary) where df_app contains the per-paper APP breakdown and\n",
    "    summary has 'app_total' and 'eligibility' text.\n",
    "\n",
    "    NOTE: APP is calculated only for journal articles with subtype == \"ar\".\n",
    "    \"\"\"\n",
    "    if df_articles is None or df_articles.empty:\n",
    "        return pd.DataFrame(), {\"app_total\": 0.0, \"eligibility\": \"No eligible items\", \"years\": []}\n",
    "\n",
    "    # Determine the 3-year window (includes current year)\n",
    "    cy = now_year or datetime.now().year\n",
    "    years_ok = {cy, cy - 1, cy - 2}\n",
    "\n",
    "    # Coerce needed fields\n",
    "    tmp = df_articles.copy()\n",
    "    tmp[\"year_i\"] = tmp.get(\"year\", \"\").apply(_to_int_year)\n",
    "    tmp[\"cs_percentile_num\"] = pd.to_numeric(tmp.get(\"cs_percentile\"), errors=\"coerce\")\n",
    "    tmp[\"authors_count_i\"] = pd.to_numeric(tmp.get(\"authors_count\"), errors=\"coerce\").fillna(1).astype(int)\n",
    "    tmp[\"subtype_norm\"] = tmp.get(\"subtype\", \"\").astype(str).str.lower()\n",
    "\n",
    "    # Eligibility: last 3 calendar years, subtype == \"ar\" only, has percentile\n",
    "    eligible = tmp[\n",
    "        tmp[\"year_i\"].isin(years_ok) &\n",
    "        tmp[\"cs_percentile_num\"].notna() &\n",
    "        (tmp[\"subtype_norm\"] == \"ar\")\n",
    "    ].copy()\n",
    "\n",
    "    if eligible.empty:\n",
    "        return pd.DataFrame(), {\"app_total\": 0.0, \"eligibility\": \"No eligible items (no 'ar' articles in window)\", \"years\": sorted(years_ok)}\n",
    "\n",
    "    # Compute QC, AC, Contribution\n",
    "    eligible[\"QC\"] = eligible[\"cs_percentile_num\"].apply(_qc_from_percentile)\n",
    "    eligible[\"AC\"] = eligible[\"authors_count_i\"].apply(_ac_from_authors)\n",
    "    # If QC missing after mapping, treat as ineligible\n",
    "    eligible = eligible[eligible[\"QC\"].notna()].copy()\n",
    "    if eligible.empty:\n",
    "        return pd.DataFrame(), {\"app_total\": 0.0, \"eligibility\": \"No eligible items (QC missing)\", \"years\": sorted(years_ok)}\n",
    "\n",
    "    eligible[\"Contribution\"] = eligible[\"AC\"] * eligible[\"QC\"]\n",
    "\n",
    "    # Round to 2 decimals per policy\n",
    "    eligible[\"AC\"] = eligible[\"AC\"].round(2)\n",
    "    eligible[\"QC\"] = eligible[\"QC\"].round(2)\n",
    "    eligible[\"Contribution\"] = eligible[\"Contribution\"].round(2)\n",
    "\n",
    "    # Select user-friendly columns\n",
    "    out_cols = [\n",
    "        \"eid\", \"title\", \"year\", \"publication_name\",\n",
    "        \"authors_count\", \"cs_percentile\", \"quartile\",\n",
    "        \"AC\", \"QC\", \"Contribution\"\n",
    "    ]\n",
    "    for c in out_cols:\n",
    "        if c not in eligible.columns:\n",
    "            eligible[c] = pd.Series(dtype=\"object\")\n",
    "    df_app = eligible[out_cols].sort_values([\"year\", \"Contribution\"], ascending=[False, False]).reset_index(drop=True)\n",
    "\n",
    "    app_total = float(df_app[\"Contribution\"].sum().round(2))\n",
    "\n",
    "    # Eligibility band (based on your table)\n",
    "    if app_total > 1.0:\n",
    "        elig = \"APP > 1.0 → up to 2 supports / AY (only 1 requires full indexing & APP check)\"\n",
    "    elif app_total >= 0.4:\n",
    "        elig = \"0.4 ≤ APP ≤ 1.0 → 1 support / AY\"\n",
    "    else:\n",
    "        elig = \"APP < 0.4 → 1 support / AY (if other criteria met)\"\n",
    "\n",
    "    summary = {\"app_total\": round(app_total, 2), \"eligibility\": elig, \"years\": sorted(years_ok)}\n",
    "    return df_app, summary\n",
    "\n",
    "# ----------------------- Excel writer (with APP sheet) -----------------\n",
    "\n",
    "def _write_excel_xlsxwriter(df: pd.DataFrame, path: Path, app_df: Optional[pd.DataFrame] = None, app_summary: Optional[Dict[str, Any]] = None):\n",
    "    with pd.ExcelWriter(path, engine=\"xlsxwriter\") as xl:\n",
    "        df.to_excel(xl, sheet_name=\"articles\", index=False)\n",
    "        ws = xl.sheets[\"articles\"]\n",
    "        try:\n",
    "            ws.freeze_panes(1, 0)\n",
    "            ws.set_column(\"A:A\", 20)  # eid\n",
    "            ws.set_column(\"B:B\", 50)  # title\n",
    "            ws.set_column(\"C:C\", 8)   # year\n",
    "            ws.set_column(\"D:D\", 36)  # publication_name\n",
    "            ws.set_column(\"E:E\", 8)   # subtype\n",
    "            ws.set_column(\"F:F\", 26)  # doi\n",
    "            ws.set_column(\"G:G\", 14)  # source_id\n",
    "            ws.set_column(\"H:I\", 14)  # ISSNs\n",
    "            ws.set_column(\"J:J\", 18)  # asjc_codes\n",
    "            ws.set_column(\"K:K\", 26)  # asjc_areas\n",
    "            ws.set_column(\"L:L\", 14)  # asjc_abbrevs\n",
    "            ws.set_column(\"M:N\", 14)  # cs_percentile / citescore\n",
    "            ws.set_column(\"O:O\", 10)  # quartile\n",
    "            ws.set_column(\"P:P\", 12)  # authors_count\n",
    "            ws.set_column(\"Q:Q\", 28)  # combined\n",
    "            ws.set_column(\"R:R\", 80)  # abstract\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Sheet 2: APP (optional)\n",
    "        if app_df is not None and not app_df.empty:\n",
    "            # write header and summary first (we'll write table starting at row 6)\n",
    "            ws2 = xl.book.add_worksheet(\"APP\")\n",
    "            xl.sheets[\"APP\"] = ws2  # make sure mapping exists\n",
    "            try:\n",
    "                ws2.write(0, 0, \"APP calculation — last 3 calendar years (journal articles, subtype == 'ar')\")\n",
    "                if app_summary:\n",
    "                    ws2.write(1, 0, \"Years considered\")\n",
    "                    ws2.write(1, 1, \", \".join(str(y) for y in app_summary.get(\"years\", [])))\n",
    "                    ws2.write(2, 0, \"APP Score\")\n",
    "                    ws2.write(2, 1, app_summary.get(\"app_total\", 0.0))\n",
    "                    ws2.write(3, 0, \"Eligibility\")\n",
    "                    ws2.write(3, 1, app_summary.get(\"eligibility\", \"\"))\n",
    "                # write the dataframe starting at row 6 (index 5) to leave space for header\n",
    "                (app_df.reset_index(drop=True)).to_excel(xl, sheet_name=\"APP\", index=False, startrow=5)\n",
    "                # adjust columns\n",
    "                ws2.freeze_panes(6, 0)\n",
    "                ws2.set_column(\"A:A\", 20)  # eid\n",
    "                ws2.set_column(\"B:B\", 60)  # title\n",
    "                ws2.set_column(\"C:C\", 8)   # year\n",
    "                ws2.set_column(\"D:D\", 36)  # journal\n",
    "                ws2.set_column(\"E:E\", 12)  # authors_count\n",
    "                ws2.set_column(\"F:F\", 14)  # cs_percentile\n",
    "                ws2.set_column(\"G:G\", 10)  # quartile\n",
    "                ws2.set_column(\"H:J\", 14)  # AC, QC, Contribution\n",
    "            except Exception:\n",
    "                # Fallback: if something goes wrong with custom worksheet, write simply by pandas\n",
    "                try:\n",
    "                    app_df.to_excel(xl, sheet_name=\"APP\", index=False)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "def _write_excel_openpyxl(df: pd.DataFrame, path: Path, app_df: Optional[pd.DataFrame] = None, app_summary: Optional[Dict[str, Any]] = None):\n",
    "    with pd.ExcelWriter(path, engine=\"openpyxl\") as xl:\n",
    "        df.to_excel(xl, sheet_name=\"articles\", index=False)\n",
    "        try:\n",
    "            ws = xl.sheets[\"articles\"]\n",
    "            ws.freeze_panes = \"A2\"\n",
    "        except Exception:\n",
    "            pass\n",
    "        if app_df is not None and not app_df.empty:\n",
    "            try:\n",
    "                app_df.to_excel(xl, sheet_name=\"APP\", index=False, startrow=5)\n",
    "                ws2 = xl.sheets.get(\"APP\", None)\n",
    "                if ws2 is not None:\n",
    "                    ws2.freeze_panes = \"A6\"\n",
    "            except Exception:\n",
    "                try:\n",
    "                    app_df.to_excel(xl, sheet_name=\"APP\", index=False)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "def save_author_excel(author_id: str, author_name: str, df_articles: pd.DataFrame, out_dir: Path) -> str:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    path = out_dir / make_author_filename(author_name, author_id)\n",
    "    need = [\n",
    "        \"eid\",\"title\",\"year\",\"publication_name\",\"subtype\",\"doi\",\n",
    "        \"source_id\",\"issn_print\",\"issn_electronic\",\n",
    "        \"asjc_codes\",\"asjc_areas\",\"asjc_abbrevs\",\n",
    "        \"cs_percentile\",\"citescore\",\"quartile\",\n",
    "        \"authors_count\",\"combined\",\"abstract\"\n",
    "    ]\n",
    "    df = df_articles.copy()\n",
    "    for c in need:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.Series(dtype=\"object\")\n",
    "    df = df[need]\n",
    "\n",
    "    # Build APP breakdown & summary\n",
    "    app_df, app_summary = build_app_sheet(df)\n",
    "\n",
    "    try:\n",
    "        _write_excel_xlsxwriter(df, path, app_df=app_df if not app_df.empty else None, app_summary=app_summary)\n",
    "    except Exception:\n",
    "        _write_excel_openpyxl(df, path, app_df=app_df if not app_df.empty else None, app_summary=app_summary)\n",
    "    return str(path)\n",
    "\n",
    "# ----------------------- Orchestration --------------------------------\n",
    "\n",
    "def process_author(author_id: str, cs_table: pd.DataFrame, out_dir: Path, sleep: float, serial_sleep: float, cs_by_source: Optional[pd.DataFrame] = None) -> Optional[str]:\n",
    "    try:\n",
    "        name = get_author_name(author_id)\n",
    "        print(f\"→ AU-ID {author_id} — {name} (filtering by AFF_ID={AFF_ID})\")\n",
    "        eids = get_author_eids(author_id)\n",
    "        recs: List[Dict[str, Any]] = []\n",
    "        for eid in eids:\n",
    "            md = get_article_metadata(eid)\n",
    "            if md:\n",
    "                recs.append(md)\n",
    "            if sleep:\n",
    "                time.sleep(sleep)\n",
    "        df = pd.DataFrame(recs)\n",
    "\n",
    "        if not df.empty:\n",
    "            # build cs_by_source if not provided (backwards-compatible)\n",
    "            if cs_by_source is None:\n",
    "                cs_by_source = build_cs_by_source(cs_table, serial_sleep=serial_sleep)\n",
    "            df = enrich_with_citescore_sourceid_asjc(df, cs_table, cs_by_source)\n",
    "\n",
    "        path = save_author_excel(author_id, name, df if not df.empty else pd.DataFrame(), out_dir)\n",
    "        print(f\"   ✓ Wrote {Path(path).name}  ({0 if df is None else len(df)} rows)\")\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Failed {author_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ----------------------- Public API (Jupyter) -------------------------\n",
    "\n",
    "def run(\n",
    "    auids: Optional[str] = None,\n",
    "    citescore: Optional[str] = None,\n",
    "    outdir: str = \"authors\",\n",
    "    sleep: float = 0.05,\n",
    "    serial_sleep: float = 0.1,\n",
    "    no_prompt: bool = True,\n",
    "    aff_id: Optional[str] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Jupyter-friendly entrypoint. Pass aff_id to override the default AFF_ID filter.\n",
    "    \"\"\"\n",
    "    global AFF_ID\n",
    "    if aff_id:\n",
    "        AFF_ID = aff_id\n",
    "\n",
    "    out_dir = Path(outdir)\n",
    "    cs_path = resolve_citescore_path(citescore, no_prompt=no_prompt)\n",
    "    if not cs_path:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not locate CiteScore file. Pass citescore=..., set CITESCORE_CSV env var, or place the file in a default folder.\"\n",
    "        )\n",
    "    print(f\"Using CiteScore file: {cs_path}\")\n",
    "    cs_table = load_citescore_table(cs_path)\n",
    "\n",
    "    ids = read_auids_from_cli_or_file(auids)\n",
    "    if not ids:\n",
    "        print(\"No AU-IDs provided. Provide auids='555...,572...' or authors.txt file.\")\n",
    "        return\n",
    "    print(f\"CiteScore rows: {len(cs_table)} | Authors: {len(ids)} | Affiliation filter: {AFF_ID}\")\n",
    "\n",
    "    # Build cs_by_source once per run to avoid repeated SerialTitle lookups\n",
    "    cs_by_source = build_cs_by_source(cs_table, serial_sleep=serial_sleep)\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for i, auid in enumerate(ids, 1):\n",
    "        print(f\"[{i}/{len(ids)}]\")\n",
    "        process_author(auid, cs_table, out_dir, sleep=sleep, serial_sleep=serial_sleep, cs_by_source=cs_by_source)\n",
    "    print(f\"\\nDone. Files saved to: {out_dir.resolve()}\")\n",
    "\n",
    "# ----------------------- CLI -----------------------------------------\n",
    "\n",
    "def main(argv: Optional[list] = None):\n",
    "    global AFF_ID\n",
    "    ap = argparse.ArgumentParser(description=\"Fetch Scopus pubs by AU-ID and match CiteScore by Source ID then ASJC (ISSN fallback).\")\n",
    "    ap.add_argument(\"--auids\", type=str, default=None, help=\"Comma/space-separated Scopus Author IDs. If omitted, reads authors.txt\")\n",
    "    ap.add_argument(\"--citescore\", type=str, default=None, help=\"Path to CiteScore CSV/XLSX, or a folder containing it\")\n",
    "    ap.add_argument(\"--outdir\", type=str, default=\"authors\", help=\"Output directory (default: ./authors)\")\n",
    "    ap.add_argument(\"--sleep\", type=float, default=0.05, help=\"Sleep between EID fetches (seconds)\")\n",
    "    ap.add_argument(\"--serial-sleep\", type=float, default=0.1, help=\"Sleep between SerialTitle ISSN lookups (seconds)\")\n",
    "    ap.add_argument(\"--no-prompt\", action=\"store_true\", help=\"Do not prompt for missing CiteScore path; exit with error\")\n",
    "    ap.add_argument(\"--aff-id\", type=str, default=None, help=f\"Optional affiliation ID to filter publications by (default: {AFF_ID_DEFAULT})\")\n",
    "    args, _unknown = ap.parse_known_args(argv)\n",
    "\n",
    "    if args.aff_id:\n",
    "        AFF_ID = args.aff_id\n",
    "\n",
    "    cs_path = resolve_citescore_path(args.citescore, no_prompt=args.no_prompt)\n",
    "    if not cs_path:\n",
    "        if _is_ipython() and not args.no_prompt:\n",
    "            try:\n",
    "                entered = input(\"Enter CiteScore CSV/XLSX path or containing folder: \").strip('\"').strip()\n",
    "            except EOFError:\n",
    "                entered = \"\"\n",
    "            if entered:\n",
    "                cs_path = resolve_citescore_path(entered, no_prompt=True)\n",
    "        if not cs_path:\n",
    "            print(\"❌ Could not locate the CiteScore file. Pass --citescore or set CITESCORE_CSV.\")\n",
    "            if _is_ipython():\n",
    "                return\n",
    "            sys.exit(2)\n",
    "    print(f\"Using CiteScore file: {cs_path}\")\n",
    "    cs_table = load_citescore_table(cs_path)\n",
    "\n",
    "    auids = read_auids_from_cli_or_file(args.auids)\n",
    "    if not auids and _is_ipython():\n",
    "        try:\n",
    "            entered = input(\"Enter Scopus Author IDs (comma or space separated), or leave blank to cancel: \").strip()\n",
    "        except EOFError:\n",
    "            entered = \"\"\n",
    "        if entered:\n",
    "            auids = read_auids_from_cli_or_file(entered)\n",
    "    if not auids:\n",
    "        print(\"No AU-IDs provided. Use --auids, or create authors.txt with one AU-ID per line.\")\n",
    "        if _is_ipython():\n",
    "            return\n",
    "        sys.exit(0)\n",
    "\n",
    "    out_dir = Path(args.outdir)\n",
    "    print(f\"CiteScore rows: {len(cs_table)} | Authors: {len(auids)} | Out: {out_dir.resolve()} | Affiliation filter: {AFF_ID}\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Build cs_by_source once for the whole run\n",
    "    cs_by_source = build_cs_by_source(cs_table, serial_sleep=args.serial_sleep)\n",
    "\n",
    "    written = []\n",
    "    for i, auid in enumerate(auids, 1):\n",
    "        print(f\"[{i}/{len(auids)}] Processing AU-ID {auid} … (AFF_ID={AFF_ID})\")\n",
    "        p = process_author(auid, cs_table, out_dir, sleep=args.sleep, serial_sleep=args.serial_sleep, cs_by_source=cs_by_source)\n",
    "        if p:\n",
    "            written.append(p)\n",
    "\n",
    "    print(f\"\\nDone. Wrote {len(written)} file(s) to {out_dir.resolve()}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8903c-26a6-4111-975c-224bcdc77ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea91fe65-1e0e-46ac-8a35-6ec378789b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c63d550-6c5a-4360-9fba-b0aa2e29e0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d72bf6fe-9748-4816-8d26-19b7193a7f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CiteScore file: C:\\Users\\yusef.atteyih\\Desktop\\Academic Research Unit\\Yusef ATTEYIH\\Data Solutions\\Data Solutions 2.0\\APP Calculation\\CiteScore 2024\\CiteScore 2024 annual values.csv\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Scopus Author IDs (comma or space separated), or leave blank to cancel:  55042518500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CiteScore rows: 75679 | Authors: 1 | Out: C:\\Users\\yusef.atteyih\\Desktop\\Academic Research Unit\\Yusef ATTEYIH\\Data Solutions\\Data Solutions 2.0\\APP Calculation\\authors | Affiliation filter: 60021379\n",
      "[1/1] Processing AU-ID 55042518500 … (AFF_ID=60021379)\n",
      "→ AU-ID 55042518500 — Turker D. Kilic (AFF_ID filter: 60021379)\n",
      "   ✓ Wrote _turker_d_kilic___55042518500.xlsx  (72 rows)\n",
      "\n",
      "Done. Wrote 1 file(s) to C:\\Users\\yusef.atteyih\\Desktop\\Academic Research Unit\\Yusef ATTEYIH\\Data Solutions\\Data Solutions 2.0\\APP Calculation\\authors.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "r\"\"\"\n",
    "Scopus → CiteScore with disambiguation by **Source ID then ASJC (sub-subject area)**\n",
    "\n",
    "This version:\n",
    "• Stores ASJC sets as frozenset (hashable).\n",
    "• Matches publications to CiteScore by Source ID then ASJC (ISSN fallback).\n",
    "• Adds an affiliation-*detection and isolation* flow equivalent to the\n",
    "  logic used in the longer ETL script: per-article author→affiliation mapping\n",
    "  is parsed from the Scopus AbstractRetrieval results and each article is\n",
    "  kept for an author only if that author appears on the article *and* is\n",
    "  affiliated with the configured AFF_ID on that article.\n",
    "• Adds an \"APP\" sheet to the output Excel workbook (Participation Score\n",
    "  calculation). APP counts only journal **articles** (subtype == \"ar\")\n",
    "  within the last three calendar years (current year, -1, -2).\n",
    "• APP uses Author Coefficient (AC = 1.2 if sole author else 1.2 / n_authors)\n",
    "  and Quartile Coefficient (QC) mapped from CiteScore percentile,\n",
    "  Contribution = AC × QC, summed and rounded.\n",
    "\n",
    "Usage:\n",
    "  - In Jupyter: call run(auids=\"12345,67890\", citescore=\"path/to/CiteScore.csv\", aff_id=\"60021379\")\n",
    "  - CLI: python this_script.py --auids 57193254610 --citescore \"CiteScore 2024.csv\" --aff-id 60021379\n",
    "\n",
    "Requirements:\n",
    "  pybliometrics, pandas, xlsxwriter or openpyxl\n",
    "  Configure pybliometrics Scopus API credentials\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import unicodedata\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Set, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Scopus (pybliometrics) ---\n",
    "try:\n",
    "    from pybliometrics.scopus import AbstractRetrieval, AuthorRetrieval, ScopusSearch, SerialTitle\n",
    "    from pybliometrics.scopus.exception import ScopusException\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"pybliometrics is required. Install with `pip install pybliometrics` \"\n",
    "        \"and ensure your Scopus API credentials are configured.\"\n",
    "    ) from e\n",
    "\n",
    "# ----------------------- IPython detection ----------------------------\n",
    "\n",
    "def _is_ipython() -> bool:\n",
    "    return (\"ipykernel\" in sys.modules) or (\"IPython\" in sys.modules)\n",
    "\n",
    "# ----------------------- Defaults & Affiliation filter ----------------\n",
    "\n",
    "# Default affiliation id to filter publications by (Bahçeşehir University Scopus Affil ID)\n",
    "AFF_ID_DEFAULT = \"60021379\"\n",
    "AFF_ID: Optional[str] = AFF_ID_DEFAULT\n",
    "\n",
    "DEFAULT_USER_CITESCORE_DIRS = [\n",
    "    Path(r\"C:\\\\Users\\\\yusef.atteyih\\\\Desktop\\\\Academic Research Unit\\\\Yusef ATTEYIH\\\\Data Solutions\\\\Data Solutions 2.0\\\\APP Calculation\\\\CiteScore 2024\"),\n",
    "]\n",
    "DEFAULT_CITESCORE_DIRS = [Path(\"CiteScore 2024\"), Path(\".\")]\n",
    "POSSIBLE_CS_FILENAMES = [\n",
    "    \"CiteScore 2024 annual values.csv\",\n",
    "    \"CiteScore 2024 annual values.xlsx\",\n",
    "    \"CiteScore 2024.csv\",\n",
    "    \"citescore.csv\",\n",
    "    \"citescore.xlsx\",\n",
    "]\n",
    "\n",
    "# ----------------------- Small helpers --------------------------------\n",
    "\n",
    "def _s(x) -> str:\n",
    "    try:\n",
    "        if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return x if isinstance(x, str) else str(x)\n",
    "\n",
    "def _norm_issn(s: str) -> str:\n",
    "    \"\"\"Uppercase; keep 0-9 and 'X'; strip others.\"\"\"\n",
    "    return re.sub(r\"[^0-9X]\", \"\", _s(s).upper())\n",
    "\n",
    "def _norm_asjc_codes(raw: Any) -> Set[str]:\n",
    "    \"\"\"Return a set of 4-digit ASJC codes as strings.\"\"\"\n",
    "    out: Set[str] = set()\n",
    "    if raw is None:\n",
    "        return out\n",
    "    if isinstance(raw, (list, tuple, set)):\n",
    "        it = raw\n",
    "    else:\n",
    "        # split on non-digits; keep 4-digit tokens\n",
    "        it = re.split(r\"[^0-9]\", _s(raw))\n",
    "    for tok in it:\n",
    "        if tok and tok.isdigit():\n",
    "            if len(tok) == 4:\n",
    "                out.add(tok)\n",
    "            elif len(tok) > 4:\n",
    "                # sometimes concatenated; take last 4 digits\n",
    "                out.add(tok[-4:])\n",
    "    return out\n",
    "\n",
    "def _coerce_percentile(val) -> Optional[float]:\n",
    "    txt = _s(val).strip()\n",
    "    if not txt:\n",
    "        return None\n",
    "    txt = txt.replace(\"%\", \"\").replace(\" \", \"\").replace(\",\", \".\")\n",
    "    m = re.search(r\"[-+]?\\d*\\.?\\d+\", txt)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _coerce_float(val) -> Optional[float]:\n",
    "    txt = _s(val).strip().replace(\",\", \".\")\n",
    "    if not txt:\n",
    "        return None\n",
    "    m = re.search(r\"[-+]?\\d*\\.?\\d+\", txt)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def quartile_from_percentile(p: Optional[float]) -> str:\n",
    "    if p is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        p = float(p)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    if p >= 90: return \"QT\"\n",
    "    if p >= 75: return \"Q1\"\n",
    "    if p >= 50: return \"Q2\"\n",
    "    if p >= 25: return \"Q3\"\n",
    "    return \"Q4\"\n",
    "\n",
    "# ----------------------- Filenames ------------------------------------\n",
    "\n",
    "def _ascii_slug(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKD\", _s(s)).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^0-9a-z]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def make_author_filename(author_name: str, author_id: str) -> str:\n",
    "    name_slug = _ascii_slug(author_name)\n",
    "    leading = \"_\" + name_slug if not name_slug.startswith(\"_\") else name_slug\n",
    "    return f\"{leading}___{author_id}.xlsx\"\n",
    "\n",
    "# ----------------------- Read AU-IDs ----------------------------------\n",
    "\n",
    "author_id_file_candidates = (\"authors.txt\", \"auids.txt\", \"ScopusAuthorIDs.txt\")\n",
    "\n",
    "def read_auids_from_cli_or_file(cli_auids: Optional[str]) -> List[str]:\n",
    "    ids: List[str] = []\n",
    "    if cli_auids:\n",
    "        for tok in re.split(r\"[,\\s]+\", cli_auids.strip()):\n",
    "            if tok and tok.isdigit():\n",
    "                ids.append(tok)\n",
    "    if ids:\n",
    "        return sorted(set(ids))\n",
    "    for fname in author_id_file_candidates:\n",
    "        p = Path(fname)\n",
    "        if p.exists():\n",
    "            try:\n",
    "                raw = p.read_text(encoding=\"utf-8\")\n",
    "            except Exception:\n",
    "                raw = p.read_text(errors=\"ignore\")\n",
    "            for line in raw.splitlines():\n",
    "                t = line.split(\"#\", 1)[0].strip()\n",
    "                if t.isdigit():\n",
    "                    ids.append(t)\n",
    "            if ids:\n",
    "                return sorted(set(ids))\n",
    "    return []\n",
    "\n",
    "# ----------------------- CiteScore path --------------------------------\n",
    "\n",
    "def _candidate_files_in_dir(d: Path) -> List[Path]:\n",
    "    return [p for name in POSSIBLE_CS_FILENAMES if (p := d / name).exists() and p.is_file()]\n",
    "\n",
    "def resolve_citescore_path(arg: Optional[str], no_prompt: bool = False) -> Optional[Path]:\n",
    "    if arg:\n",
    "        p = Path(arg)\n",
    "        if p.exists():\n",
    "            if p.is_file(): return p\n",
    "            if p.is_dir():\n",
    "                cand = _candidate_files_in_dir(p)\n",
    "                if cand: return cand[0]\n",
    "    envp = os.environ.get(\"CITESCORE_CSV\")\n",
    "    if envp:\n",
    "        p = Path(envp)\n",
    "        if p.exists() and p.is_file(): return p\n",
    "    for d in DEFAULT_USER_CITESCORE_DIRS + DEFAULT_CITESCORE_DIRS:\n",
    "        if d.exists() and d.is_dir():\n",
    "            cand = _candidate_files_in_dir(d)\n",
    "            if cand: return cand[0]\n",
    "    if not no_prompt:\n",
    "        try:\n",
    "            path_in = input(\"Path to CiteScore CSV/XLSX (or folder containing it): \").strip('\"').strip()\n",
    "            if path_in:\n",
    "                p = Path(path_in)\n",
    "                if p.exists():\n",
    "                    if p.is_file(): return p\n",
    "                    if p.is_dir():\n",
    "                        cand = _candidate_files_in_dir(p)\n",
    "                        if cand: return cand[0]\n",
    "        except EOFError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# ----------------------- Read CiteScore table --------------------------\n",
    "\n",
    "def robust_read_table(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"CiteScore file not found: {path}\")\n",
    "    suffix = path.suffix.lower()\n",
    "    if suffix in (\".xlsx\", \".xls\"):\n",
    "        return pd.read_excel(path)\n",
    "    encodings = [\"utf-8-sig\", \"utf-16\", \"utf-16le\", \"utf-16be\", \"cp1254\", \"iso-8859-9\", \"cp1252\", \"latin1\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, engine=\"python\", sep=None)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding=\"latin1\", engine=\"python\", sep=None, on_bad_lines=\"skip\")\n",
    "    except TypeError:\n",
    "        return pd.read_csv(path, encoding=\"latin1\", engine=\"python\", sep=None, error_bad_lines=False)  # type: ignore\n",
    "\n",
    "def load_citescore_table(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return columns:\n",
    "      source_id (if present), issn_key, eissn_key, asjc_set (frozenset), cs_percentile, citescore\n",
    "    Accepts flexible column names (case-insensitive)\n",
    "    \"\"\"\n",
    "    cs = robust_read_table(path)\n",
    "    norm = {c.strip().lower(): c for c in cs.columns}\n",
    "\n",
    "    p = norm.get(\"print issn\") or norm.get(\"p-issn\") or norm.get(\"issn\")\n",
    "    e = norm.get(\"e-issn\") or norm.get(\"eissn\")\n",
    "    pct = norm.get(\"percentile\") or norm.get(\"citescore percentile\")\n",
    "    val = norm.get(\"citescore\") or norm.get(\"citescore 2024\")\n",
    "    src = norm.get(\"source id\") or norm.get(\"scopus source id\") or norm.get(\"scopus sourceid\")\n",
    "    asjc_col = norm.get(\"asjc\") or norm.get(\"asjc code\") or norm.get(\"asjc codes\") or norm.get(\"subject area asjc\")\n",
    "\n",
    "    if not all([p, e, pct, val]):\n",
    "        raise KeyError(\n",
    "            f\"CiteScore table must include 'Print ISSN', 'E-ISSN', 'Percentile', 'CiteScore'. Found: {list(cs.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Standardize names\n",
    "    cs = cs.rename(columns={p: \"print_issn\", e: \"e_issn\", pct: \"cs_percentile\", val: \"citescore\"})\n",
    "    if src:\n",
    "        cs = cs.rename(columns={src: \"source_id\"})\n",
    "    if asjc_col:\n",
    "        cs = cs.rename(columns={asjc_col: \"asjc_raw\"})\n",
    "    else:\n",
    "        cs[\"asjc_raw\"] = \"\"\n",
    "\n",
    "    # Coerce types\n",
    "    cs[\"print_issn\"] = cs[\"print_issn\"].astype(str)\n",
    "    cs[\"e_issn\"] = cs[\"e_issn\"].astype(str)\n",
    "    cs[\"cs_percentile\"] = cs[\"cs_percentile\"].apply(_coerce_percentile)\n",
    "    cs[\"citescore\"] = cs[\"citescore\"].apply(_coerce_float)\n",
    "    if \"source_id\" in cs.columns:\n",
    "        cs[\"source_id\"] = cs[\"source_id\"].astype(str).str.extract(r\"(\\d+)\", expand=False).fillna(\"\")\n",
    "\n",
    "    # Normalized keys\n",
    "    cs[\"issn_key\"] = cs[\"print_issn\"].map(_norm_issn)\n",
    "    cs[\"eissn_key\"] = cs[\"e_issn\"].map(_norm_issn)\n",
    "\n",
    "    # ASJC as frozenset (hashable)\n",
    "    cs[\"asjc_set\"] = cs[\"asjc_raw\"].apply(lambda v: frozenset(_norm_asjc_codes(v)))\n",
    "\n",
    "    # Keep minimal columns\n",
    "    keep = [\"issn_key\", \"eissn_key\", \"asjc_set\", \"cs_percentile\", \"citescore\"]\n",
    "    if \"source_id\" in cs.columns:\n",
    "        keep.insert(0, \"source_id\")\n",
    "    cs = cs[keep]\n",
    "    return cs\n",
    "\n",
    "# ----------------------- Scopus helpers --------------------------------\n",
    "\n",
    "def _extract_issns(ar: Any) -> Tuple[str, str]:\n",
    "    p = \"\"; e = \"\"\n",
    "    if hasattr(ar, \"eIssn\"):\n",
    "        e = _s(getattr(ar, \"eIssn\"))\n",
    "    if not e and hasattr(ar, \"e_issn\"):\n",
    "        e = _s(getattr(ar, \"e_issn\"))\n",
    "    if hasattr(ar, \"issn\"):\n",
    "        obj = getattr(ar, \"issn\")\n",
    "        if isinstance(obj, str):\n",
    "            if \"ISSN(\" in obj:\n",
    "                mp = re.search(r\"print\\s*=\\s*'([^']+)'\", obj)\n",
    "                me = re.search(r\"electronic\\s*=\\s*'([^']+)'\", obj)\n",
    "                if mp: p = mp.group(1)\n",
    "                if me and not e: e = me.group(1)\n",
    "            else:\n",
    "                p = _s(obj)\n",
    "        else:\n",
    "            try:\n",
    "                p_obj = getattr(obj, \"print\", \"\"); e_obj = getattr(obj, \"electronic\", \"\")\n",
    "                if p_obj: p = _s(p_obj)\n",
    "                if not e and e_obj: e = _s(e_obj)\n",
    "            except Exception:\n",
    "                txt = _s(obj)\n",
    "                mp = re.search(r\"print\\s*=\\s*'([^']+)'\", txt)\n",
    "                me = re.search(r\"electronic\\s*=\\s*'([^']+)'\", txt)\n",
    "                if mp: p = mp.group(1)\n",
    "                if me and not e: e = me.group(1)\n",
    "    return p, e\n",
    "\n",
    "\n",
    "def _extract_asjc(ar: Any) -> Tuple[str, str, str, Set[str]]:\n",
    "    codes: Set[str] = set(); areas: Set[str] = set(); abbrevs: Set[str] = set()\n",
    "    sa = getattr(ar, \"subject_areas\", None)\n",
    "    if sa:\n",
    "        try:\n",
    "            for it in sa:\n",
    "                c = getattr(it, \"code\", None)\n",
    "                a = getattr(it, \"area\", None)\n",
    "                ab = getattr(it, \"abbrev\", None)\n",
    "                if c is not None:\n",
    "                    codes.add(f\"{int(c):04d}\" if str(c).isdigit() else str(c))\n",
    "                if a: areas.add(_s(a))\n",
    "                if ab: abbrevs.add(_s(ab))\n",
    "        except Exception:\n",
    "            try:  # dict-like\n",
    "                for it in sa:\n",
    "                    c = it.get(\"code\"); a = it.get(\"area\"); ab = it.get(\"abbrev\")\n",
    "                    if c is not None:\n",
    "                        codes.add(f\"{int(c):04d}\" if str(c).isdigit() else str(c))\n",
    "                    if a: areas.add(_s(a))\n",
    "                    if ab: abbrevs.add(_s(ab))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return \", \".join(sorted(codes)), \", \".join(sorted(areas)), \", \".join(sorted(abbrevs)), codes\n",
    "\n",
    "# ----------------------- ISSN → Source ID (optional) -------------------\n",
    "\n",
    "def fetch_source_id_for_issn(issn: str) -> Optional[str]:\n",
    "    issn = _norm_issn(issn)\n",
    "    if not issn:\n",
    "        return None\n",
    "    try:\n",
    "        res = SerialTitle(issn)\n",
    "        items: Iterable[Any]\n",
    "        try:\n",
    "            items = list(res) if isinstance(res, (list, tuple)) else [res]\n",
    "        except Exception:\n",
    "            items = [res]\n",
    "        for it in items:\n",
    "            for attr in (\"source_id\", \"sourcerecord_id\", \"sourceid\"):\n",
    "                if hasattr(it, attr):\n",
    "                    sid = _s(getattr(it, attr))\n",
    "                    if sid.isdigit():\n",
    "                        return sid\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"SerialTitle lookup failed for ISSN {issn}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_cs_by_source(cs_table: pd.DataFrame, serial_sleep: float = 0.1) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a cs_by_source table with 'source_id' present, mapping from cs_table.\n",
    "    If cs_table already has source_id, just dedupe and return.\n",
    "    Otherwise, look up source_id from ISSN/e-ISSN via SerialTitle.\n",
    "    \"\"\"\n",
    "    if \"source_id\" in cs_table.columns:\n",
    "        df = cs_table.copy()\n",
    "    else:\n",
    "        df = cs_table.copy()\n",
    "        # NOTE: These are vectorized calls; add throttling inside fetch if needed.\n",
    "        df[\"source_id_from_print\"] = df[\"issn_key\"].apply(fetch_source_id_for_issn)\n",
    "        if serial_sleep:\n",
    "            time.sleep(serial_sleep)\n",
    "        df[\"source_id_from_e\"] = df[\"eissn_key\"].apply(fetch_source_id_for_issn)\n",
    "        df[\"source_id\"] = df[\"source_id_from_print\"].where(df[\"source_id_from_print\"].notna(), df[\"source_id_from_e\"])\n",
    "        df = df.drop(columns=[\"source_id_from_print\", \"source_id_from_e\"], errors=\"ignore\")\n",
    "    # Keep only rows with a source_id\n",
    "    df = df[df[\"source_id\"].astype(str).str.strip().ne(\"\")].copy()\n",
    "    # Deduplicate by (source_id, asjc_set); asjc_set is frozenset → hashable\n",
    "    df = df.drop_duplicates(subset=[\"source_id\", \"asjc_set\"], keep=\"first\")\n",
    "    return df[[\"source_id\", \"asjc_set\", \"cs_percentile\", \"citescore\"]]\n",
    "\n",
    "# ----------------------- Matching logic --------------------------------\n",
    "\n",
    "def _pick_best_candidate(cands: pd.DataFrame, article_asjc: Set[str]) -> Tuple[Optional[float], Optional[float]]:\n",
    "    \"\"\"Choose best row: prefer ASJC overlap, then highest percentile, else first.\n",
    "    Returns (cs_percentile, citescore).\n",
    "    \"\"\"\n",
    "    if cands is None or cands.empty:\n",
    "        return None, None\n",
    "    # compute overlap count\n",
    "    over = []\n",
    "    for _i, row in cands.iterrows():\n",
    "        cs_set = row.get(\"asjc_set\") or set()\n",
    "        # Accept frozenset/list/tuple/set/string\n",
    "        if isinstance(cs_set, (set, frozenset)):\n",
    "            cs_set2 = set(cs_set)\n",
    "        elif isinstance(cs_set, (list, tuple)):\n",
    "            cs_set2 = {str(x) for x in cs_set}\n",
    "        else:\n",
    "            cs_set2 = _norm_asjc_codes(cs_set)\n",
    "        over.append(len(article_asjc & cs_set2))\n",
    "    cands = cands.copy()\n",
    "    cands[\"_overlap\"] = over\n",
    "    # sort by: overlap desc, percentile desc (None last)\n",
    "    cands[\"_pct\"] = cands[\"cs_percentile\"].fillna(-1e9)\n",
    "    cands = cands.sort_values([\"_overlap\", \"_pct\"], ascending=[False, False])\n",
    "    top = cands.iloc[0]\n",
    "    return top.get(\"cs_percentile\"), top.get(\"citescore\")\n",
    "\n",
    "\n",
    "def enrich_with_citescore_sourceid_asjc(\n",
    "    df_articles: pd.DataFrame,\n",
    "    cs_table: pd.DataFrame,\n",
    "    cs_by_source: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    if df_articles is None or df_articles.empty:\n",
    "        return df_articles.copy()\n",
    "\n",
    "    df = df_articles.copy()\n",
    "    for col in (\"issn_print\", \"issn_electronic\", \"source_id\"):\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "\n",
    "    # Prepare article keys\n",
    "    df[\"issn_key\"] = df[\"issn_print\"].astype(str).map(_norm_issn)\n",
    "    df[\"eissn_key\"] = df[\"issn_electronic\"].astype(str).map(_norm_issn)\n",
    "\n",
    "    # Pre-split article ASJC sets\n",
    "    a_asjc_sets: List[Set[str]] = []\n",
    "    for v in df.get(\"asjc_codes\", pd.Series([\"\"] * len(df))):\n",
    "        a_asjc_sets.append(_norm_asjc_codes(v))\n",
    "\n",
    "    cs_p = cs_table[[\"issn_key\", \"asjc_set\", \"cs_percentile\", \"citescore\"]].drop_duplicates()\n",
    "    cs_e = cs_table[[\"eissn_key\", \"asjc_set\", \"cs_percentile\", \"citescore\"]].drop_duplicates()\n",
    "\n",
    "    out_pct: List[Optional[float]] = []\n",
    "    out_val: List[Optional[float]] = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        article_asjc = a_asjc_sets[idx] if idx < len(a_asjc_sets) else set()\n",
    "        sid = _s(row.get(\"source_id\"))\n",
    "        pct = None; val = None\n",
    "        # 1) try source_id\n",
    "        if sid:\n",
    "            cands = cs_by_source[cs_by_source[\"source_id\"].astype(str) == sid]\n",
    "            pct, val = _pick_best_candidate(cands, article_asjc)\n",
    "        # 2) fallback by ISSN\n",
    "        if pct is None and val is None:\n",
    "            issn = _s(row.get(\"issn_key\"))\n",
    "            eissn = _s(row.get(\"eissn_key\"))\n",
    "            cands = pd.concat([\n",
    "                cs_p[cs_p[\"issn_key\"] == issn],\n",
    "                cs_e[cs_e[\"eissn_key\"] == eissn],\n",
    "            ], ignore_index=True)\n",
    "            pct, val = _pick_best_candidate(cands, article_asjc)\n",
    "        out_pct.append(pct)\n",
    "        out_val.append(val)\n",
    "\n",
    "    df[\"cs_percentile\"] = out_pct\n",
    "    df[\"citescore\"] = out_val\n",
    "    df[\"quartile\"] = df[\"cs_percentile\"].apply(quartile_from_percentile)\n",
    "    return df\n",
    "\n",
    "# ----------------------- Scopus collectors (with per-author aff detection) -------\n",
    "\n",
    "def get_author_name(author_id: str) -> str:\n",
    "    try:\n",
    "        ar = AuthorRetrieval(author_id)\n",
    "        name = f\"{_s(ar.given_name)} {_s(ar.surname)}\".strip()\n",
    "        return name or author_id\n",
    "    except Exception:\n",
    "        return author_id\n",
    "\n",
    "def get_author_eids(author_id: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Return EIDs for the given author. We fetch by AU-ID only (no AF-ID here)\n",
    "    and then perform per-article per-author affiliation detection so we can\n",
    "    precisely determine whether the *author on that paper* was affiliated with\n",
    "    AFF_ID on that particular record.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = ScopusSearch(f\"AU-ID({author_id})\", subscriber=True)\n",
    "        return s.get_eids() or []\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"ScopusSearch failed for AU-ID({author_id}): {e}\")\n",
    "        return []\n",
    "\n",
    "def _safe_int(v) -> Optional[int]:\n",
    "    try:\n",
    "        return int(v)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_article_metadata(eid: str, target_auid: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Return article metadata dict. Also detect whether the given target_auid\n",
    "    appears in the article and whether that author entry has affiliation_id == AFF_ID.\n",
    "    This replicates the per-author affiliation isolation logic used in the longer ETL script.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ar = AbstractRetrieval(eid, view=\"FULL\")\n",
    "    except ScopusException as e:\n",
    "        warnings.warn(f\"AbstractRetrieval failed for {eid}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # keep only articles and reviews for general collection; APP will only count subtype == 'ar'\n",
    "    if ar.subtype not in (\"ar\", \"re\"):\n",
    "        return None\n",
    "\n",
    "    year = \"\"\n",
    "    if getattr(ar, \"coverDate\", None):\n",
    "        year = _s(ar.coverDate)[:4]\n",
    "\n",
    "    issn_print, issn_elec = _extract_issns(ar)\n",
    "    asjc_codes_csv, asjc_areas_csv, asjc_abbrevs_csv, _codes_set = _extract_asjc(ar)\n",
    "\n",
    "    # Build author-affiliation structures from authorgroup where available\n",
    "    bau_flag = False  # whether target_auid is present with AFF_ID on this paper\n",
    "    bau_orgs = []     # organizations for bau-affiliated author(s)\n",
    "    auid_org_map: Dict[str, str] = {}\n",
    "    auth_err = None\n",
    "    groups = getattr(ar, \"authorgroup\", None)\n",
    "    if groups:\n",
    "        try:\n",
    "            for g in groups:\n",
    "                try:\n",
    "                    g_auid = str(getattr(g, \"auid\", \"\") or \"\")\n",
    "                    org_val = getattr(g, \"organization\", None)\n",
    "                    aff_id_val = _safe_int(getattr(g, \"affiliation_id\", None))\n",
    "                    if g_auid:\n",
    "                        auid_org_map[g_auid] = _s(org_val)\n",
    "                    # if this group entry belongs to our target author and affiliation id matches AFF_ID\n",
    "                    if target_auid and g_auid == str(target_auid) and AFF_ID and aff_id_val == _safe_int(AFF_ID):\n",
    "                        bau_flag = True\n",
    "                        if org_val:\n",
    "                            bau_orgs.append(_s(org_val))\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception as ex:\n",
    "            # fallback: try authors list mapping if authorgroup parsing failed\n",
    "            auth_err = str(ex)\n",
    "    else:\n",
    "        # authorgroup not present — try to use 'authors' entries and affiliations\n",
    "        try:\n",
    "            authors = getattr(ar, \"authors\", None) or []\n",
    "            for a in authors:\n",
    "                try:\n",
    "                    g_auid = str(getattr(a, \"auid\", \"\") or \"\")\n",
    "                    # some authors may have affiliation info inline\n",
    "                    org_val = getattr(a, \"orgname\", None) or getattr(a, \"affiliation\", None) or \"\"\n",
    "                    if g_auid:\n",
    "                        auid_org_map[g_auid] = _s(org_val)\n",
    "                    # affiliation id may not be present here — can't reliably detect AFF_ID\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Additional: collect affiliation list for the record (for collaboration detection)\n",
    "    insts = getattr(ar, \"affiliation\", None) or []\n",
    "\n",
    "    return {\n",
    "        \"eid\": _s(eid),\n",
    "        \"title\": _s(ar.title),\n",
    "        \"year\": _s(year),\n",
    "        \"publication_name\": _s(getattr(ar, \"publicationName\", \"\")),\n",
    "        \"subtype\": _s(ar.subtype),\n",
    "        \"doi\": _s(getattr(ar, \"doi\", \"\")),\n",
    "        \"source_id\": _s(getattr(ar, \"source_id\", \"\")),\n",
    "        \"issn_print\": _s(issn_print),\n",
    "        \"issn_electronic\": _s(issn_elec),\n",
    "        \"asjc_codes\": asjc_codes_csv,\n",
    "        \"asjc_areas\": asjc_areas_csv,\n",
    "        \"asjc_abbrevs\": asjc_abbrevs_csv,\n",
    "        \"authors_count\": len(ar.authors) if getattr(ar, \"authors\", None) else 1,\n",
    "        \"combined\": \"; \".join([t for t in (getattr(ar, \"authkeywords\", []) or []) if _s(t)]),\n",
    "        \"abstract\": _s(getattr(ar, \"description\", \"\")),\n",
    "        # affiliation-detection fields:\n",
    "        \"author_org_map_json\": json.dumps(auid_org_map, ensure_ascii=False),\n",
    "        \"is_bau_author\": bau_flag,\n",
    "        \"bau_author_orgs\": \"; \".join(dict.fromkeys(bau_orgs)),\n",
    "        \"affiliations_list\": [(getattr(i, \"name\", \"\") if hasattr(i, \"name\") else _s(i)) for i in insts],\n",
    "        \"auth_group_error\": auth_err,\n",
    "    }\n",
    "\n",
    "# ----------------------- APP calculation helpers -----------------------\n",
    "\n",
    "def _qc_from_percentile(p: Optional[float]) -> Optional[float]:\n",
    "    \"\"\"QC mapping assuming higher percentile is better (>=90 is Top 10%).\"\"\"\n",
    "    if p is None:\n",
    "        return None\n",
    "    try:\n",
    "        p = float(p)\n",
    "    except Exception:\n",
    "        return None\n",
    "    if p >= 90:   # Top 10%\n",
    "        return 1.4\n",
    "    if p >= 75:   # Q1\n",
    "        return 1.0\n",
    "    if p >= 50:   # Q2\n",
    "        return 0.8\n",
    "    if p >= 25:   # Q3\n",
    "        return 0.6\n",
    "    if p >= 0:    # Q4\n",
    "        return 0.4\n",
    "    return None\n",
    "\n",
    "def _ac_from_authors(n: Any) -> float:\n",
    "    \"\"\"Author Coefficient: 1.2 if single author else 1.2 / n_authors.\"\"\"\n",
    "    try:\n",
    "        n = int(n)\n",
    "    except Exception:\n",
    "        n = 1\n",
    "    return 1.2 if n <= 1 else 1.2 / max(n, 1)\n",
    "\n",
    "def _to_int_year(y: Any) -> Optional[int]:\n",
    "    try:\n",
    "        s = str(y)\n",
    "        m = re.search(r\"\\d{4}\", s)\n",
    "        return int(m.group(0)) if m else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def build_app_sheet(df_articles: pd.DataFrame, now_year: Optional[int] = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Returns (df_app, summary) where df_app contains the per-paper APP breakdown and\n",
    "    summary has 'app_total' and 'eligibility' text.\n",
    "\n",
    "    NOTE: APP is calculated only for journal articles with subtype == \"ar\".\n",
    "    \"\"\"\n",
    "    if df_articles is None or df_articles.empty:\n",
    "        return pd.DataFrame(), {\"app_total\": 0.0, \"eligibility\": \"No eligible items\", \"years\": []}\n",
    "\n",
    "    # Determine the 3-year window (includes current year)\n",
    "    cy = now_year or datetime.now().year\n",
    "    years_ok = {cy, cy - 1, cy - 2}\n",
    "\n",
    "    # Coerce needed fields\n",
    "    tmp = df_articles.copy()\n",
    "    tmp[\"year_i\"] = tmp.get(\"year\", \"\").apply(_to_int_year)\n",
    "    tmp[\"cs_percentile_num\"] = pd.to_numeric(tmp.get(\"cs_percentile\"), errors=\"coerce\")\n",
    "    tmp[\"authors_count_i\"] = pd.to_numeric(tmp.get(\"authors_count\"), errors=\"coerce\").fillna(1).astype(int)\n",
    "    tmp[\"subtype_norm\"] = tmp.get(\"subtype\", \"\").astype(str).str.lower()\n",
    "\n",
    "    # Eligibility: last 3 calendar years, subtype == \"ar\" only, has percentile\n",
    "    eligible = tmp[\n",
    "        tmp[\"year_i\"].isin(years_ok) &\n",
    "        tmp[\"cs_percentile_num\"].notna() &\n",
    "        (tmp[\"subtype_norm\"] == \"ar\")\n",
    "    ].copy()\n",
    "\n",
    "    if eligible.empty:\n",
    "        return pd.DataFrame(), {\"app_total\": 0.0, \"eligibility\": \"No eligible items (no 'ar' articles in window)\", \"years\": sorted(years_ok)}\n",
    "\n",
    "    # Compute QC, AC, Contribution\n",
    "    eligible[\"QC\"] = eligible[\"cs_percentile_num\"].apply(_qc_from_percentile)\n",
    "    eligible[\"AC\"] = eligible[\"authors_count_i\"].apply(_ac_from_authors)\n",
    "    # If QC missing after mapping, treat as ineligible\n",
    "    eligible = eligible[eligible[\"QC\"].notna()].copy()\n",
    "    if eligible.empty:\n",
    "        return pd.DataFrame(), {\"app_total\": 0.0, \"eligibility\": \"No eligible items (QC missing)\", \"years\": sorted(years_ok)}\n",
    "\n",
    "    eligible[\"Contribution\"] = eligible[\"AC\"] * eligible[\"QC\"]\n",
    "\n",
    "    # Round to 2 decimals per policy\n",
    "    eligible[\"AC\"] = eligible[\"AC\"].round(2)\n",
    "    eligible[\"QC\"] = eligible[\"QC\"].round(2)\n",
    "    eligible[\"Contribution\"] = eligible[\"Contribution\"].round(2)\n",
    "\n",
    "    # Select user-friendly columns\n",
    "    out_cols = [\n",
    "        \"eid\", \"title\", \"year\", \"publication_name\",\n",
    "        \"authors_count\", \"cs_percentile\", \"quartile\",\n",
    "        \"AC\", \"QC\", \"Contribution\"\n",
    "    ]\n",
    "    for c in out_cols:\n",
    "        if c not in eligible.columns:\n",
    "            eligible[c] = pd.Series(dtype=\"object\")\n",
    "    df_app = eligible[out_cols].sort_values([\"year\", \"Contribution\"], ascending=[False, False]).reset_index(drop=True)\n",
    "\n",
    "    app_total = float(df_app[\"Contribution\"].sum().round(2))\n",
    "\n",
    "    # Eligibility band (based on your table)\n",
    "    if app_total > 1.0:\n",
    "        elig = \"APP > 1.0 → up to 2 supports / AY (only 1 requires full indexing & APP check)\"\n",
    "    elif app_total >= 0.4:\n",
    "        elig = \"0.4 ≤ APP ≤ 1.0 → 1 support / AY\"\n",
    "    else:\n",
    "        elig = \"APP < 0.4 → 1 support / AY (if other criteria met)\"\n",
    "\n",
    "    summary = {\"app_total\": round(app_total, 2), \"eligibility\": elig, \"years\": sorted(years_ok)}\n",
    "    return df_app, summary\n",
    "\n",
    "# ----------------------- Excel writer (with APP sheet) -----------------\n",
    "\n",
    "def _write_excel_xlsxwriter(df: pd.DataFrame, path: Path, app_df: Optional[pd.DataFrame] = None, app_summary: Optional[Dict[str, Any]] = None):\n",
    "    with pd.ExcelWriter(path, engine=\"xlsxwriter\") as xl:\n",
    "        df.to_excel(xl, sheet_name=\"articles\", index=False)\n",
    "        ws = xl.sheets[\"articles\"]\n",
    "        try:\n",
    "            ws.freeze_panes(1, 0)\n",
    "            ws.set_column(\"A:A\", 20)  # eid\n",
    "            ws.set_column(\"B:B\", 50)  # title\n",
    "            ws.set_column(\"C:C\", 8)   # year\n",
    "            ws.set_column(\"D:D\", 36)  # publication_name\n",
    "            ws.set_column(\"E:E\", 8)   # subtype\n",
    "            ws.set_column(\"F:F\", 26)  # doi\n",
    "            ws.set_column(\"G:G\", 14)  # source_id\n",
    "            ws.set_column(\"H:I\", 14)  # ISSNs\n",
    "            ws.set_column(\"J:J\", 18)  # asjc_codes\n",
    "            ws.set_column(\"K:K\", 26)  # asjc_areas\n",
    "            ws.set_column(\"L:L\", 14)  # asjc_abbrevs\n",
    "            ws.set_column(\"M:N\", 14)  # cs_percentile / citescore\n",
    "            ws.set_column(\"O:O\", 10)  # quartile\n",
    "            ws.set_column(\"P:P\", 12)  # authors_count\n",
    "            ws.set_column(\"Q:Q\", 28)  # combined\n",
    "            ws.set_column(\"R:R\", 80)  # abstract\n",
    "            # extra affiliation columns\n",
    "            ws.set_column(\"S:S\", 12)  # is_bau_author\n",
    "            ws.set_column(\"T:T\", 28)  # bau_author_orgs\n",
    "            ws.set_column(\"U:U\", 36)  # author_org_map_json\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Sheet 2: APP (optional)\n",
    "        if app_df is not None and not app_df.empty:\n",
    "            # write header and summary first (we'll write table starting at row 6)\n",
    "            ws2 = xl.book.add_worksheet(\"APP\")\n",
    "            xl.sheets[\"APP\"] = ws2  # ensure mapping exists\n",
    "            try:\n",
    "                ws2.write(0, 0, \"APP calculation — last 3 calendar years (journal articles only; subtype == 'ar')\")\n",
    "                if app_summary:\n",
    "                    ws2.write(1, 0, \"Years considered\")\n",
    "                    ws2.write(1, 1, \", \".join(str(y) for y in app_summary.get(\"years\", [])))\n",
    "                    ws2.write(2, 0, \"APP Score\")\n",
    "                    ws2.write(2, 1, app_summary.get(\"app_total\", 0.0))\n",
    "                    ws2.write(3, 0, \"Eligibility\")\n",
    "                    ws2.write(3, 1, app_summary.get(\"eligibility\", \"\"))\n",
    "                # write the dataframe starting at row 6 (index 5) to leave space for header\n",
    "                (app_df.reset_index(drop=True)).to_excel(xl, sheet_name=\"APP\", index=False, startrow=5)\n",
    "                # adjust columns\n",
    "                ws2.freeze_panes(6, 0)\n",
    "                ws2.set_column(\"A:A\", 20)  # eid\n",
    "                ws2.set_column(\"B:B\", 60)  # title\n",
    "                ws2.set_column(\"C:C\", 8)   # year\n",
    "                ws2.set_column(\"D:D\", 36)  # journal\n",
    "                ws2.set_column(\"E:E\", 12)  # authors_count\n",
    "                ws2.set_column(\"F:F\", 14)  # cs_percentile\n",
    "                ws2.set_column(\"G:G\", 10)  # quartile\n",
    "                ws2.set_column(\"H:J\", 14)  # AC, QC, Contribution\n",
    "            except Exception:\n",
    "                # Fallback: write simply by pandas\n",
    "                try:\n",
    "                    app_df.to_excel(xl, sheet_name=\"APP\", index=False)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "def _write_excel_openpyxl(df: pd.DataFrame, path: Path, app_df: Optional[pd.DataFrame] = None, app_summary: Optional[Dict[str, Any]] = None):\n",
    "    with pd.ExcelWriter(path, engine=\"openpyxl\") as xl:\n",
    "        df.to_excel(xl, sheet_name=\"articles\", index=False)\n",
    "        try:\n",
    "            ws = xl.sheets[\"articles\"]\n",
    "            ws.freeze_panes = \"A2\"\n",
    "        except Exception:\n",
    "            pass\n",
    "        if app_df is not None and not app_df.empty:\n",
    "            try:\n",
    "                app_df.to_excel(xl, sheet_name=\"APP\", index=False, startrow=5)\n",
    "                ws2 = xl.sheets.get(\"APP\", None)\n",
    "                if ws2 is not None:\n",
    "                    ws2.freeze_panes = \"A6\"\n",
    "            except Exception:\n",
    "                try:\n",
    "                    app_df.to_excel(xl, sheet_name=\"APP\", index=False)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "def save_author_excel(author_id: str, author_name: str, df_articles: pd.DataFrame, out_dir: Path) -> str:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    path = out_dir / make_author_filename(author_name, author_id)\n",
    "    need = [\n",
    "        \"eid\",\"title\",\"year\",\"publication_name\",\"subtype\",\"doi\",\n",
    "        \"source_id\",\"issn_print\",\"issn_electronic\",\n",
    "        \"asjc_codes\",\"asjc_areas\",\"asjc_abbrevs\",\n",
    "        \"cs_percentile\",\"citescore\",\"quartile\",\n",
    "        \"authors_count\",\"combined\",\"abstract\",\n",
    "        # affiliation-specific\n",
    "        \"is_bau_author\",\"bau_author_orgs\",\"author_org_map_json\"\n",
    "    ]\n",
    "    df = df_articles.copy()\n",
    "    for c in need:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.Series(dtype=\"object\")\n",
    "    df = df[need]\n",
    "\n",
    "    # Build APP breakdown & summary (APP counts only subtype == 'ar')\n",
    "    app_df, app_summary = build_app_sheet(df)\n",
    "\n",
    "    try:\n",
    "        _write_excel_xlsxwriter(df, path, app_df=app_df if not app_df.empty else None, app_summary=app_summary)\n",
    "    except Exception:\n",
    "        _write_excel_openpyxl(df, path, app_df=app_df if not app_df.empty else None, app_summary=app_summary)\n",
    "    return str(path)\n",
    "\n",
    "# ----------------------- Orchestration --------------------------------\n",
    "\n",
    "def process_author(author_id: str, cs_table: pd.DataFrame, out_dir: Path, sleep: float, serial_sleep: float, cs_by_source: Optional[pd.DataFrame] = None) -> Optional[str]:\n",
    "    try:\n",
    "        name = get_author_name(author_id)\n",
    "        print(f\"→ AU-ID {author_id} — {name} (AFF_ID filter: {AFF_ID})\")\n",
    "        eids = get_author_eids(author_id)\n",
    "        recs: List[Dict[str, Any]] = []\n",
    "        for eid in eids:\n",
    "            md = get_article_metadata(eid, target_auid=author_id)\n",
    "            # If AFF_ID specified, only keep records where this author is affiliated with AFF_ID on the paper\n",
    "            if md:\n",
    "                if AFF_ID:\n",
    "                    if not md.get(\"is_bau_author\", False):\n",
    "                        # skip this article — the author is not shown as affiliated with AFF_ID on this record\n",
    "                        if sleep:\n",
    "                            time.sleep(sleep)\n",
    "                        continue\n",
    "                # attach Scopus-derived fields remapped to column names expected downstream\n",
    "                rec = {\n",
    "                    \"eid\": md.get(\"eid\"),\n",
    "                    \"title\": md.get(\"title\"),\n",
    "                    \"year\": md.get(\"year\"),\n",
    "                    \"publication_name\": md.get(\"publication_name\"),\n",
    "                    \"subtype\": md.get(\"subtype\"),\n",
    "                    \"doi\": md.get(\"doi\"),\n",
    "                    \"source_id\": md.get(\"source_id\"),\n",
    "                    \"issn_print\": md.get(\"issn_print\"),\n",
    "                    \"issn_electronic\": md.get(\"issn_electronic\"),\n",
    "                    \"asjc_codes\": md.get(\"asjc_codes\"),\n",
    "                    \"asjc_areas\": md.get(\"asjc_areas\"),\n",
    "                    \"asjc_abbrevs\": md.get(\"asjc_abbrevs\"),\n",
    "                    \"authors_count\": md.get(\"authors_count\"),\n",
    "                    \"combined\": md.get(\"combined\"),\n",
    "                    \"abstract\": md.get(\"abstract\"),\n",
    "                    # affiliation detection fields (new)\n",
    "                    \"is_bau_author\": md.get(\"is_bau_author\"),\n",
    "                    \"bau_author_orgs\": md.get(\"bau_author_orgs\"),\n",
    "                    \"author_org_map_json\": md.get(\"author_org_map_json\"),\n",
    "                }\n",
    "                recs.append(rec)\n",
    "            if sleep:\n",
    "                time.sleep(sleep)\n",
    "        df = pd.DataFrame(recs)\n",
    "\n",
    "        if not df.empty:\n",
    "            # build cs_by_source if not provided (single lookup per run is preferred)\n",
    "            if cs_by_source is None:\n",
    "                cs_by_source = build_cs_by_source(cs_table, serial_sleep=serial_sleep)\n",
    "            df = enrich_with_citescore_sourceid_asjc(df, cs_table, cs_by_source)\n",
    "\n",
    "        path = save_author_excel(author_id, name, df if not df.empty else pd.DataFrame(), out_dir)\n",
    "        print(f\"   ✓ Wrote {Path(path).name}  ({0 if df is None else len(df)} rows)\")\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Failed {author_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ----------------------- Public API (Jupyter) -------------------------\n",
    "\n",
    "def run(\n",
    "    auids: Optional[str] = None,\n",
    "    citescore: Optional[str] = None,\n",
    "    outdir: str = \"authors\",\n",
    "    sleep: float = 0.05,\n",
    "    serial_sleep: float = 0.1,\n",
    "    no_prompt: bool = True,\n",
    "    aff_id: Optional[str] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Jupyter-friendly entrypoint. Pass aff_id to override the default AFF_ID filter.\n",
    "    \"\"\"\n",
    "    global AFF_ID\n",
    "    if aff_id is not None:\n",
    "        AFF_ID = aff_id\n",
    "\n",
    "    out_dir = Path(outdir)\n",
    "    cs_path = resolve_citescore_path(citescore, no_prompt=no_prompt)\n",
    "    if not cs_path:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not locate CiteScore file. Pass citescore=..., set CITESCORE_CSV env var, or place the file in a default folder.\"\n",
    "        )\n",
    "    print(f\"Using CiteScore file: {cs_path}\")\n",
    "    cs_table = load_citescore_table(cs_path)\n",
    "\n",
    "    ids = read_auids_from_cli_or_file(auids)\n",
    "    if not ids:\n",
    "        print(\"No AU-IDs provided. Provide auids='555...,572...' or authors.txt file.\")\n",
    "        return\n",
    "    print(f\"CiteScore rows: {len(cs_table)} | Authors: {len(ids)} | Affiliation filter: {AFF_ID}\")\n",
    "\n",
    "    # Build cs_by_source once per run to avoid repeated SerialTitle lookups\n",
    "    cs_by_source = build_cs_by_source(cs_table, serial_sleep=serial_sleep)\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for i, auid in enumerate(ids, 1):\n",
    "        print(f\"[{i}/{len(ids)}]\")\n",
    "        process_author(auid, cs_table, out_dir, sleep=sleep, serial_sleep=serial_sleep, cs_by_source=cs_by_source)\n",
    "    print(f\"\\nDone. Files saved to: {out_dir.resolve()}\")\n",
    "\n",
    "# ----------------------- CLI -----------------------------------------\n",
    "\n",
    "def main(argv: Optional[list] = None):\n",
    "    global AFF_ID\n",
    "    ap = argparse.ArgumentParser(description=\"Fetch Scopus pubs by AU-ID and match CiteScore by Source ID then ASJC (ISSN fallback). Per-article affiliation detection isolates items where the author is affiliated with the configured aff_id on that paper.\")\n",
    "    ap.add_argument(\"--auids\", type=str, default=None, help=\"Comma/space-separated Scopus Author IDs. If omitted, reads authors.txt\")\n",
    "    ap.add_argument(\"--citescore\", type=str, default=None, help=\"Path to CiteScore CSV/XLSX, or a folder containing it\")\n",
    "    ap.add_argument(\"--outdir\", type=str, default=\"authors\", help=\"Output directory (default: ./authors)\")\n",
    "    ap.add_argument(\"--sleep\", type=float, default=0.05, help=\"Sleep between EID fetches (seconds)\")\n",
    "    ap.add_argument(\"--serial-sleep\", type=float, default=0.1, help=\"Sleep between SerialTitle ISSN lookups (seconds)\")\n",
    "    ap.add_argument(\"--no-prompt\", action=\"store_true\", help=\"Do not prompt for missing CiteScore path; exit with error\")\n",
    "    ap.add_argument(\"--aff-id\", type=str, default=None, help=f\"Optional affiliation ID to isolate author-affiliated publications (default: {AFF_ID_DEFAULT})\")\n",
    "    args, _unknown = ap.parse_known_args(argv)\n",
    "\n",
    "    if args.aff_id is not None:\n",
    "        AFF_ID = args.aff_id\n",
    "\n",
    "    cs_path = resolve_citescore_path(args.citescore, no_prompt=args.no_prompt)\n",
    "    if not cs_path:\n",
    "        if _is_ipython() and not args.no_prompt:\n",
    "            try:\n",
    "                entered = input(\"Enter CiteScore CSV/XLSX path or containing folder: \").strip('\"').strip()\n",
    "            except EOFError:\n",
    "                entered = \"\"\n",
    "            if entered:\n",
    "                cs_path = resolve_citescore_path(entered, no_prompt=True)\n",
    "        if not cs_path:\n",
    "            print(\"❌ Could not locate the CiteScore file. Pass --citescore or set CITESCORE_CSV.\")\n",
    "            if _is_ipython():\n",
    "                return\n",
    "            sys.exit(2)\n",
    "    print(f\"Using CiteScore file: {cs_path}\")\n",
    "    cs_table = load_citescore_table(cs_path)\n",
    "\n",
    "    auids = read_auids_from_cli_or_file(args.auids)\n",
    "    if not auids and _is_ipython():\n",
    "        try:\n",
    "            entered = input(\"Enter Scopus Author IDs (comma or space separated), or leave blank to cancel: \").strip()\n",
    "        except EOFError:\n",
    "            entered = \"\"\n",
    "        if entered:\n",
    "            auids = read_auids_from_cli_or_file(entered)\n",
    "    if not auids:\n",
    "        print(\"No AU-IDs provided. Use --auids, or create authors.txt with one AU-ID per line.\")\n",
    "        if _is_ipython():\n",
    "            return\n",
    "        sys.exit(0)\n",
    "\n",
    "    out_dir = Path(args.outdir)\n",
    "    print(f\"CiteScore rows: {len(cs_table)} | Authors: {len(auids)} | Out: {out_dir.resolve()} | Affiliation filter: {AFF_ID}\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Build cs_by_source once for the whole run\n",
    "    cs_by_source = build_cs_by_source(cs_table, serial_sleep=args.serial_sleep)\n",
    "\n",
    "    written = []\n",
    "    for i, auid in enumerate(auids, 1):\n",
    "        print(f\"[{i}/{len(auids)}] Processing AU-ID {auid} … (AFF_ID={AFF_ID})\")\n",
    "        p = process_author(auid, cs_table, out_dir, sleep=args.sleep, serial_sleep=args.serial_sleep, cs_by_source=cs_by_source)\n",
    "        if p:\n",
    "            written.append(p)\n",
    "\n",
    "    print(f\"\\nDone. Wrote {len(written)} file(s) to {out_dir.resolve()}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dcac4a-41ed-4aa4-af61-a1ae23525ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5246171-7c7c-43d4-b557-d92a956bc82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6275c089-6101-4b72-be87-e1ca30397728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38a2109a-8bb2-4c04-a1af-e71e2b301d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CiteScore file: C:\\Users\\yusef.atteyih\\Desktop\\Academic Research Unit\\Yusef ATTEYIH\\Data Solutions\\Data Solutions 2.0\\APP Calculation\\CiteScore 2024\\CiteScore 2024 annual values.csv\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Scopus Author IDs (comma or space separated), or leave blank to cancel:  57193254610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CiteScore rows: 75679 | Authors: 1 | Out: C:\\Users\\yusef.atteyih\\Desktop\\Academic Research Unit\\Yusef ATTEYIH\\Data Solutions\\Data Solutions 2.0\\APP Calculation\\authors | Affiliation filter: 60021379\n",
      "[1/1] Processing AU-ID 57193254610 … (AFF_ID=60021379)\n",
      "→ AU-ID 57193254610 — Fadime İrem Doğan (AFF_ID filter: 60021379)\n",
      "   ✓ Wrote _fadime_irem_dogan___57193254610.xlsx  (Articles: 5, BAU Articles: 3)\n",
      "\n",
      "Done. Wrote 1 file(s) to C:\\Users\\yusef.atteyih\\Desktop\\Academic Research Unit\\Yusef ATTEYIH\\Data Solutions\\Data Solutions 2.0\\APP Calculation\\authors.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "r\"\"\"\n",
    "Scopus → CiteScore with disambiguation by **Source ID then ASJC (sub-subject area)**\n",
    "\n",
    "This version:\n",
    "• Stores ASJC sets as frozenset (hashable).\n",
    "• Matches publications to CiteScore by Source ID then ASJC (ISSN fallback).\n",
    "• Performs per-article author→affiliation mapping and isolates items where the\n",
    "  author is affiliated with the configured AFF_ID on that paper.\n",
    "• Produces three sheets in each author workbook:\n",
    "    - \"Articles\"      : ALL articles for the author (no affiliation filtering)\n",
    "    - \"BAU Articles\"  : only those articles where the author is affiliated with AFF_ID\n",
    "    - \"APP\"           : Participation Score computed from BAU Articles (subtype == \"ar\")\n",
    "• APP uses Author Coefficient (AC = 1.2 if sole author else 1.2 / n_authors)\n",
    "  and Quartile Coefficient (QC) mapped from CiteScore percentile,\n",
    "  Contribution = AC × QC, summed and rounded.\n",
    "\n",
    "Usage:\n",
    "  - Jupyter: run(auids=\"12345,67890\", citescore=\"path/to/CiteScore.csv\", aff_id=\"60021379\")\n",
    "  - CLI: python this_script.py --auids 57193254610 --citescore \"CiteScore 2024.csv\" --aff-id 60021379\n",
    "\n",
    "Requirements:\n",
    "  pybliometrics, pandas, xlsxwriter or openpyxl\n",
    "  Configure pybliometrics Scopus API credentials\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import unicodedata\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Set, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Scopus (pybliometrics) ---\n",
    "try:\n",
    "    from pybliometrics.scopus import AbstractRetrieval, AuthorRetrieval, ScopusSearch, SerialTitle\n",
    "    from pybliometrics.scopus.exception import ScopusException\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"pybliometrics is required. Install with `pip install pybliometrics` \"\n",
    "        \"and ensure your Scopus API credentials are configured.\"\n",
    "    ) from e\n",
    "\n",
    "# ----------------------- IPython detection ----------------------------\n",
    "\n",
    "def _is_ipython() -> bool:\n",
    "    return (\"ipykernel\" in sys.modules) or (\"IPython\" in sys.modules)\n",
    "\n",
    "# ----------------------- Defaults & Affiliation filter ----------------\n",
    "\n",
    "AFF_ID_DEFAULT = \"60021379\"\n",
    "AFF_ID: Optional[str] = AFF_ID_DEFAULT\n",
    "\n",
    "DEFAULT_USER_CITESCORE_DIRS = [\n",
    "    Path(r\"C:\\\\Users\\\\yusef.atteyih\\\\Desktop\\\\Academic Research Unit\\\\Yusef ATTEYIH\\\\Data Solutions\\\\Data Solutions 2.0\\\\APP Calculation\\\\CiteScore 2024\"),\n",
    "]\n",
    "DEFAULT_CITESCORE_DIRS = [Path(\"CiteScore 2024\"), Path(\".\")]\n",
    "POSSIBLE_CS_FILENAMES = [\n",
    "    \"CiteScore 2024 annual values.csv\",\n",
    "    \"CiteScore 2024 annual values.xlsx\",\n",
    "    \"CiteScore 2024.csv\",\n",
    "    \"citescore.csv\",\n",
    "    \"citescore.xlsx\",\n",
    "]\n",
    "\n",
    "# ----------------------- Small helpers --------------------------------\n",
    "\n",
    "def _s(x) -> str:\n",
    "    try:\n",
    "        if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return x if isinstance(x, str) else str(x)\n",
    "\n",
    "def _norm_issn(s: str) -> str:\n",
    "    \"\"\"Uppercase; keep 0-9 and 'X'; strip others.\"\"\"\n",
    "    return re.sub(r\"[^0-9X]\", \"\", _s(s).upper())\n",
    "\n",
    "def _norm_asjc_codes(raw: Any) -> Set[str]:\n",
    "    \"\"\"Return a set of 4-digit ASJC codes as strings.\"\"\"\n",
    "    out: Set[str] = set()\n",
    "    if raw is None:\n",
    "        return out\n",
    "    if isinstance(raw, (list, tuple, set)):\n",
    "        it = raw\n",
    "    else:\n",
    "        it = re.split(r\"[^0-9]\", _s(raw))\n",
    "    for tok in it:\n",
    "        if tok and tok.isdigit():\n",
    "            if len(tok) == 4:\n",
    "                out.add(tok)\n",
    "            elif len(tok) > 4:\n",
    "                out.add(tok[-4:])\n",
    "    return out\n",
    "\n",
    "def _coerce_percentile(val) -> Optional[float]:\n",
    "    txt = _s(val).strip()\n",
    "    if not txt:\n",
    "        return None\n",
    "    txt = txt.replace(\"%\", \"\").replace(\" \", \"\").replace(\",\", \".\")\n",
    "    m = re.search(r\"[-+]?\\d*\\.?\\d+\", txt)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _coerce_float(val) -> Optional[float]:\n",
    "    txt = _s(val).strip().replace(\",\", \".\")\n",
    "    if not txt:\n",
    "        return None\n",
    "    m = re.search(r\"[-+]?\\d*\\.?\\d+\", txt)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def quartile_from_percentile(p: Optional[float]) -> str:\n",
    "    if p is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        p = float(p)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    if p >= 90: return \"QT\"\n",
    "    if p >= 75: return \"Q1\"\n",
    "    if p >= 50: return \"Q2\"\n",
    "    if p >= 25: return \"Q3\"\n",
    "    return \"Q4\"\n",
    "\n",
    "# ----------------------- Filenames ------------------------------------\n",
    "\n",
    "def _ascii_slug(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKD\", _s(s)).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^0-9a-z]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def make_author_filename(author_name: str, author_id: str) -> str:\n",
    "    name_slug = _ascii_slug(author_name)\n",
    "    leading = \"_\" + name_slug if not name_slug.startswith(\"_\") else name_slug\n",
    "    return f\"{leading}___{author_id}.xlsx\"\n",
    "\n",
    "# ----------------------- Read AU-IDs ----------------------------------\n",
    "\n",
    "author_id_file_candidates = (\"authors.txt\", \"auids.txt\", \"ScopusAuthorIDs.txt\")\n",
    "\n",
    "def read_auids_from_cli_or_file(cli_auids: Optional[str]) -> List[str]:\n",
    "    ids: List[str] = []\n",
    "    if cli_auids:\n",
    "        for tok in re.split(r\"[,\\s]+\", cli_auids.strip()):\n",
    "            if tok and tok.isdigit():\n",
    "                ids.append(tok)\n",
    "    if ids:\n",
    "        return sorted(set(ids))\n",
    "    for fname in author_id_file_candidates:\n",
    "        p = Path(fname)\n",
    "        if p.exists():\n",
    "            try:\n",
    "                raw = p.read_text(encoding=\"utf-8\")\n",
    "            except Exception:\n",
    "                raw = p.read_text(errors=\"ignore\")\n",
    "            for line in raw.splitlines():\n",
    "                t = line.split(\"#\", 1)[0].strip()\n",
    "                if t.isdigit():\n",
    "                    ids.append(t)\n",
    "            if ids:\n",
    "                return sorted(set(ids))\n",
    "    return []\n",
    "\n",
    "# ----------------------- CiteScore path --------------------------------\n",
    "\n",
    "def _candidate_files_in_dir(d: Path) -> List[Path]:\n",
    "    return [p for name in POSSIBLE_CS_FILENAMES if (p := d / name).exists() and p.is_file()]\n",
    "\n",
    "def resolve_citescore_path(arg: Optional[str], no_prompt: bool = False) -> Optional[Path]:\n",
    "    if arg:\n",
    "        p = Path(arg)\n",
    "        if p.exists():\n",
    "            if p.is_file(): return p\n",
    "            if p.is_dir():\n",
    "                cand = _candidate_files_in_dir(p)\n",
    "                if cand: return cand[0]\n",
    "    envp = os.environ.get(\"CITESCORE_CSV\")\n",
    "    if envp:\n",
    "        p = Path(envp)\n",
    "        if p.exists() and p.is_file(): return p\n",
    "    for d in DEFAULT_USER_CITESCORE_DIRS + DEFAULT_CITESCORE_DIRS:\n",
    "        if d.exists() and d.is_dir():\n",
    "            cand = _candidate_files_in_dir(d)\n",
    "            if cand: return cand[0]\n",
    "    if not no_prompt:\n",
    "        try:\n",
    "            path_in = input(\"Path to CiteScore CSV/XLSX (or folder containing it): \").strip('\"').strip()\n",
    "            if path_in:\n",
    "                p = Path(path_in)\n",
    "                if p.exists():\n",
    "                    if p.is_file(): return p\n",
    "                    if p.is_dir():\n",
    "                        cand = _candidate_files_in_dir(p)\n",
    "                        if cand: return cand[0]\n",
    "        except EOFError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# ----------------------- Read CiteScore table --------------------------\n",
    "\n",
    "def robust_read_table(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"CiteScore file not found: {path}\")\n",
    "    suffix = path.suffix.lower()\n",
    "    if suffix in (\".xlsx\", \".xls\"):\n",
    "        return pd.read_excel(path)\n",
    "    encodings = [\"utf-8-sig\", \"utf-16\", \"utf-16le\", \"utf-16be\", \"cp1254\", \"iso-8859-9\", \"cp1252\", \"latin1\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, engine=\"python\", sep=None)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding=\"latin1\", engine=\"python\", sep=None, on_bad_lines=\"skip\")\n",
    "    except TypeError:\n",
    "        return pd.read_csv(path, encoding=\"latin1\", engine=\"python\", sep=None, error_bad_lines=False)  # type: ignore\n",
    "\n",
    "def load_citescore_table(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return columns:\n",
    "      source_id (if present), issn_key, eissn_key, asjc_set (frozenset), cs_percentile, citescore\n",
    "    \"\"\"\n",
    "    cs = robust_read_table(path)\n",
    "    norm = {c.strip().lower(): c for c in cs.columns}\n",
    "\n",
    "    p = norm.get(\"print issn\") or norm.get(\"p-issn\") or norm.get(\"issn\")\n",
    "    e = norm.get(\"e-issn\") or norm.get(\"eissn\")\n",
    "    pct = norm.get(\"percentile\") or norm.get(\"citescore percentile\")\n",
    "    val = norm.get(\"citescore\") or norm.get(\"citescore 2024\")\n",
    "    src = norm.get(\"source id\") or norm.get(\"scopus source id\") or norm.get(\"scopus sourceid\")\n",
    "    asjc_col = norm.get(\"asjc\") or norm.get(\"asjc code\") or norm.get(\"asjc codes\") or norm.get(\"subject area asjc\")\n",
    "\n",
    "    if not all([p, e, pct, val]):\n",
    "        raise KeyError(\n",
    "            f\"CiteScore table must include 'Print ISSN', 'E-ISSN', 'Percentile', 'CiteScore'. Found: {list(cs.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Standardize names\n",
    "    cs = cs.rename(columns={p: \"print_issn\", e: \"e_issn\", pct: \"cs_percentile\", val: \"citescore\"})\n",
    "    if src:\n",
    "        cs = cs.rename(columns={src: \"source_id\"})\n",
    "    if asjc_col:\n",
    "        cs = cs.rename(columns={asjc_col: \"asjc_raw\"})\n",
    "    else:\n",
    "        cs[\"asjc_raw\"] = \"\"\n",
    "\n",
    "    # Coerce types\n",
    "    cs[\"print_issn\"] = cs[\"print_issn\"].astype(str)\n",
    "    cs[\"e_issn\"] = cs[\"e_issn\"].astype(str)\n",
    "    cs[\"cs_percentile\"] = cs[\"cs_percentile\"].apply(_coerce_percentile)\n",
    "    cs[\"citescore\"] = cs[\"citescore\"].apply(_coerce_float)\n",
    "    if \"source_id\" in cs.columns:\n",
    "        cs[\"source_id\"] = cs[\"source_id\"].astype(str).str.extract(r\"(\\d+)\", expand=False).fillna(\"\")\n",
    "\n",
    "    # Normalized keys\n",
    "    cs[\"issn_key\"] = cs[\"print_issn\"].map(_norm_issn)\n",
    "    cs[\"eissn_key\"] = cs[\"e_issn\"].map(_norm_issn)\n",
    "\n",
    "    # ASJC as frozenset (hashable)\n",
    "    cs[\"asjc_set\"] = cs[\"asjc_raw\"].apply(lambda v: frozenset(_norm_asjc_codes(v)))\n",
    "\n",
    "    # Keep minimal columns\n",
    "    keep = [\"issn_key\", \"eissn_key\", \"asjc_set\", \"cs_percentile\", \"citescore\"]\n",
    "    if \"source_id\" in cs.columns:\n",
    "        keep.insert(0, \"source_id\")\n",
    "    cs = cs[keep]\n",
    "    return cs\n",
    "\n",
    "# ----------------------- Scopus helpers --------------------------------\n",
    "\n",
    "def _extract_issns(ar: Any) -> Tuple[str, str]:\n",
    "    p = \"\"; e = \"\"\n",
    "    if hasattr(ar, \"eIssn\"):\n",
    "        e = _s(getattr(ar, \"eIssn\"))\n",
    "    if not e and hasattr(ar, \"e_issn\"):\n",
    "        e = _s(getattr(ar, \"e_issn\"))\n",
    "    if hasattr(ar, \"issn\"):\n",
    "        obj = getattr(ar, \"issn\")\n",
    "        if isinstance(obj, str):\n",
    "            if \"ISSN(\" in obj:\n",
    "                mp = re.search(r\"print\\s*=\\s*'([^']+)'\", obj)\n",
    "                me = re.search(r\"electronic\\s*=\\s*'([^']+)'\", obj)\n",
    "                if mp: p = mp.group(1)\n",
    "                if me and not e: e = me.group(1)\n",
    "            else:\n",
    "                p = _s(obj)\n",
    "        else:\n",
    "            try:\n",
    "                p_obj = getattr(obj, \"print\", \"\"); e_obj = getattr(obj, \"electronic\", \"\")\n",
    "                if p_obj: p = _s(p_obj)\n",
    "                if not e and e_obj: e = _s(e_obj)\n",
    "            except Exception:\n",
    "                txt = _s(obj)\n",
    "                mp = re.search(r\"print\\s*=\\s*'([^']+)'\", txt)\n",
    "                me = re.search(r\"electronic\\s*=\\s*'([^']+)'\", txt)\n",
    "                if mp: p = mp.group(1)\n",
    "                if me and not e: e = me.group(1)\n",
    "    return p, e\n",
    "\n",
    "\n",
    "def _extract_asjc(ar: Any) -> Tuple[str, str, str, Set[str]]:\n",
    "    codes: Set[str] = set(); areas: Set[str] = set(); abbrevs: Set[str] = set()\n",
    "    sa = getattr(ar, \"subject_areas\", None)\n",
    "    if sa:\n",
    "        try:\n",
    "            for it in sa:\n",
    "                c = getattr(it, \"code\", None)\n",
    "                a = getattr(it, \"area\", None)\n",
    "                ab = getattr(it, \"abbrev\", None)\n",
    "                if c is not None:\n",
    "                    codes.add(f\"{int(c):04d}\" if str(c).isdigit() else str(c))\n",
    "                if a: areas.add(_s(a))\n",
    "                if ab: abbrevs.add(_s(ab))\n",
    "        except Exception:\n",
    "            try:  # dict-like\n",
    "                for it in sa:\n",
    "                    c = it.get(\"code\"); a = it.get(\"area\"); ab = it.get(\"abbrev\")\n",
    "                    if c is not None:\n",
    "                        codes.add(f\"{int(c):04d}\" if str(c).isdigit() else str(c))\n",
    "                    if a: areas.add(_s(a))\n",
    "                    if ab: abbrevs.add(_s(ab))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return \", \".join(sorted(codes)), \", \".join(sorted(areas)), \", \".join(sorted(abbrevs)), codes\n",
    "\n",
    "# ----------------------- ISSN → Source ID (optional) -------------------\n",
    "\n",
    "def fetch_source_id_for_issn(issn: str) -> Optional[str]:\n",
    "    issn = _norm_issn(issn)\n",
    "    if not issn:\n",
    "        return None\n",
    "    try:\n",
    "        res = SerialTitle(issn)\n",
    "        items: Iterable[Any]\n",
    "        try:\n",
    "            items = list(res) if isinstance(res, (list, tuple)) else [res]\n",
    "        except Exception:\n",
    "            items = [res]\n",
    "        for it in items:\n",
    "            for attr in (\"source_id\", \"sourcerecord_id\", \"sourceid\"):\n",
    "                if hasattr(it, attr):\n",
    "                    sid = _s(getattr(it, attr))\n",
    "                    if sid.isdigit():\n",
    "                        return sid\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"SerialTitle lookup failed for ISSN {issn}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_cs_by_source(cs_table: pd.DataFrame, serial_sleep: float = 0.1) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a cs_by_source table with 'source_id' present, mapping from cs_table.\n",
    "    If cs_table already has source_id, just dedupe and return.\n",
    "    Otherwise, look up source_id from ISSN/e-ISSN via SerialTitle.\n",
    "    \"\"\"\n",
    "    if \"source_id\" in cs_table.columns:\n",
    "        df = cs_table.copy()\n",
    "    else:\n",
    "        df = cs_table.copy()\n",
    "        df[\"source_id_from_print\"] = df[\"issn_key\"].apply(fetch_source_id_for_issn)\n",
    "        if serial_sleep:\n",
    "            time.sleep(serial_sleep)\n",
    "        df[\"source_id_from_e\"] = df[\"eissn_key\"].apply(fetch_source_id_for_issn)\n",
    "        df[\"source_id\"] = df[\"source_id_from_print\"].where(df[\"source_id_from_print\"].notna(), df[\"source_id_from_e\"])\n",
    "        df = df.drop(columns=[\"source_id_from_print\", \"source_id_from_e\"], errors=\"ignore\")\n",
    "    df = df[df[\"source_id\"].astype(str).str.strip().ne(\"\")].copy()\n",
    "    df = df.drop_duplicates(subset=[\"source_id\", \"asjc_set\"], keep=\"first\")\n",
    "    return df[[\"source_id\", \"asjc_set\", \"cs_percentile\", \"citescore\"]]\n",
    "\n",
    "# ----------------------- Matching logic --------------------------------\n",
    "\n",
    "def _pick_best_candidate(cands: pd.DataFrame, article_asjc: Set[str]) -> Tuple[Optional[float], Optional[float]]:\n",
    "    \"\"\"Choose best row: prefer ASJC overlap, then highest percentile, else first.\n",
    "    Returns (cs_percentile, citescore).\n",
    "    \"\"\"\n",
    "    if cands is None or cands.empty:\n",
    "        return None, None\n",
    "    over = []\n",
    "    for _i, row in cands.iterrows():\n",
    "        cs_set = row.get(\"asjc_set\") or set()\n",
    "        if isinstance(cs_set, (set, frozenset)):\n",
    "            cs_set2 = set(cs_set)\n",
    "        elif isinstance(cs_set, (list, tuple)):\n",
    "            cs_set2 = {str(x) for x in cs_set}\n",
    "        else:\n",
    "            cs_set2 = _norm_asjc_codes(cs_set)\n",
    "        over.append(len(article_asjc & cs_set2))\n",
    "    cands = cands.copy()\n",
    "    cands[\"_overlap\"] = over\n",
    "    cands[\"_pct\"] = cands[\"cs_percentile\"].fillna(-1e9)\n",
    "    cands = cands.sort_values([\"_overlap\", \"_pct\"], ascending=[False, False])\n",
    "    top = cands.iloc[0]\n",
    "    return top.get(\"cs_percentile\"), top.get(\"citescore\")\n",
    "\n",
    "\n",
    "def enrich_with_citescore_sourceid_asjc(\n",
    "    df_articles: pd.DataFrame,\n",
    "    cs_table: pd.DataFrame,\n",
    "    cs_by_source: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    if df_articles is None or df_articles.empty:\n",
    "        return df_articles.copy()\n",
    "\n",
    "    df = df_articles.copy()\n",
    "    for col in (\"issn_print\", \"issn_electronic\", \"source_id\"):\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "\n",
    "    df[\"issn_key\"] = df[\"issn_print\"].astype(str).map(_norm_issn)\n",
    "    df[\"eissn_key\"] = df[\"issn_electronic\"].astype(str).map(_norm_issn)\n",
    "\n",
    "    a_asjc_sets: List[Set[str]] = []\n",
    "    for v in df.get(\"asjc_codes\", pd.Series([\"\"] * len(df))):\n",
    "        a_asjc_sets.append(_norm_asjc_codes(v))\n",
    "\n",
    "    cs_p = cs_table[[\"issn_key\", \"asjc_set\", \"cs_percentile\", \"citescore\"]].drop_duplicates()\n",
    "    cs_e = cs_table[[\"eissn_key\", \"asjc_set\", \"cs_percentile\", \"citescore\"]].drop_duplicates()\n",
    "\n",
    "    out_pct: List[Optional[float]] = []\n",
    "    out_val: List[Optional[float]] = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        article_asjc = a_asjc_sets[idx] if idx < len(a_asjc_sets) else set()\n",
    "        sid = _s(row.get(\"source_id\"))\n",
    "        pct = None; val = None\n",
    "        if sid:\n",
    "            cands = cs_by_source[cs_by_source[\"source_id\"].astype(str) == sid]\n",
    "            pct, val = _pick_best_candidate(cands, article_asjc)\n",
    "        if pct is None and val is None:\n",
    "            issn = _s(row.get(\"issn_key\"))\n",
    "            eissn = _s(row.get(\"eissn_key\"))\n",
    "            cands = pd.concat([\n",
    "                cs_p[cs_p[\"issn_key\"] == issn],\n",
    "                cs_e[cs_e[\"eissn_key\"] == eissn],\n",
    "            ], ignore_index=True)\n",
    "            pct, val = _pick_best_candidate(cands, article_asjc)\n",
    "        out_pct.append(pct)\n",
    "        out_val.append(val)\n",
    "\n",
    "    df[\"cs_percentile\"] = out_pct\n",
    "    df[\"citescore\"] = out_val\n",
    "    df[\"quartile\"] = df[\"cs_percentile\"].apply(quartile_from_percentile)\n",
    "    return df\n",
    "\n",
    "# ----------------------- Scopus collectors (with per-author aff detection) -------\n",
    "\n",
    "def get_author_name(author_id: str) -> str:\n",
    "    try:\n",
    "        ar = AuthorRetrieval(author_id)\n",
    "        name = f\"{_s(ar.given_name)} {_s(ar.surname)}\".strip()\n",
    "        return name or author_id\n",
    "    except Exception:\n",
    "        return author_id\n",
    "\n",
    "def get_author_eids(author_id: str) -> List[str]:\n",
    "    try:\n",
    "        s = ScopusSearch(f\"AU-ID({author_id})\", subscriber=True)\n",
    "        return s.get_eids() or []\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"ScopusSearch failed for AU-ID({author_id}): {e}\")\n",
    "        return []\n",
    "\n",
    "def _safe_int(v) -> Optional[int]:\n",
    "    try:\n",
    "        return int(v)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_article_metadata(eid: str, target_auid: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Return article metadata dict. Also detect whether the given target_auid\n",
    "    appears in the article and whether that author entry has affiliation_id == AFF_ID.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ar = AbstractRetrieval(eid, view=\"FULL\")\n",
    "    except ScopusException as e:\n",
    "        warnings.warn(f\"AbstractRetrieval failed for {eid}: {e}\")\n",
    "        return None\n",
    "\n",
    "    if ar.subtype not in (\"ar\", \"re\"):\n",
    "        return None\n",
    "\n",
    "    year = \"\"\n",
    "    if getattr(ar, \"coverDate\", None):\n",
    "        year = _s(ar.coverDate)[:4]\n",
    "\n",
    "    issn_print, issn_elec = _extract_issns(ar)\n",
    "    asjc_codes_csv, asjc_areas_csv, asjc_abbrevs_csv, _codes_set = _extract_asjc(ar)\n",
    "\n",
    "    bau_flag = False\n",
    "    bau_orgs = []\n",
    "    auid_org_map: Dict[str, str] = {}\n",
    "    auth_err = None\n",
    "    groups = getattr(ar, \"authorgroup\", None)\n",
    "    if groups:\n",
    "        try:\n",
    "            for g in groups:\n",
    "                try:\n",
    "                    g_auid = str(getattr(g, \"auid\", \"\") or \"\")\n",
    "                    org_val = getattr(g, \"organization\", None)\n",
    "                    aff_id_val = _safe_int(getattr(g, \"affiliation_id\", None))\n",
    "                    if g_auid:\n",
    "                        auid_org_map[g_auid] = _s(org_val)\n",
    "                    if target_auid and g_auid == str(target_auid) and AFF_ID and aff_id_val == _safe_int(AFF_ID):\n",
    "                        bau_flag = True\n",
    "                        if org_val:\n",
    "                            bau_orgs.append(_s(org_val))\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception as ex:\n",
    "            auth_err = str(ex)\n",
    "    else:\n",
    "        try:\n",
    "            authors = getattr(ar, \"authors\", None) or []\n",
    "            for a in authors:\n",
    "                try:\n",
    "                    g_auid = str(getattr(a, \"auid\", \"\") or \"\")\n",
    "                    org_val = getattr(a, \"orgname\", None) or getattr(a, \"affiliation\", None) or \"\"\n",
    "                    if g_auid:\n",
    "                        auid_org_map[g_auid] = _s(org_val)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    insts = getattr(ar, \"affiliation\", None) or []\n",
    "\n",
    "    return {\n",
    "        \"eid\": _s(eid),\n",
    "        \"title\": _s(ar.title),\n",
    "        \"year\": _s(year),\n",
    "        \"publication_name\": _s(getattr(ar, \"publicationName\", \"\")),\n",
    "        \"subtype\": _s(ar.subtype),\n",
    "        \"doi\": _s(getattr(ar, \"doi\", \"\")),\n",
    "        \"source_id\": _s(getattr(ar, \"source_id\", \"\")),\n",
    "        \"issn_print\": _s(issn_print),\n",
    "        \"issn_electronic\": _s(issn_elec),\n",
    "        \"asjc_codes\": asjc_codes_csv,\n",
    "        \"asjc_areas\": asjc_areas_csv,\n",
    "        \"asjc_abbrevs\": asjc_abbrevs_csv,\n",
    "        \"authors_count\": len(ar.authors) if getattr(ar, \"authors\", None) else 1,\n",
    "        \"combined\": \"; \".join([t for t in (getattr(ar, \"authkeywords\", []) or []) if _s(t)]),\n",
    "        \"abstract\": _s(getattr(ar, \"description\", \"\")),\n",
    "        \"author_org_map_json\": json.dumps(auid_org_map, ensure_ascii=False),\n",
    "        \"is_bau_author\": bau_flag,\n",
    "        \"bau_author_orgs\": \"; \".join(dict.fromkeys(bau_orgs)),\n",
    "        \"affiliations_list\": [(getattr(i, \"name\", \"\") if hasattr(i, \"name\") else _s(i)) for i in insts],\n",
    "        \"auth_group_error\": auth_err,\n",
    "    }\n",
    "\n",
    "# ----------------------- APP calculation helpers -----------------------\n",
    "\n",
    "def _qc_from_percentile(p: Optional[float]) -> Optional[float]:\n",
    "    if p is None:\n",
    "        return None\n",
    "    try:\n",
    "        p = float(p)\n",
    "    except Exception:\n",
    "        return None\n",
    "    if p >= 90:   # Top 10%\n",
    "        return 1.4\n",
    "    if p >= 75:   # Q1\n",
    "        return 1.0\n",
    "    if p >= 50:   # Q2\n",
    "        return 0.8\n",
    "    if p >= 25:   # Q3\n",
    "        return 0.6\n",
    "    if p >= 0:    # Q4\n",
    "        return 0.4\n",
    "    return None\n",
    "\n",
    "def _ac_from_authors(n: Any) -> float:\n",
    "    try:\n",
    "        n = int(n)\n",
    "    except Exception:\n",
    "        n = 1\n",
    "    return 1.2 if n <= 1 else 1.2 / max(n, 1)\n",
    "\n",
    "def _to_int_year(y: Any) -> Optional[int]:\n",
    "    try:\n",
    "        s = str(y)\n",
    "        m = re.search(r\"\\d{4}\", s)\n",
    "        return int(m.group(0)) if m else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def build_app_sheet(df_articles: pd.DataFrame, now_year: Optional[int] = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    if df_articles is None or df_articles.empty:\n",
    "        return pd.DataFrame(), {\"app_total\": 0.0, \"eligibility\": \"No eligible items\", \"years\": []}\n",
    "\n",
    "    cy = now_year or datetime.now().year\n",
    "    years_ok = {cy, cy - 1, cy - 2}\n",
    "\n",
    "    tmp = df_articles.copy()\n",
    "    tmp[\"year_i\"] = tmp.get(\"year\", \"\").apply(_to_int_year)\n",
    "    tmp[\"cs_percentile_num\"] = pd.to_numeric(tmp.get(\"cs_percentile\"), errors=\"coerce\")\n",
    "    tmp[\"authors_count_i\"] = pd.to_numeric(tmp.get(\"authors_count\"), errors=\"coerce\").fillna(1).astype(int)\n",
    "    tmp[\"subtype_norm\"] = tmp.get(\"subtype\", \"\").astype(str).str.lower()\n",
    "\n",
    "    eligible = tmp[\n",
    "        tmp[\"year_i\"].isin(years_ok) &\n",
    "        tmp[\"cs_percentile_num\"].notna() &\n",
    "        (tmp[\"subtype_norm\"] == \"ar\")\n",
    "    ].copy()\n",
    "\n",
    "    if eligible.empty:\n",
    "        return pd.DataFrame(), {\"app_total\": 0.0, \"eligibility\": \"No eligible items (no 'ar' articles in window)\", \"years\": sorted(years_ok)}\n",
    "\n",
    "    eligible[\"QC\"] = eligible[\"cs_percentile_num\"].apply(_qc_from_percentile)\n",
    "    eligible[\"AC\"] = eligible[\"authors_count_i\"].apply(_ac_from_authors)\n",
    "    eligible = eligible[eligible[\"QC\"].notna()].copy()\n",
    "    if eligible.empty:\n",
    "        return pd.DataFrame(), {\"app_total\": 0.0, \"eligibility\": \"No eligible items (QC missing)\", \"years\": sorted(years_ok)}\n",
    "\n",
    "    eligible[\"Contribution\"] = eligible[\"AC\"] * eligible[\"QC\"]\n",
    "    eligible[\"AC\"] = eligible[\"AC\"].round(2)\n",
    "    eligible[\"QC\"] = eligible[\"QC\"].round(2)\n",
    "    eligible[\"Contribution\"] = eligible[\"Contribution\"].round(2)\n",
    "\n",
    "    out_cols = [\n",
    "        \"eid\", \"title\", \"year\", \"publication_name\",\n",
    "        \"authors_count\", \"cs_percentile\", \"quartile\",\n",
    "        \"AC\", \"QC\", \"Contribution\"\n",
    "    ]\n",
    "    for c in out_cols:\n",
    "        if c not in eligible.columns:\n",
    "            eligible[c] = pd.Series(dtype=\"object\")\n",
    "    df_app = eligible[out_cols].sort_values([\"year\", \"Contribution\"], ascending=[False, False]).reset_index(drop=True)\n",
    "\n",
    "    app_total = float(df_app[\"Contribution\"].sum().round(2))\n",
    "\n",
    "    if app_total > 1.0:\n",
    "        elig = \"APP > 1.0 → up to 2 supports / AY (only 1 requires full indexing & APP check)\"\n",
    "    elif app_total >= 0.4:\n",
    "        elig = \"0.4 ≤ APP ≤ 1.0 → 1 support / AY\"\n",
    "    else:\n",
    "        elig = \"APP < 0.4 → 1 support / AY (if other criteria met)\"\n",
    "\n",
    "    summary = {\"app_total\": round(app_total, 2), \"eligibility\": elig, \"years\": sorted(years_ok)}\n",
    "    return df_app, summary\n",
    "\n",
    "# ----------------------- Excel writer (Articles / BAU Articles / APP) -----------------\n",
    "\n",
    "def _write_excel_xlsxwriter(articles_df: pd.DataFrame, bau_df: pd.DataFrame, path: Path, app_df: Optional[pd.DataFrame] = None, app_summary: Optional[Dict[str, Any]] = None):\n",
    "    with pd.ExcelWriter(path, engine=\"xlsxwriter\") as xl:\n",
    "        # Articles (ALL)\n",
    "        articles_df.to_excel(xl, sheet_name=\"Articles\", index=False)\n",
    "        ws = xl.sheets[\"Articles\"]\n",
    "        try:\n",
    "            ws.freeze_panes(1, 0)\n",
    "            ws.set_column(\"A:A\", 20)  # eid\n",
    "            ws.set_column(\"B:B\", 50)  # title\n",
    "            ws.set_column(\"C:C\", 8)   # year\n",
    "            ws.set_column(\"D:D\", 36)  # publication_name\n",
    "            ws.set_column(\"E:E\", 8)   # subtype\n",
    "            ws.set_column(\"F:F\", 26)  # doi\n",
    "            ws.set_column(\"G:G\", 14)  # source_id\n",
    "            ws.set_column(\"H:I\", 14)  # ISSNs\n",
    "            ws.set_column(\"J:J\", 18)  # asjc_codes\n",
    "            ws.set_column(\"K:K\", 26)  # asjc_areas\n",
    "            ws.set_column(\"L:L\", 14)  # asjc_abbrevs\n",
    "            ws.set_column(\"M:N\", 14)  # cs_percentile / citescore\n",
    "            ws.set_column(\"O:O\", 10)  # quartile\n",
    "            ws.set_column(\"P:P\", 12)  # authors_count\n",
    "            ws.set_column(\"Q:Q\", 28)  # combined\n",
    "            ws.set_column(\"R:R\", 80)  # abstract\n",
    "            ws.set_column(\"S:U\", 28)  # affiliation fields\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # BAU Articles\n",
    "        bau_df.to_excel(xl, sheet_name=\"BAU Articles\", index=False)\n",
    "        ws2 = xl.sheets[\"BAU Articles\"]\n",
    "        try:\n",
    "            ws2.freeze_panes(1, 0)\n",
    "            ws2.set_column(\"A:A\", 20)\n",
    "            ws2.set_column(\"B:B\", 50)\n",
    "            ws2.set_column(\"C:C\", 8)\n",
    "            ws2.set_column(\"D:D\", 36)\n",
    "            ws2.set_column(\"E:E\", 8)\n",
    "            ws2.set_column(\"F:F\", 26)\n",
    "            ws2.set_column(\"G:G\", 14)\n",
    "            ws2.set_column(\"H:I\", 14)\n",
    "            ws2.set_column(\"J:J\", 18)\n",
    "            ws2.set_column(\"K:K\", 26)\n",
    "            ws2.set_column(\"L:L\", 14)\n",
    "            ws2.set_column(\"M:N\", 14)\n",
    "            ws2.set_column(\"O:O\", 10)\n",
    "            ws2.set_column(\"P:P\", 12)\n",
    "            ws2.set_column(\"Q:Q\", 28)\n",
    "            ws2.set_column(\"R:R\", 80)\n",
    "            ws2.set_column(\"S:U\", 28)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # APP sheet (uses BAU Articles)\n",
    "        if app_df is not None and not app_df.empty:\n",
    "            ws3 = xl.book.add_worksheet(\"APP\")\n",
    "            xl.sheets[\"APP\"] = ws3\n",
    "            try:\n",
    "                ws3.write(0, 0, \"APP calculation — last 3 calendar years (journal articles only; subtype == 'ar')\")\n",
    "                if app_summary:\n",
    "                    ws3.write(1, 0, \"Years considered\")\n",
    "                    ws3.write(1, 1, \", \".join(str(y) for y in app_summary.get(\"years\", [])))\n",
    "                    ws3.write(2, 0, \"APP Score\")\n",
    "                    ws3.write(2, 1, app_summary.get(\"app_total\", 0.0))\n",
    "                    ws3.write(3, 0, \"Eligibility\")\n",
    "                    ws3.write(3, 1, app_summary.get(\"eligibility\", \"\"))\n",
    "                (app_df.reset_index(drop=True)).to_excel(xl, sheet_name=\"APP\", index=False, startrow=5)\n",
    "                ws3.freeze_panes(6, 0)\n",
    "                ws3.set_column(\"A:A\", 20)\n",
    "                ws3.set_column(\"B:B\", 60)\n",
    "                ws3.set_column(\"C:C\", 8)\n",
    "                ws3.set_column(\"D:D\", 36)\n",
    "                ws3.set_column(\"E:E\", 12)\n",
    "                ws3.set_column(\"F:F\", 14)\n",
    "                ws3.set_column(\"G:G\", 10)\n",
    "                ws3.set_column(\"H:J\", 14)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    app_df.to_excel(xl, sheet_name=\"APP\", index=False)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "def _write_excel_openpyxl(articles_df: pd.DataFrame, bau_df: pd.DataFrame, path: Path, app_df: Optional[pd.DataFrame] = None, app_summary: Optional[Dict[str, Any]] = None):\n",
    "    with pd.ExcelWriter(path, engine=\"openpyxl\") as xl:\n",
    "        articles_df.to_excel(xl, sheet_name=\"Articles\", index=False)\n",
    "        bau_df.to_excel(xl, sheet_name=\"BAU Articles\", index=False)\n",
    "        try:\n",
    "            if app_df is not None and not app_df.empty:\n",
    "                app_df.to_excel(xl, sheet_name=\"APP\", index=False, startrow=5)\n",
    "        except Exception:\n",
    "            try:\n",
    "                app_df.to_excel(xl, sheet_name=\"APP\", index=False)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "def save_author_excel(author_id: str, author_name: str, articles_df: pd.DataFrame, bau_df: pd.DataFrame, out_dir: Path) -> str:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    path = out_dir / make_author_filename(author_name, author_id)\n",
    "\n",
    "    need = [\n",
    "        \"eid\",\"title\",\"year\",\"publication_name\",\"subtype\",\"doi\",\n",
    "        \"source_id\",\"issn_print\",\"issn_electronic\",\n",
    "        \"asjc_codes\",\"asjc_areas\",\"asjc_abbrevs\",\n",
    "        \"cs_percentile\",\"citescore\",\"quartile\",\n",
    "        \"authors_count\",\"combined\",\"abstract\",\n",
    "        \"is_bau_author\",\"bau_author_orgs\",\"author_org_map_json\"\n",
    "    ]\n",
    "\n",
    "    # Ensure columns present for both frames\n",
    "    art = articles_df.copy() if articles_df is not None else pd.DataFrame()\n",
    "    bau = bau_df.copy() if bau_df is not None else pd.DataFrame()\n",
    "    for df in (art, bau):\n",
    "        for c in need:\n",
    "            if c not in df.columns:\n",
    "                df[c] = pd.Series(dtype=\"object\")\n",
    "    art = art[need]\n",
    "    bau = bau[need]\n",
    "\n",
    "    # APP computed from BAU articles\n",
    "    app_df, app_summary = build_app_sheet(bau)\n",
    "\n",
    "    try:\n",
    "        _write_excel_xlsxwriter(art, bau, path, app_df=app_df if not app_df.empty else None, app_summary=app_summary)\n",
    "    except Exception:\n",
    "        _write_excel_openpyxl(art, bau, path, app_df=app_df if not app_df.empty else None, app_summary=app_summary)\n",
    "    return str(path)\n",
    "\n",
    "# ----------------------- Orchestration --------------------------------\n",
    "\n",
    "def process_author(author_id: str, cs_table: pd.DataFrame, out_dir: Path, sleep: float, serial_sleep: float, cs_by_source: Optional[pd.DataFrame] = None) -> Optional[str]:\n",
    "    try:\n",
    "        name = get_author_name(author_id)\n",
    "        print(f\"→ AU-ID {author_id} — {name} (AFF_ID filter: {AFF_ID})\")\n",
    "        eids = get_author_eids(author_id)\n",
    "        recs_all: List[Dict[str, Any]] = []\n",
    "        recs_bau: List[Dict[str, Any]] = []\n",
    "\n",
    "        for eid in eids:\n",
    "            md = get_article_metadata(eid, target_auid=author_id)\n",
    "            if md:\n",
    "                rec = {\n",
    "                    \"eid\": md.get(\"eid\"),\n",
    "                    \"title\": md.get(\"title\"),\n",
    "                    \"year\": md.get(\"year\"),\n",
    "                    \"publication_name\": md.get(\"publication_name\"),\n",
    "                    \"subtype\": md.get(\"subtype\"),\n",
    "                    \"doi\": md.get(\"doi\"),\n",
    "                    \"source_id\": md.get(\"source_id\"),\n",
    "                    \"issn_print\": md.get(\"issn_print\"),\n",
    "                    \"issn_electronic\": md.get(\"issn_electronic\"),\n",
    "                    \"asjc_codes\": md.get(\"asjc_codes\"),\n",
    "                    \"asjc_areas\": md.get(\"asjc_areas\"),\n",
    "                    \"asjc_abbrevs\": md.get(\"asjc_abbrevs\"),\n",
    "                    \"authors_count\": md.get(\"authors_count\"),\n",
    "                    \"combined\": md.get(\"combined\"),\n",
    "                    \"abstract\": md.get(\"abstract\"),\n",
    "                    \"is_bau_author\": md.get(\"is_bau_author\"),\n",
    "                    \"bau_author_orgs\": md.get(\"bau_author_orgs\"),\n",
    "                    \"author_org_map_json\": md.get(\"author_org_map_json\"),\n",
    "                }\n",
    "                # add to \"all articles\" unconditionally\n",
    "                recs_all.append(rec)\n",
    "                # add to BAU-only if author-affiliated on this record\n",
    "                if md.get(\"is_bau_author\", False):\n",
    "                    recs_bau.append(rec)\n",
    "            if sleep:\n",
    "                time.sleep(sleep)\n",
    "\n",
    "        df_all = pd.DataFrame(recs_all)\n",
    "        df_bau = pd.DataFrame(recs_bau)\n",
    "\n",
    "        if not df_all.empty:\n",
    "            if cs_by_source is None:\n",
    "                cs_by_source = build_cs_by_source(cs_table, serial_sleep=serial_sleep)\n",
    "            df_all = enrich_with_citescore_sourceid_asjc(df_all, cs_table, cs_by_source)\n",
    "\n",
    "        if not df_bau.empty:\n",
    "            if cs_by_source is None:\n",
    "                cs_by_source = build_cs_by_source(cs_table, serial_sleep=serial_sleep)\n",
    "            df_bau = enrich_with_citescore_sourceid_asjc(df_bau, cs_table, cs_by_source)\n",
    "\n",
    "        path = save_author_excel(author_id, name, df_all if not df_all.empty else pd.DataFrame(), df_bau if not df_bau.empty else pd.DataFrame(), out_dir)\n",
    "        print(f\"   ✓ Wrote {Path(path).name}  (Articles: {len(df_all) if df_all is not None else 0}, BAU Articles: {len(df_bau) if df_bau is not None else 0})\")\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Failed {author_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ----------------------- Public API (Jupyter) -------------------------\n",
    "\n",
    "def run(\n",
    "    auids: Optional[str] = None,\n",
    "    citescore: Optional[str] = None,\n",
    "    outdir: str = \"authors\",\n",
    "    sleep: float = 0.05,\n",
    "    serial_sleep: float = 0.1,\n",
    "    no_prompt: bool = True,\n",
    "    aff_id: Optional[str] = None,\n",
    ") -> None:\n",
    "    global AFF_ID\n",
    "    if aff_id is not None:\n",
    "        AFF_ID = aff_id\n",
    "\n",
    "    out_dir = Path(outdir)\n",
    "    cs_path = resolve_citescore_path(citescore, no_prompt=no_prompt)\n",
    "    if not cs_path:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not locate CiteScore file. Pass citescore=..., set CITESCORE_CSV env var, or place the file in a default folder.\"\n",
    "        )\n",
    "    print(f\"Using CiteScore file: {cs_path}\")\n",
    "    cs_table = load_citescore_table(cs_path)\n",
    "\n",
    "    ids = read_auids_from_cli_or_file(auids)\n",
    "    if not ids:\n",
    "        print(\"No AU-IDs provided. Provide auids='555...,572...' or authors.txt file.\")\n",
    "        return\n",
    "    print(f\"CiteScore rows: {len(cs_table)} | Authors: {len(ids)} | Affiliation filter: {AFF_ID}\")\n",
    "\n",
    "    cs_by_source = build_cs_by_source(cs_table, serial_sleep=serial_sleep)\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for i, auid in enumerate(ids, 1):\n",
    "        print(f\"[{i}/{len(ids)}]\")\n",
    "        process_author(auid, cs_table, out_dir, sleep=sleep, serial_sleep=serial_sleep, cs_by_source=cs_by_source)\n",
    "    print(f\"\\nDone. Files saved to: {out_dir.resolve()}\")\n",
    "\n",
    "# ----------------------- CLI -----------------------------------------\n",
    "\n",
    "def main(argv: Optional[list] = None):\n",
    "    global AFF_ID\n",
    "    ap = argparse.ArgumentParser(description=\"Fetch Scopus pubs by AU-ID and match CiteScore by Source ID then ASJC (ISSN fallback). Produces 'Articles', 'BAU Articles' and 'APP' sheets per author.\")\n",
    "    ap.add_argument(\"--auids\", type=str, default=None, help=\"Comma/space-separated Scopus Author IDs. If omitted, reads authors.txt\")\n",
    "    ap.add_argument(\"--citescore\", type=str, default=None, help=\"Path to CiteScore CSV/XLSX, or a folder containing it\")\n",
    "    ap.add_argument(\"--outdir\", type=str, default=\"authors\", help=\"Output directory (default: ./authors)\")\n",
    "    ap.add_argument(\"--sleep\", type=float, default=0.05, help=\"Sleep between EID fetches (seconds)\")\n",
    "    ap.add_argument(\"--serial-sleep\", type=float, default=0.1, help=\"Sleep between SerialTitle ISSN lookups (seconds)\")\n",
    "    ap.add_argument(\"--no-prompt\", action=\"store_true\", help=\"Do not prompt for missing CiteScore path; exit with error\")\n",
    "    ap.add_argument(\"--aff-id\", type=str, default=None, help=f\"Optional affiliation ID to isolate author-affiliated publications (default: {AFF_ID_DEFAULT})\")\n",
    "    args, _unknown = ap.parse_known_args(argv)\n",
    "\n",
    "    if args.aff_id is not None:\n",
    "        AFF_ID = args.aff_id\n",
    "\n",
    "    cs_path = resolve_citescore_path(args.citescore, no_prompt=args.no_prompt)\n",
    "    if not cs_path:\n",
    "        if _is_ipython() and not args.no_prompt:\n",
    "            try:\n",
    "                entered = input(\"Enter CiteScore CSV/XLSX path or containing folder: \").strip('\"').strip()\n",
    "            except EOFError:\n",
    "                entered = \"\"\n",
    "            if entered:\n",
    "                cs_path = resolve_citescore_path(entered, no_prompt=True)\n",
    "        if not cs_path:\n",
    "            print(\"❌ Could not locate the CiteScore file. Pass --citescore or set CITESCORE_CSV.\")\n",
    "            if _is_ipython():\n",
    "                return\n",
    "            sys.exit(2)\n",
    "    print(f\"Using CiteScore file: {cs_path}\")\n",
    "    cs_table = load_citescore_table(cs_path)\n",
    "\n",
    "    auids = read_auids_from_cli_or_file(args.auids)\n",
    "    if not auids and _is_ipython():\n",
    "        try:\n",
    "            entered = input(\"Enter Scopus Author IDs (comma or space separated), or leave blank to cancel: \").strip()\n",
    "        except EOFError:\n",
    "            entered = \"\"\n",
    "        if entered:\n",
    "            auids = read_auids_from_cli_or_file(entered)\n",
    "    if not auids:\n",
    "        print(\"No AU-IDs provided. Use --auids, or create authors.txt with one AU-ID per line.\")\n",
    "        if _is_ipython():\n",
    "            return\n",
    "        sys.exit(0)\n",
    "\n",
    "    out_dir = Path(args.outdir)\n",
    "    print(f\"CiteScore rows: {len(cs_table)} | Authors: {len(auids)} | Out: {out_dir.resolve()} | Affiliation filter: {AFF_ID}\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    cs_by_source = build_cs_by_source(cs_table, serial_sleep=args.serial_sleep)\n",
    "\n",
    "    written = []\n",
    "    for i, auid in enumerate(auids, 1):\n",
    "        print(f\"[{i}/{len(auids)}] Processing AU-ID {auid} … (AFF_ID={AFF_ID})\")\n",
    "        p = process_author(auid, cs_table, out_dir, sleep=args.sleep, serial_sleep=args.serial_sleep, cs_by_source=cs_by_source)\n",
    "        if p:\n",
    "            written.append(p)\n",
    "\n",
    "    print(f\"\\nDone. Wrote {len(written)} file(s) to {out_dir.resolve()}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b0b5cc-586f-4ee1-b0f3-c594467b47cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94075ff-821d-4143-89fd-c4fb460eb000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5365cd62-1a28-4d53-9b94-d988d9a9d98e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5e8b4-eb90-4ff5-9d70-6ccb350119e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10a06172-1f45-476b-af23-e237ca86da5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CiteScore file: C:\\Users\\yusef.atteyih\\Desktop\\Academic Research Unit\\Yusef ATTEYIH\\Data Solutions\\Data Solutions 2.0\\APP Calculation\\CiteScore 2024\\CiteScore 2024 annual values.csv\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Scopus Author IDs (comma or space separated), or leave blank to cancel:  57193254610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CiteScore rows: 75679 | Authors: 1 | Out: C:\\Users\\yusef.atteyih\\Desktop\\Academic Research Unit\\Yusef ATTEYIH\\Data Solutions\\Data Solutions 2.0\\APP Calculation\\authors | Affiliation filter: 60021379\n",
      "[1/1] Processing AU-ID 57193254610 … (AFF_ID=60021379)\n",
      "→ AU-ID 57193254610 — Fadime İrem Doğan (AFF_ID filter: 60021379)\n",
      "   ✓ Wrote _fadime_irem_dogan___57193254610.xlsx  (Articles: 5, BAU Articles: 3)\n",
      "\n",
      "Done. Wrote 1 file(s) to C:\\Users\\yusef.atteyih\\Desktop\\Academic Research Unit\\Yusef ATTEYIH\\Data Solutions\\Data Solutions 2.0\\APP Calculation\\authors.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "r\"\"\"\n",
    "Scopus → CiteScore with disambiguation by **Source ID then ASJC (sub-subject area)**\n",
    "\n",
    "This version:\n",
    "• Stores ASJC sets as frozenset (hashable).\n",
    "• Matches publications to CiteScore by Source ID then ASJC (ISSN fallback).\n",
    "• Performs per-article author→affiliation mapping and isolates items where the\n",
    "  author is affiliated with the configured AFF_ID on that paper.\n",
    "• Produces three sheets in each author workbook:\n",
    "    - \"Articles\"      : ALL articles for the author (no affiliation filtering)\n",
    "    - \"BAU Articles\"  : only those articles where the author is affiliated with AFF_ID\n",
    "    - \"APP\"           : Participation Score computed from BAU Articles (subtype == \"ar\")\n",
    "• APP uses Author Coefficient (AC = 1.2 if sole author else 1.2 / n_authors)\n",
    "  and Quartile Coefficient (QC) mapped from CiteScore percentile,\n",
    "  Contribution = AC × QC, summed and rounded.\n",
    "• NOTE: APP eligibility and support thresholds are expressed **per academic year (AY)**.\n",
    "\n",
    "Usage:\n",
    "  - Jupyter: run(auids=\"12345,67890\", citescore=\"path/to/CiteScore.csv\", aff_id=\"60021379\")\n",
    "  - CLI: python this_script.py --auids 57193254610 --citescore \"CiteScore 2024.csv\" --aff-id 60021379\n",
    "\n",
    "Requirements:\n",
    "  pybliometrics, pandas, xlsxwriter or openpyxl\n",
    "  Configure pybliometrics Scopus API credentials\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import unicodedata\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Set, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Scopus (pybliometrics) ---\n",
    "try:\n",
    "    from pybliometrics.scopus import AbstractRetrieval, AuthorRetrieval, ScopusSearch, SerialTitle\n",
    "    from pybliometrics.scopus.exception import ScopusException\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"pybliometrics is required. Install with `pip install pybliometrics` \"\n",
    "        \"and ensure your Scopus API credentials are configured.\"\n",
    "    ) from e\n",
    "\n",
    "# ----------------------- IPython detection ----------------------------\n",
    "\n",
    "def _is_ipython() -> bool:\n",
    "    return (\"ipykernel\" in sys.modules) or (\"IPython\" in sys.modules)\n",
    "\n",
    "# ----------------------- Defaults & Affiliation filter ----------------\n",
    "\n",
    "AFF_ID_DEFAULT = \"60021379\"\n",
    "AFF_ID: Optional[str] = AFF_ID_DEFAULT\n",
    "\n",
    "DEFAULT_USER_CITESCORE_DIRS = [\n",
    "    Path(r\"C:\\\\Users\\\\yusef.atteyih\\\\Desktop\\\\Academic Research Unit\\\\Yusef ATTEYIH\\\\Data Solutions\\\\Data Solutions 2.0\\\\APP Calculation\\\\CiteScore 2024\"),\n",
    "]\n",
    "DEFAULT_CITESCORE_DIRS = [Path(\"CiteScore 2024\"), Path(\".\")]\n",
    "POSSIBLE_CS_FILENAMES = [\n",
    "    \"CiteScore 2024 annual values.csv\",\n",
    "    \"CiteScore 2024 annual values.xlsx\",\n",
    "    \"CiteScore 2024.csv\",\n",
    "    \"citescore.csv\",\n",
    "    \"citescore.xlsx\",\n",
    "]\n",
    "\n",
    "# ----------------------- Small helpers --------------------------------\n",
    "\n",
    "def _s(x) -> str:\n",
    "    try:\n",
    "        if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return x if isinstance(x, str) else str(x)\n",
    "\n",
    "def _norm_issn(s: str) -> str:\n",
    "    \"\"\"Uppercase; keep 0-9 and 'X'; strip others.\"\"\"\n",
    "    return re.sub(r\"[^0-9X]\", \"\", _s(s).upper())\n",
    "\n",
    "def _norm_asjc_codes(raw: Any) -> Set[str]:\n",
    "    \"\"\"Return a set of 4-digit ASJC codes as strings.\"\"\"\n",
    "    out: Set[str] = set()\n",
    "    if raw is None:\n",
    "        return out\n",
    "    if isinstance(raw, (list, tuple, set)):\n",
    "        it = raw\n",
    "    else:\n",
    "        it = re.split(r\"[^0-9]\", _s(raw))\n",
    "    for tok in it:\n",
    "        if tok and tok.isdigit():\n",
    "            if len(tok) == 4:\n",
    "                out.add(tok)\n",
    "            elif len(tok) > 4:\n",
    "                out.add(tok[-4:])\n",
    "    return out\n",
    "\n",
    "def _coerce_percentile(val) -> Optional[float]:\n",
    "    txt = _s(val).strip()\n",
    "    if not txt:\n",
    "        return None\n",
    "    txt = txt.replace(\"%\", \"\").replace(\" \", \"\").replace(\",\", \".\")\n",
    "    m = re.search(r\"[-+]?\\d*\\.?\\d+\", txt)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _coerce_float(val) -> Optional[float]:\n",
    "    txt = _s(val).strip().replace(\",\", \".\")\n",
    "    if not txt:\n",
    "        return None\n",
    "    m = re.search(r\"[-+]?\\d*\\.?\\d+\", txt)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def quartile_from_percentile(p: Optional[float]) -> str:\n",
    "    if p is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        p = float(p)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    if p >= 90: return \"QT\"\n",
    "    if p >= 75: return \"Q1\"\n",
    "    if p >= 50: return \"Q2\"\n",
    "    if p >= 25: return \"Q3\"\n",
    "    return \"Q4\"\n",
    "\n",
    "# ----------------------- Filenames ------------------------------------\n",
    "\n",
    "def _ascii_slug(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKD\", _s(s)).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^0-9a-z]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def make_author_filename(author_name: str, author_id: str) -> str:\n",
    "    name_slug = _ascii_slug(author_name)\n",
    "    leading = \"_\" + name_slug if not name_slug.startswith(\"_\") else name_slug\n",
    "    return f\"{leading}___{author_id}.xlsx\"\n",
    "\n",
    "# ----------------------- Read AU-IDs ----------------------------------\n",
    "\n",
    "author_id_file_candidates = (\"authors.txt\", \"auids.txt\", \"ScopusAuthorIDs.txt\")\n",
    "\n",
    "def read_auids_from_cli_or_file(cli_auids: Optional[str]) -> List[str]:\n",
    "    ids: List[str] = []\n",
    "    if cli_auids:\n",
    "        for tok in re.split(r\"[,\\s]+\", cli_auids.strip()):\n",
    "            if tok and tok.isdigit():\n",
    "                ids.append(tok)\n",
    "    if ids:\n",
    "        return sorted(set(ids))\n",
    "    for fname in author_id_file_candidates:\n",
    "        p = Path(fname)\n",
    "        if p.exists():\n",
    "            try:\n",
    "                raw = p.read_text(encoding=\"utf-8\")\n",
    "            except Exception:\n",
    "                raw = p.read_text(errors=\"ignore\")\n",
    "            for line in raw.splitlines():\n",
    "                t = line.split(\"#\", 1)[0].strip()\n",
    "                if t.isdigit():\n",
    "                    ids.append(t)\n",
    "            if ids:\n",
    "                return sorted(set(ids))\n",
    "    return []\n",
    "\n",
    "# ----------------------- CiteScore path --------------------------------\n",
    "\n",
    "def _candidate_files_in_dir(d: Path) -> List[Path]:\n",
    "    return [p for name in POSSIBLE_CS_FILENAMES if (p := d / name).exists() and p.is_file()]\n",
    "\n",
    "def resolve_citescore_path(arg: Optional[str], no_prompt: bool = False) -> Optional[Path]:\n",
    "    if arg:\n",
    "        p = Path(arg)\n",
    "        if p.exists():\n",
    "            if p.is_file(): return p\n",
    "            if p.is_dir():\n",
    "                cand = _candidate_files_in_dir(p)\n",
    "                if cand: return cand[0]\n",
    "    envp = os.environ.get(\"CITESCORE_CSV\")\n",
    "    if envp:\n",
    "        p = Path(envp)\n",
    "        if p.exists() and p.is_file(): return p\n",
    "    for d in DEFAULT_USER_CITESCORE_DIRS + DEFAULT_CITESCORE_DIRS:\n",
    "        if d.exists() and d.is_dir():\n",
    "            cand = _candidate_files_in_dir(d)\n",
    "            if cand: return cand[0]\n",
    "    if not no_prompt:\n",
    "        try:\n",
    "            path_in = input(\"Path to CiteScore CSV/XLSX (or folder containing it): \").strip('\"').strip()\n",
    "            if path_in:\n",
    "                p = Path(path_in)\n",
    "                if p.exists():\n",
    "                    if p.is_file(): return p\n",
    "                    if p.is_dir():\n",
    "                        cand = _candidate_files_in_dir(p)\n",
    "                        if cand: return cand[0]\n",
    "        except EOFError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# ----------------------- Read CiteScore table --------------------------\n",
    "\n",
    "def robust_read_table(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"CiteScore file not found: {path}\")\n",
    "    suffix = path.suffix.lower()\n",
    "    if suffix in (\".xlsx\", \".xls\"):\n",
    "        return pd.read_excel(path)\n",
    "    encodings = [\"utf-8-sig\", \"utf-16\", \"utf-16le\", \"utf-16be\", \"cp1254\", \"iso-8859-9\", \"cp1252\", \"latin1\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, engine=\"python\", sep=None)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding=\"latin1\", engine=\"python\", sep=None, on_bad_lines=\"skip\")\n",
    "    except TypeError:\n",
    "        return pd.read_csv(path, encoding=\"latin1\", engine=\"python\", sep=None, error_bad_lines=False)  # type: ignore\n",
    "\n",
    "def load_citescore_table(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return columns:\n",
    "      source_id (if present), issn_key, eissn_key, asjc_set (frozenset), cs_percentile, citescore\n",
    "    \"\"\"\n",
    "    cs = robust_read_table(path)\n",
    "    norm = {c.strip().lower(): c for c in cs.columns}\n",
    "\n",
    "    p = norm.get(\"print issn\") or norm.get(\"p-issn\") or norm.get(\"issn\")\n",
    "    e = norm.get(\"e-issn\") or norm.get(\"eissn\")\n",
    "    pct = norm.get(\"percentile\") or norm.get(\"citescore percentile\")\n",
    "    val = norm.get(\"citescore\") or norm.get(\"citescore 2024\")\n",
    "    src = norm.get(\"source id\") or norm.get(\"scopus source id\") or norm.get(\"scopus sourceid\")\n",
    "    asjc_col = norm.get(\"asjc\") or norm.get(\"asjc code\") or norm.get(\"asjc codes\") or norm.get(\"subject area asjc\")\n",
    "\n",
    "    if not all([p, e, pct, val]):\n",
    "        raise KeyError(\n",
    "            f\"CiteScore table must include 'Print ISSN', 'E-ISSN', 'Percentile', 'CiteScore'. Found: {list(cs.columns)}\"\n",
    "        )\n",
    "\n",
    "    # Standardize names\n",
    "    cs = cs.rename(columns={p: \"print_issn\", e: \"e_issn\", pct: \"cs_percentile\", val: \"citescore\"})\n",
    "    if src:\n",
    "        cs = cs.rename(columns={src: \"source_id\"})\n",
    "    if asjc_col:\n",
    "        cs = cs.rename(columns={asjc_col: \"asjc_raw\"})\n",
    "    else:\n",
    "        cs[\"asjc_raw\"] = \"\"\n",
    "\n",
    "    # Coerce types\n",
    "    cs[\"print_issn\"] = cs[\"print_issn\"].astype(str)\n",
    "    cs[\"e_issn\"] = cs[\"e_issn\"].astype(str)\n",
    "    cs[\"cs_percentile\"] = cs[\"cs_percentile\"].apply(_coerce_percentile)\n",
    "    cs[\"citescore\"] = cs[\"citescore\"].apply(_coerce_float)\n",
    "    if \"source_id\" in cs.columns:\n",
    "        cs[\"source_id\"] = cs[\"source_id\"].astype(str).str.extract(r\"(\\d+)\", expand=False).fillna(\"\")\n",
    "\n",
    "    # Normalized keys\n",
    "    cs[\"issn_key\"] = cs[\"print_issn\"].map(_norm_issn)\n",
    "    cs[\"eissn_key\"] = cs[\"e_issn\"].map(_norm_issn)\n",
    "\n",
    "    # ASJC as frozenset (hashable)\n",
    "    cs[\"asjc_set\"] = cs[\"asjc_raw\"].apply(lambda v: frozenset(_norm_asjc_codes(v)))\n",
    "\n",
    "    # Keep minimal columns\n",
    "    keep = [\"issn_key\", \"eissn_key\", \"asjc_set\", \"cs_percentile\", \"citescore\"]\n",
    "    if \"source_id\" in cs.columns:\n",
    "        keep.insert(0, \"source_id\")\n",
    "    cs = cs[keep]\n",
    "    return cs\n",
    "\n",
    "# ----------------------- Scopus helpers --------------------------------\n",
    "\n",
    "def _extract_issns(ar: Any) -> Tuple[str, str]:\n",
    "    p = \"\"; e = \"\"\n",
    "    if hasattr(ar, \"eIssn\"):\n",
    "        e = _s(getattr(ar, \"eIssn\"))\n",
    "    if not e and hasattr(ar, \"e_issn\"):\n",
    "        e = _s(getattr(ar, \"e_issn\"))\n",
    "    if hasattr(ar, \"issn\"):\n",
    "        obj = getattr(ar, \"issn\")\n",
    "        if isinstance(obj, str):\n",
    "            if \"ISSN(\" in obj:\n",
    "                mp = re.search(r\"print\\s*=\\s*'([^']+)'\", obj)\n",
    "                me = re.search(r\"electronic\\s*=\\s*'([^']+)'\", obj)\n",
    "                if mp: p = mp.group(1)\n",
    "                if me and not e: e = me.group(1)\n",
    "            else:\n",
    "                p = _s(obj)\n",
    "        else:\n",
    "            try:\n",
    "                p_obj = getattr(obj, \"print\", \"\"); e_obj = getattr(obj, \"electronic\", \"\")\n",
    "                if p_obj: p = _s(p_obj)\n",
    "                if not e and e_obj: e = _s(e_obj)\n",
    "            except Exception:\n",
    "                txt = _s(obj)\n",
    "                mp = re.search(r\"print\\s*=\\s*'([^']+)'\", txt)\n",
    "                me = re.search(r\"electronic\\s*=\\s*'([^']+)'\", txt)\n",
    "                if mp: p = mp.group(1)\n",
    "                if me and not e: e = me.group(1)\n",
    "    return p, e\n",
    "\n",
    "\n",
    "def _extract_asjc(ar: Any) -> Tuple[str, str, str, Set[str]]:\n",
    "    codes: Set[str] = set(); areas: Set[str] = set(); abbrevs: Set[str] = set()\n",
    "    sa = getattr(ar, \"subject_areas\", None)\n",
    "    if sa:\n",
    "        try:\n",
    "            for it in sa:\n",
    "                c = getattr(it, \"code\", None)\n",
    "                a = getattr(it, \"area\", None)\n",
    "                ab = getattr(it, \"abbrev\", None)\n",
    "                if c is not None:\n",
    "                    codes.add(f\"{int(c):04d}\" if str(c).isdigit() else str(c))\n",
    "                if a: areas.add(_s(a))\n",
    "                if ab: abbrevs.add(_s(ab))\n",
    "        except Exception:\n",
    "            try:  # dict-like\n",
    "                for it in sa:\n",
    "                    c = it.get(\"code\"); a = it.get(\"area\"); ab = it.get(\"abbrev\")\n",
    "                    if c is not None:\n",
    "                        codes.add(f\"{int(c):04d}\" if str(c).isdigit() else str(c))\n",
    "                    if a: areas.add(_s(a))\n",
    "                    if ab: abbrevs.add(_s(ab))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return \", \".join(sorted(codes)), \", \".join(sorted(areas)), \", \".join(sorted(abbrevs)), codes\n",
    "\n",
    "# ----------------------- ISSN → Source ID (optional) -------------------\n",
    "\n",
    "def fetch_source_id_for_issn(issn: str) -> Optional[str]:\n",
    "    issn = _norm_issn(issn)\n",
    "    if not issn:\n",
    "        return None\n",
    "    try:\n",
    "        res = SerialTitle(issn)\n",
    "        items: Iterable[Any]\n",
    "        try:\n",
    "            items = list(res) if isinstance(res, (list, tuple)) else [res]\n",
    "        except Exception:\n",
    "            items = [res]\n",
    "        for it in items:\n",
    "            for attr in (\"source_id\", \"sourcerecord_id\", \"sourceid\"):\n",
    "                if hasattr(it, attr):\n",
    "                    sid = _s(getattr(it, attr))\n",
    "                    if sid.isdigit():\n",
    "                        return sid\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"SerialTitle lookup failed for ISSN {issn}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_cs_by_source(cs_table: pd.DataFrame, serial_sleep: float = 0.1) -> pd.DataFrame:\n",
    "    \"\"\"Ensure a cs_by_source table with 'source_id' present, mapping from cs_table.\n",
    "    If cs_table already has source_id, just dedupe and return.\n",
    "    Otherwise, look up source_id from ISSN/e-ISSN via SerialTitle.\n",
    "    \"\"\"\n",
    "    if \"source_id\" in cs_table.columns:\n",
    "        df = cs_table.copy()\n",
    "    else:\n",
    "        df = cs_table.copy()\n",
    "        df[\"source_id_from_print\"] = df[\"issn_key\"].apply(fetch_source_id_for_issn)\n",
    "        if serial_sleep:\n",
    "            time.sleep(serial_sleep)\n",
    "        df[\"source_id_from_e\"] = df[\"eissn_key\"].apply(fetch_source_id_for_issn)\n",
    "        df[\"source_id\"] = df[\"source_id_from_print\"].where(df[\"source_id_from_print\"].notna(), df[\"source_id_from_e\"])\n",
    "        df = df.drop(columns=[\"source_id_from_print\", \"source_id_from_e\"], errors=\"ignore\")\n",
    "    df = df[df[\"source_id\"].astype(str).str.strip().ne(\"\")].copy()\n",
    "    df = df.drop_duplicates(subset=[\"source_id\", \"asjc_set\"], keep=\"first\")\n",
    "    return df[[\"source_id\", \"asjc_set\", \"cs_percentile\", \"citescore\"]]\n",
    "\n",
    "# ----------------------- Matching logic --------------------------------\n",
    "\n",
    "def _pick_best_candidate(cands: pd.DataFrame, article_asjc: Set[str]) -> Tuple[Optional[float], Optional[float]]:\n",
    "    \"\"\"Choose best row: prefer ASJC overlap, then highest percentile, else first.\n",
    "    Returns (cs_percentile, citescore).\n",
    "    \"\"\"\n",
    "    if cands is None or cands.empty:\n",
    "        return None, None\n",
    "    over = []\n",
    "    for _i, row in cands.iterrows():\n",
    "        cs_set = row.get(\"asjc_set\") or set()\n",
    "        if isinstance(cs_set, (set, frozenset)):\n",
    "            cs_set2 = set(cs_set)\n",
    "        elif isinstance(cs_set, (list, tuple)):\n",
    "            cs_set2 = {str(x) for x in cs_set}\n",
    "        else:\n",
    "            cs_set2 = _norm_asjc_codes(cs_set)\n",
    "        over.append(len(article_asjc & cs_set2))\n",
    "    cands = cands.copy()\n",
    "    cands[\"_overlap\"] = over\n",
    "    cands[\"_pct\"] = cands[\"cs_percentile\"].fillna(-1e9)\n",
    "    cands = cands.sort_values([\"_overlap\", \"_pct\"], ascending=[False, False])\n",
    "    top = cands.iloc[0]\n",
    "    return top.get(\"cs_percentile\"), top.get(\"citescore\")\n",
    "\n",
    "\n",
    "def enrich_with_citescore_sourceid_asjc(\n",
    "    df_articles: pd.DataFrame,\n",
    "    cs_table: pd.DataFrame,\n",
    "    cs_by_source: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    if df_articles is None or df_articles.empty:\n",
    "        return df_articles.copy()\n",
    "\n",
    "    df = df_articles.copy()\n",
    "    for col in (\"issn_print\", \"issn_electronic\", \"source_id\"):\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "\n",
    "    df[\"issn_key\"] = df[\"issn_print\"].astype(str).map(_norm_issn)\n",
    "    df[\"eissn_key\"] = df[\"issn_electronic\"].astype(str).map(_norm_issn)\n",
    "\n",
    "    a_asjc_sets: List[Set[str]] = []\n",
    "    for v in df.get(\"asjc_codes\", pd.Series([\"\"] * len(df))):\n",
    "        a_asjc_sets.append(_norm_asjc_codes(v))\n",
    "\n",
    "    cs_p = cs_table[[\"issn_key\", \"asjc_set\", \"cs_percentile\", \"citescore\"]].drop_duplicates()\n",
    "    cs_e = cs_table[[\"eissn_key\", \"asjc_set\", \"cs_percentile\", \"citescore\"]].drop_duplicates()\n",
    "\n",
    "    out_pct: List[Optional[float]] = []\n",
    "    out_val: List[Optional[float]] = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        article_asjc = a_asjc_sets[idx] if idx < len(a_asjc_sets) else set()\n",
    "        sid = _s(row.get(\"source_id\"))\n",
    "        pct = None; val = None\n",
    "        if sid:\n",
    "            cands = cs_by_source[cs_by_source[\"source_id\"].astype(str) == sid]\n",
    "            pct, val = _pick_best_candidate(cands, article_asjc)\n",
    "        if pct is None and val is None:\n",
    "            issn = _s(row.get(\"issn_key\"))\n",
    "            eissn = _s(row.get(\"eissn_key\"))\n",
    "            cands = pd.concat([\n",
    "                cs_p[cs_p[\"issn_key\"] == issn],\n",
    "                cs_e[cs_e[\"eissn_key\"] == eissn],\n",
    "            ], ignore_index=True)\n",
    "            pct, val = _pick_best_candidate(cands, article_asjc)\n",
    "        out_pct.append(pct)\n",
    "        out_val.append(val)\n",
    "\n",
    "    df[\"cs_percentile\"] = out_pct\n",
    "    df[\"citescore\"] = out_val\n",
    "    df[\"quartile\"] = df[\"cs_percentile\"].apply(quartile_from_percentile)\n",
    "    return df\n",
    "\n",
    "# ----------------------- Scopus collectors (with per-author aff detection) -------\n",
    "\n",
    "def get_author_name(author_id: str) -> str:\n",
    "    try:\n",
    "        ar = AuthorRetrieval(author_id)\n",
    "        name = f\"{_s(ar.given_name)} {_s(ar.surname)}\".strip()\n",
    "        return name or author_id\n",
    "    except Exception:\n",
    "        return author_id\n",
    "\n",
    "def get_author_eids(author_id: str) -> List[str]:\n",
    "    try:\n",
    "        s = ScopusSearch(f\"AU-ID({author_id})\", subscriber=True)\n",
    "        return s.get_eids() or []\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"ScopusSearch failed for AU-ID({author_id}): {e}\")\n",
    "        return []\n",
    "\n",
    "def _safe_int(v) -> Optional[int]:\n",
    "    try:\n",
    "        return int(v)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_article_metadata(eid: str, target_auid: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Return article metadata dict. Also detect whether the given target_auid\n",
    "    appears in the article and whether that author entry has affiliation_id == AFF_ID.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ar = AbstractRetrieval(eid, view=\"FULL\")\n",
    "    except ScopusException as e:\n",
    "        warnings.warn(f\"AbstractRetrieval failed for {eid}: {e}\")\n",
    "        return None\n",
    "\n",
    "    if ar.subtype not in (\"ar\", \"re\"):\n",
    "        return None\n",
    "\n",
    "    year = \"\"\n",
    "    if getattr(ar, \"coverDate\", None):\n",
    "        year = _s(ar.coverDate)[:4]\n",
    "\n",
    "    issn_print, issn_elec = _extract_issns(ar)\n",
    "    asjc_codes_csv, asjc_areas_csv, asjc_abbrevs_csv, _codes_set = _extract_asjc(ar)\n",
    "\n",
    "    bau_flag = False\n",
    "    bau_orgs = []\n",
    "    auid_org_map: Dict[str, str] = {}\n",
    "    auth_err = None\n",
    "    groups = getattr(ar, \"authorgroup\", None)\n",
    "    if groups:\n",
    "        try:\n",
    "            for g in groups:\n",
    "                try:\n",
    "                    g_auid = str(getattr(g, \"auid\", \"\") or \"\")\n",
    "                    org_val = getattr(g, \"organization\", None)\n",
    "                    aff_id_val = _safe_int(getattr(g, \"affiliation_id\", None))\n",
    "                    if g_auid:\n",
    "                        auid_org_map[g_auid] = _s(org_val)\n",
    "                    if target_auid and g_auid == str(target_auid) and AFF_ID and aff_id_val == _safe_int(AFF_ID):\n",
    "                        bau_flag = True\n",
    "                        if org_val:\n",
    "                            bau_orgs.append(_s(org_val))\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception as ex:\n",
    "            auth_err = str(ex)\n",
    "    else:\n",
    "        try:\n",
    "            authors = getattr(ar, \"authors\", None) or []\n",
    "            for a in authors:\n",
    "                try:\n",
    "                    g_auid = str(getattr(a, \"auid\", \"\") or \"\")\n",
    "                    org_val = getattr(a, \"orgname\", None) or getattr(a, \"affiliation\", None) or \"\"\n",
    "                    if g_auid:\n",
    "                        auid_org_map[g_auid] = _s(org_val)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    insts = getattr(ar, \"affiliation\", None) or []\n",
    "\n",
    "    return {\n",
    "        \"eid\": _s(eid),\n",
    "        \"title\": _s(ar.title),\n",
    "        \"year\": _s(year),\n",
    "        \"publication_name\": _s(getattr(ar, \"publicationName\", \"\")),\n",
    "        \"subtype\": _s(ar.subtype),\n",
    "        \"doi\": _s(getattr(ar, \"doi\", \"\")),\n",
    "        \"source_id\": _s(getattr(ar, \"source_id\", \"\")),\n",
    "        \"issn_print\": _s(issn_print),\n",
    "        \"issn_electronic\": _s(issn_elec),\n",
    "        \"asjc_codes\": asjc_codes_csv,\n",
    "        \"asjc_areas\": asjc_areas_csv,\n",
    "        \"asjc_abbrevs\": asjc_abbrevs_csv,\n",
    "        \"authors_count\": len(ar.authors) if getattr(ar, \"authors\", None) else 1,\n",
    "        \"combined\": \"; \".join([t for t in (getattr(ar, \"authkeywords\", []) or []) if _s(t)]),\n",
    "        \"abstract\": _s(getattr(ar, \"description\", \"\")),\n",
    "        \"author_org_map_json\": json.dumps(auid_org_map, ensure_ascii=False),\n",
    "        \"is_bau_author\": bau_flag,\n",
    "        \"bau_author_orgs\": \"; \".join(dict.fromkeys(bau_orgs)),\n",
    "        \"affiliations_list\": [(getattr(i, \"name\", \"\") if hasattr(i, \"name\") else _s(i)) for i in insts],\n",
    "        \"auth_group_error\": auth_err,\n",
    "    }\n",
    "\n",
    "# ----------------------- APP calculation helpers -----------------------\n",
    "\n",
    "def _qc_from_percentile(p: Optional[float]) -> Optional[float]:\n",
    "    if p is None:\n",
    "        return None\n",
    "    try:\n",
    "        p = float(p)\n",
    "    except Exception:\n",
    "        return None\n",
    "    if p >= 90:   # Top 10%\n",
    "        return 1.4\n",
    "    if p >= 75:   # Q1\n",
    "        return 1.0\n",
    "    if p >= 50:   # Q2\n",
    "        return 0.8\n",
    "    if p >= 25:   # Q3\n",
    "        return 0.6\n",
    "    if p >= 0:    # Q4\n",
    "        return 0.4\n",
    "    return None\n",
    "\n",
    "def _ac_from_authors(n: Any) -> float:\n",
    "    try:\n",
    "        n = int(n)\n",
    "    except Exception:\n",
    "        n = 1\n",
    "    return 1.2 if n <= 1 else 1.2 / max(n, 1)\n",
    "\n",
    "def _to_int_year(y: Any) -> Optional[int]:\n",
    "    try:\n",
    "        s = str(y)\n",
    "        m = re.search(r\"\\d{4}\", s)\n",
    "        return int(m.group(0)) if m else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def build_app_sheet(df_articles: pd.DataFrame, now_year: Optional[int] = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build APP sheet and summary.\n",
    "    NOTE: Years considered are fixed to 2022, 2023, 2024 as requested.\n",
    "    Only subtype == 'ar' (article) are eligible.\n",
    "    APP totals and eligibility are described per academic year (AY).\n",
    "    \"\"\"\n",
    "    if df_articles is None or df_articles.empty:\n",
    "        return pd.DataFrame(), {\"app_total\": 0.0, \"eligibility\": \"No eligible items\", \"years\": []}\n",
    "\n",
    "    # Fixed window per user's request\n",
    "    years_ok = {2022, 2023, 2024}\n",
    "\n",
    "    tmp = df_articles.copy()\n",
    "    tmp[\"year_i\"] = tmp.get(\"year\", \"\").apply(_to_int_year)\n",
    "    tmp[\"cs_percentile_num\"] = pd.to_numeric(tmp.get(\"cs_percentile\"), errors=\"coerce\")\n",
    "    tmp[\"authors_count_i\"] = pd.to_numeric(tmp.get(\"authors_count\"), errors=\"coerce\").fillna(1).astype(int)\n",
    "    tmp[\"subtype_norm\"] = tmp.get(\"subtype\", \"\").astype(str).str.lower()\n",
    "\n",
    "    eligible = tmp[\n",
    "        tmp[\"year_i\"].isin(years_ok) &\n",
    "        tmp[\"cs_percentile_num\"].notna() &\n",
    "        (tmp[\"subtype_norm\"] == \"ar\")\n",
    "    ].copy()\n",
    "\n",
    "    if eligible.empty:\n",
    "        return pd.DataFrame(), {\"app_total\": 0.0, \"eligibility\": \"No eligible items (no 'ar' articles in 2022-2024)\", \"years\": sorted(years_ok)}\n",
    "\n",
    "    eligible[\"QC\"] = eligible[\"cs_percentile_num\"].apply(_qc_from_percentile)\n",
    "    eligible[\"AC\"] = eligible[\"authors_count_i\"].apply(_ac_from_authors)\n",
    "    eligible = eligible[eligible[\"QC\"].notna()].copy()\n",
    "    if eligible.empty:\n",
    "        return pd.DataFrame(), {\"app_total\": 0.0, \"eligibility\": \"No eligible items (QC missing)\", \"years\": sorted(years_ok)}\n",
    "\n",
    "    eligible[\"Contribution\"] = eligible[\"AC\"] * eligible[\"QC\"]\n",
    "    eligible[\"AC\"] = eligible[\"AC\"].round(2)\n",
    "    eligible[\"QC\"] = eligible[\"QC\"].round(2)\n",
    "    eligible[\"Contribution\"] = eligible[\"Contribution\"].round(2)\n",
    "\n",
    "    out_cols = [\n",
    "        \"eid\", \"title\", \"year\", \"publication_name\",\n",
    "        \"authors_count\", \"cs_percentile\", \"quartile\",\n",
    "        \"AC\", \"QC\", \"Contribution\"\n",
    "    ]\n",
    "    for c in out_cols:\n",
    "        if c not in eligible.columns:\n",
    "            eligible[c] = pd.Series(dtype=\"object\")\n",
    "    df_app = eligible[out_cols].sort_values([\"year\", \"Contribution\"], ascending=[False, False]).reset_index(drop=True)\n",
    "\n",
    "    app_total = float(df_app[\"Contribution\"].sum().round(2))\n",
    "\n",
    "    # Eligibility messages explicitly mention \"per academic year (AY)\"\n",
    "    if app_total > 1.0:\n",
    "        elig = \"APP > 1.0 → up to 2 supports per academic year (AY) (only 1 requires full indexing & APP check)\"\n",
    "    elif app_total >= 0.4:\n",
    "        elig = \"0.4 ≤ APP ≤ 1.0 → 1 support per academic year (AY)\"\n",
    "    else:\n",
    "        elig = \"APP < 0.4 → 1 support per academic year (AY) (if other criteria met)\"\n",
    "\n",
    "    summary = {\"app_total\": round(app_total, 2), \"eligibility\": elig, \"years\": sorted(years_ok)}\n",
    "    return df_app, summary\n",
    "\n",
    "# ----------------------- Excel writer (Articles / BAU Articles / APP) -----------------\n",
    "\n",
    "def _write_excel_xlsxwriter(articles_df: pd.DataFrame, bau_df: pd.DataFrame, path: Path, app_df: Optional[pd.DataFrame] = None, app_summary: Optional[Dict[str, Any]] = None):\n",
    "    with pd.ExcelWriter(path, engine=\"xlsxwriter\") as xl:\n",
    "        # Articles (ALL)\n",
    "        articles_df.to_excel(xl, sheet_name=\"Articles\", index=False)\n",
    "        ws = xl.sheets[\"Articles\"]\n",
    "        try:\n",
    "            ws.freeze_panes(1, 0)\n",
    "            ws.set_column(\"A:A\", 20)  # eid\n",
    "            ws.set_column(\"B:B\", 50)  # title\n",
    "            ws.set_column(\"C:C\", 8)   # year\n",
    "            ws.set_column(\"D:D\", 36)  # publication_name\n",
    "            ws.set_column(\"E:E\", 8)   # subtype\n",
    "            ws.set_column(\"F:F\", 26)  # doi\n",
    "            ws.set_column(\"G:G\", 14)  # source_id\n",
    "            ws.set_column(\"H:I\", 14)  # ISSNs\n",
    "            ws.set_column(\"J:J\", 18)  # asjc_codes\n",
    "            ws.set_column(\"K:K\", 26)  # asjc_areas\n",
    "            ws.set_column(\"L:L\", 14)  # asjc_abbrevs\n",
    "            ws.set_column(\"M:N\", 14)  # cs_percentile / citescore\n",
    "            ws.set_column(\"O:O\", 10)  # quartile\n",
    "            ws.set_column(\"P:P\", 12)  # authors_count\n",
    "            ws.set_column(\"Q:Q\", 28)  # combined\n",
    "            ws.set_column(\"R:R\", 80)  # abstract\n",
    "            ws.set_column(\"S:U\", 28)  # affiliation fields\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # BAU Articles\n",
    "        bau_df.to_excel(xl, sheet_name=\"BAU Articles\", index=False)\n",
    "        ws2 = xl.sheets[\"BAU Articles\"]\n",
    "        try:\n",
    "            ws2.freeze_panes(1, 0)\n",
    "            ws2.set_column(\"A:A\", 20)\n",
    "            ws2.set_column(\"B:B\", 50)\n",
    "            ws2.set_column(\"C:C\", 8)\n",
    "            ws2.set_column(\"D:D\", 36)\n",
    "            ws2.set_column(\"E:E\", 8)\n",
    "            ws2.set_column(\"F:F\", 26)\n",
    "            ws2.set_column(\"G:G\", 14)\n",
    "            ws2.set_column(\"H:I\", 14)\n",
    "            ws2.set_column(\"J:J\", 18)\n",
    "            ws2.set_column(\"K:K\", 26)\n",
    "            ws2.set_column(\"L:L\", 14)\n",
    "            ws2.set_column(\"M:N\", 14)\n",
    "            ws2.set_column(\"O:O\", 10)\n",
    "            ws2.set_column(\"P:P\", 12)\n",
    "            ws2.set_column(\"Q:Q\", 28)\n",
    "            ws2.set_column(\"R:R\", 80)\n",
    "            ws2.set_column(\"S:U\", 28)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # APP sheet (uses BAU Articles)\n",
    "        if app_df is not None and not app_df.empty:\n",
    "            ws3 = xl.book.add_worksheet(\"APP\")\n",
    "            xl.sheets[\"APP\"] = ws3\n",
    "            try:\n",
    "                ws3.write(0, 0, \"APP calculation — fixed years: 2022, 2023, 2024 (journal articles only; subtype == 'ar')\")\n",
    "                ws3.write(1, 0, \"NOTE: APP totals and support thresholds are shown per academic year (AY).\")\n",
    "                if app_summary:\n",
    "                    ws3.write(2, 0, \"Years considered\")\n",
    "                    ws3.write(2, 1, \", \".join(str(y) for y in app_summary.get(\"years\", [])))\n",
    "                    ws3.write(3, 0, \"APP Score (sum of AC×QC, rounded)\")\n",
    "                    ws3.write(3, 1, app_summary.get(\"app_total\", 0.0))\n",
    "                    ws3.write(4, 0, \"Eligibility (per academic year)\")\n",
    "                    ws3.write(4, 1, app_summary.get(\"eligibility\", \"\"))\n",
    "                (app_df.reset_index(drop=True)).to_excel(xl, sheet_name=\"APP\", index=False, startrow=6)\n",
    "                ws3.freeze_panes(7, 0)\n",
    "                ws3.set_column(\"A:A\", 20)\n",
    "                ws3.set_column(\"B:B\", 60)\n",
    "                ws3.set_column(\"C:C\", 8)\n",
    "                ws3.set_column(\"D:D\", 36)\n",
    "                ws3.set_column(\"E:E\", 12)\n",
    "                ws3.set_column(\"F:F\", 14)\n",
    "                ws3.set_column(\"G:G\", 10)\n",
    "                ws3.set_column(\"H:J\", 14)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    app_df.to_excel(xl, sheet_name=\"APP\", index=False)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "def _write_excel_openpyxl(articles_df: pd.DataFrame, bau_df: pd.DataFrame, path: Path, app_df: Optional[pd.DataFrame] = None, app_summary: Optional[Dict[str, Any]] = None):\n",
    "    with pd.ExcelWriter(path, engine=\"openpyxl\") as xl:\n",
    "        articles_df.to_excel(xl, sheet_name=\"Articles\", index=False)\n",
    "        bau_df.to_excel(xl, sheet_name=\"BAU Articles\", index=False)\n",
    "        try:\n",
    "            if app_df is not None and not app_df.empty:\n",
    "                app_df.to_excel(xl, sheet_name=\"APP\", index=False, startrow=5)\n",
    "        except Exception:\n",
    "            try:\n",
    "                app_df.to_excel(xl, sheet_name=\"APP\", index=False)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "def save_author_excel(author_id: str, author_name: str, articles_df: pd.DataFrame, bau_df: pd.DataFrame, out_dir: Path) -> str:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    path = out_dir / make_author_filename(author_name, author_id)\n",
    "\n",
    "    need = [\n",
    "        \"eid\",\"title\",\"year\",\"publication_name\",\"subtype\",\"doi\",\n",
    "        \"source_id\",\"issn_print\",\"issn_electronic\",\n",
    "        \"asjc_codes\",\"asjc_areas\",\"asjc_abbrevs\",\n",
    "        \"cs_percentile\",\"citescore\",\"quartile\",\n",
    "        \"authors_count\",\"combined\",\"abstract\",\n",
    "        \"is_bau_author\",\"bau_author_orgs\",\"author_org_map_json\"\n",
    "    ]\n",
    "\n",
    "    # Ensure columns present for both frames\n",
    "    art = articles_df.copy() if articles_df is not None else pd.DataFrame()\n",
    "    bau = bau_df.copy() if bau_df is not None else pd.DataFrame()\n",
    "    for df in (art, bau):\n",
    "        for c in need:\n",
    "            if c not in df.columns:\n",
    "                df[c] = pd.Series(dtype=\"object\")\n",
    "    art = art[need]\n",
    "    bau = bau[need]\n",
    "\n",
    "    # APP computed from BAU articles\n",
    "    app_df, app_summary = build_app_sheet(bau)\n",
    "\n",
    "    try:\n",
    "        _write_excel_xlsxwriter(art, bau, path, app_df=app_df if not app_df.empty else None, app_summary=app_summary)\n",
    "    except Exception:\n",
    "        _write_excel_openpyxl(art, bau, path, app_df=app_df if not app_df.empty else None, app_summary=app_summary)\n",
    "    return str(path)\n",
    "\n",
    "# ----------------------- Orchestration --------------------------------\n",
    "\n",
    "def process_author(author_id: str, cs_table: pd.DataFrame, out_dir: Path, sleep: float, serial_sleep: float, cs_by_source: Optional[pd.DataFrame] = None) -> Optional[str]:\n",
    "    try:\n",
    "        name = get_author_name(author_id)\n",
    "        print(f\"→ AU-ID {author_id} — {name} (AFF_ID filter: {AFF_ID})\")\n",
    "        eids = get_author_eids(author_id)\n",
    "        recs_all: List[Dict[str, Any]] = []\n",
    "        recs_bau: List[Dict[str, Any]] = []\n",
    "\n",
    "        for eid in eids:\n",
    "            md = get_article_metadata(eid, target_auid=author_id)\n",
    "            if md:\n",
    "                rec = {\n",
    "                    \"eid\": md.get(\"eid\"),\n",
    "                    \"title\": md.get(\"title\"),\n",
    "                    \"year\": md.get(\"year\"),\n",
    "                    \"publication_name\": md.get(\"publication_name\"),\n",
    "                    \"subtype\": md.get(\"subtype\"),\n",
    "                    \"doi\": md.get(\"doi\"),\n",
    "                    \"source_id\": md.get(\"source_id\"),\n",
    "                    \"issn_print\": md.get(\"issn_print\"),\n",
    "                    \"issn_electronic\": md.get(\"issn_electronic\"),\n",
    "                    \"asjc_codes\": md.get(\"asjc_codes\"),\n",
    "                    \"asjc_areas\": md.get(\"asjc_areas\"),\n",
    "                    \"asjc_abbrevs\": md.get(\"asjc_abbrevs\"),\n",
    "                    \"authors_count\": md.get(\"authors_count\"),\n",
    "                    \"combined\": md.get(\"combined\"),\n",
    "                    \"abstract\": md.get(\"abstract\"),\n",
    "                    \"is_bau_author\": md.get(\"is_bau_author\"),\n",
    "                    \"bau_author_orgs\": md.get(\"bau_author_orgs\"),\n",
    "                    \"author_org_map_json\": md.get(\"author_org_map_json\"),\n",
    "                }\n",
    "                # add to \"all articles\" unconditionally\n",
    "                recs_all.append(rec)\n",
    "                # add to BAU-only if author-affiliated on this record\n",
    "                if md.get(\"is_bau_author\", False):\n",
    "                    recs_bau.append(rec)\n",
    "            if sleep:\n",
    "                time.sleep(sleep)\n",
    "\n",
    "        df_all = pd.DataFrame(recs_all)\n",
    "        df_bau = pd.DataFrame(recs_bau)\n",
    "\n",
    "        if not df_all.empty:\n",
    "            if cs_by_source is None:\n",
    "                cs_by_source = build_cs_by_source(cs_table, serial_sleep=serial_sleep)\n",
    "            df_all = enrich_with_citescore_sourceid_asjc(df_all, cs_table, cs_by_source)\n",
    "\n",
    "        if not df_bau.empty:\n",
    "            if cs_by_source is None:\n",
    "                cs_by_source = build_cs_by_source(cs_table, serial_sleep=serial_sleep)\n",
    "            df_bau = enrich_with_citescore_sourceid_asjc(df_bau, cs_table, cs_by_source)\n",
    "\n",
    "        path = save_author_excel(author_id, name, df_all if not df_all.empty else pd.DataFrame(), df_bau if not df_bau.empty else pd.DataFrame(), out_dir)\n",
    "        print(f\"   ✓ Wrote {Path(path).name}  (Articles: {len(df_all) if df_all is not None else 0}, BAU Articles: {len(df_bau) if df_bau is not None else 0})\")\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Failed {author_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ----------------------- Public API (Jupyter) -------------------------\n",
    "\n",
    "def run(\n",
    "    auids: Optional[str] = None,\n",
    "    citescore: Optional[str] = None,\n",
    "    outdir: str = \"authors\",\n",
    "    sleep: float = 0.05,\n",
    "    serial_sleep: float = 0.1,\n",
    "    no_prompt: bool = True,\n",
    "    aff_id: Optional[str] = None,\n",
    ") -> None:\n",
    "    global AFF_ID\n",
    "    if aff_id is not None:\n",
    "        AFF_ID = aff_id\n",
    "\n",
    "    out_dir = Path(outdir)\n",
    "    cs_path = resolve_citescore_path(citescore, no_prompt=no_prompt)\n",
    "    if not cs_path:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not locate CiteScore file. Pass citescore=..., set CITESCORE_CSV env var, or place the file in a default folder.\"\n",
    "        )\n",
    "    print(f\"Using CiteScore file: {cs_path}\")\n",
    "    cs_table = load_citescore_table(cs_path)\n",
    "\n",
    "    ids = read_auids_from_cli_or_file(auids)\n",
    "    if not ids:\n",
    "        print(\"No AU-IDs provided. Provide auids='555...,572...' or authors.txt file.\")\n",
    "        return\n",
    "    print(f\"CiteScore rows: {len(cs_table)} | Authors: {len(ids)} | Affiliation filter: {AFF_ID}\")\n",
    "\n",
    "    cs_by_source = build_cs_by_source(cs_table, serial_sleep=serial_sleep)\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for i, auid in enumerate(ids, 1):\n",
    "        print(f\"[{i}/{len(ids)}]\")\n",
    "        process_author(auid, cs_table, out_dir, sleep=sleep, serial_sleep=serial_sleep, cs_by_source=cs_by_source)\n",
    "    print(f\"\\nDone. Files saved to: {out_dir.resolve()}\")\n",
    "\n",
    "# ----------------------- CLI -----------------------------------------\n",
    "\n",
    "def main(argv: Optional[list] = None):\n",
    "    global AFF_ID\n",
    "    ap = argparse.ArgumentParser(description=\"Fetch Scopus pubs by AU-ID and match CiteScore by Source ID then ASJC (ISSN fallback). Produces 'Articles', 'BAU Articles' and 'APP' sheets per author. APP thresholds/messages are per academic year (AY).\")\n",
    "    ap.add_argument(\"--auids\", type=str, default=None, help=\"Comma/space-separated Scopus Author IDs. If omitted, reads authors.txt\")\n",
    "    ap.add_argument(\"--citescore\", type=str, default=None, help=\"Path to CiteScore CSV/XLSX, or a folder containing it\")\n",
    "    ap.add_argument(\"--outdir\", type=str, default=\"authors\", help=\"Output directory (default: ./authors)\")\n",
    "    ap.add_argument(\"--sleep\", type=float, default=0.05, help=\"Sleep between EID fetches (seconds)\")\n",
    "    ap.add_argument(\"--serial-sleep\", type=float, default=0.1, help=\"Sleep between SerialTitle ISSN lookups (seconds)\")\n",
    "    ap.add_argument(\"--no-prompt\", action=\"store_true\", help=\"Do not prompt for missing CiteScore path; exit with error\")\n",
    "    ap.add_argument(\"--aff-id\", type=str, default=None, help=f\"Optional affiliation ID to isolate author-affiliated publications (default: {AFF_ID_DEFAULT})\")\n",
    "    args, _unknown = ap.parse_known_args(argv)\n",
    "\n",
    "    if args.aff_id is not None:\n",
    "        AFF_ID = args.aff_id\n",
    "\n",
    "    cs_path = resolve_citescore_path(args.citescore, no_prompt=args.no_prompt)\n",
    "    if not cs_path:\n",
    "        if _is_ipython() and not args.no_prompt:\n",
    "            try:\n",
    "                entered = input(\"Enter CiteScore CSV/XLSX path or containing folder: \").strip('\"').strip()\n",
    "            except EOFError:\n",
    "                entered = \"\"\n",
    "            if entered:\n",
    "                cs_path = resolve_citescore_path(entered, no_prompt=True)\n",
    "        if not cs_path:\n",
    "            print(\"❌ Could not locate the CiteScore file. Pass --citescore or set CITESCORE_CSV.\")\n",
    "            if _is_ipython():\n",
    "                return\n",
    "            sys.exit(2)\n",
    "    print(f\"Using CiteScore file: {cs_path}\")\n",
    "    cs_table = load_citescore_table(cs_path)\n",
    "\n",
    "    auids = read_auids_from_cli_or_file(args.auids)\n",
    "    if not auids and _is_ipython():\n",
    "        try:\n",
    "            entered = input(\"Enter Scopus Author IDs (comma or space separated), or leave blank to cancel: \").strip()\n",
    "        except EOFError:\n",
    "            entered = \"\"\n",
    "        if entered:\n",
    "            auids = read_auids_from_cli_or_file(entered)\n",
    "    if not auids:\n",
    "        print(\"No AU-IDs provided. Use --auids, or create authors.txt with one AU-ID per line.\")\n",
    "        if _is_ipython():\n",
    "            return\n",
    "        sys.exit(0)\n",
    "\n",
    "    out_dir = Path(args.outdir)\n",
    "    print(f\"CiteScore rows: {len(cs_table)} | Authors: {len(auids)} | Out: {out_dir.resolve()} | Affiliation filter: {AFF_ID}\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    cs_by_source = build_cs_by_source(cs_table, serial_sleep=args.serial_sleep)\n",
    "\n",
    "    written = []\n",
    "    for i, auid in enumerate(auids, 1):\n",
    "        print(f\"[{i}/{len(auids)}] Processing AU-ID {auid} … (AFF_ID={AFF_ID})\")\n",
    "        p = process_author(auid, cs_table, out_dir, sleep=args.sleep, serial_sleep=args.serial_sleep, cs_by_source=cs_by_source)\n",
    "        if p:\n",
    "            written.append(p)\n",
    "\n",
    "    print(f\"\\nDone. Wrote {len(written)} file(s) to {out_dir.resolve()}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c0d38c-f8f6-43b7-9fdd-b39da23def10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
